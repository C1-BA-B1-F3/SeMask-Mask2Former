{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Line Args: Namespace(config_file='./configs/ade20k/semantic-segmentation/semask_swin/msfapn_maskformer2_semask_swin_large_IN21k_384_bs16_160k_res640_boundary_loss.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, fff='/root/.local/share/jupyter/runtime/kernel-13de9dff-41f9-4553-b9ca-640b70dfce9b.json', machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:18:35 fvcore.common.config]: \u001b[0mLoading config ./configs/ade20k/semantic-segmentation/semask_swin/msfapn_maskformer2_semask_swin_large_IN21k_384_bs16_160k_res640_boundary_loss.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:18:35 fvcore.common.config]: \u001b[0mLoading config ./configs/ade20k/semantic-segmentation/semask_swin/../Base-ADE20K-SemanticSegmentation_modified.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "\u001b[32m[10/12 11:18:35 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
      "\u001b[32m[10/12 11:18:36 detectron2]: \u001b[0mEnvironment info:\n",
      "----------------------  -----------------------------------------------------------------------------\n",
      "sys.platform            linux\n",
      "Python                  3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) [GCC 7.5.0]\n",
      "numpy                   1.19.5\n",
      "detectron2              0.5 @/root/.local/lib/python3.6/site-packages/detectron2\n",
      "Compiler                GCC 7.3\n",
      "CUDA compiler           CUDA 10.2\n",
      "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
      "DETECTRON2_ENV_MODULE   <not set>\n",
      "PyTorch                 1.7.0 @/root/.local/conda/envs/semask/lib/python3.6/site-packages/torch\n",
      "PyTorch debug build     True\n",
      "GPU available           Yes\n",
      "GPU 0                   Tesla V100S-PCIE-32GB (arch=7.0)\n",
      "CUDA_HOME               /usr/local/cuda\n",
      "Pillow                  8.4.0\n",
      "torchvision             0.8.0 @/root/.local/conda/envs/semask/lib/python3.6/site-packages/torchvision\n",
      "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
      "fvcore                  0.1.5.post20211023\n",
      "iopath                  0.1.8\n",
      "cv2                     4.5.5\n",
      "----------------------  -----------------------------------------------------------------------------\n",
      "PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 10.2\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 7.6.5\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "\u001b[32m[10/12 11:18:36 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='./configs/ade20k/semantic-segmentation/semask_swin/msfapn_maskformer2_semask_swin_large_IN21k_384_bs16_160k_res640_boundary_loss.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, fff='/root/.local/share/jupyter/runtime/kernel-13de9dff-41f9-4553-b9ca-640b70dfce9b.json', machine_rank=0, num_gpus=1, num_machines=1, opts=[], resume=False)\n",
      "\u001b[32m[10/12 11:18:36 detectron2]: \u001b[0mContents of args.config_file=./configs/ade20k/semantic-segmentation/semask_swin/msfapn_maskformer2_semask_swin_large_IN21k_384_bs16_160k_res640_boundary_loss.yaml:\n",
      "\u001b[38;5;197m_BASE_\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msemask_maskformer2_msfapn_R50_bs16_160k_modified.yaml\u001b[39m\n",
      "\u001b[38;5;197mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mD2SeMaskSwinTransformer\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m18\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m2\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m6\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m12\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m24\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m48\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSEM_WINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_SEM_BLOCKS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m./checkpoints/semask_large_mask2former_msfapn_ade20k.pth\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m123.675\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m116.280\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m103.530\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m58.395\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.120\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m57.375\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;197mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;81m!!python/object/apply:eval\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186m[int(x\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m0.1\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m*\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m640)\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186mfor\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186mx\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186min\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186mrange(5,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m21)]\u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mchoice\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mabsolute\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;242m# SIZE: (640, 640)\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m(512,\u001b[39m\u001b[38;5;141m \u001b[39m\u001b[38;5;141m512)\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\u001b[38;5;15m  \u001b[39m\u001b[38;5;242m# used in dataset mapper?\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m\"\u001b[39m\u001b[38;5;186mRGB\u001b[39m\u001b[38;5;186m\"\u001b[39m\n",
      "\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m320\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m480\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m640\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m800\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m960\u001b[39m\u001b[38;5;15m,\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m1120\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4480\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrue\u001b[39m\n",
      "\n",
      "\u001b[32m[10/12 11:18:36 detectron2]: \u001b[0mRunning with full config:\n",
      "\u001b[38;5;197mCUDNN_BENCHMARK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;197mDATALOADER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mASPECT_RATIO_GROUPING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFILTER_EMPTY_ANNOTATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNUM_WORKERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mREPEAT_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSAMPLER_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mTrainingSampler\u001b[39m\n",
      "\u001b[38;5;197mDATASETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROPOSAL_FILES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROPOSAL_FILES_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mPotsdam_test\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mTRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mPotsdam_train\u001b[39m\n",
      "\u001b[38;5;197mGLOBAL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mHACK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;197mINPUT\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCOLOR_AUG_SSD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCROP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSINGLE_CATEGORY_MAX_AREA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mabsolute\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDATASET_MAPPER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmask_former_semantic\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRGB\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mIMAGE_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_FORMAT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mpolygon\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2560\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m448\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m576\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m704\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m768\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m832\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m896\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m960\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1088\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1152\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1216\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1280\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMIN_SIZE_TRAIN_SAMPLING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mchoice\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRANDOM_FLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhorizontal\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
      "\u001b[38;5;197mMODEL\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mANCHOR_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mANGLES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-90\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m90\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPECT_RATIOS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mDefaultAnchorGenerator\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOFFSET\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m128\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFREEZE_AT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mD2SeMaskSwinTransformer\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDEVICE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mcuda\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mFPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFUSE_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msum\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mKEYPOINT_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mLOAD_PROPOSALS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_FORMER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCATE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLASS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEEP_SUPERVISION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDICE_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDIM_FEEDFORWARD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2048\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENFORCE_INPUT_PROJ\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mHIDDEN_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIMPORTANCE_SAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.75\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNHEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNO_OBJECT_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_OBJECT_QUERIES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOVERSAMPLE_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m3.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSIZE_DIVISIBILITY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m32\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mINSTANCE_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mOBJECT_MASK_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mOVERLAP_THRESHOLD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.8\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mPANOPTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mSEMANTIC_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRAIN_NUM_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12544\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRANSFORMER_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMultiScaleMaskedTransformerDecoder\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRANSFORMER_IN_FEATURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mmulti_scale_pixel_decoder\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMASK_ON\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMETA_ARCHITECTURE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mSeMaskMaskFormer\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPANOPTIC_FPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCOMBINE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mINSTANCES_CONFIDENCE_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mOVERLAP_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;197mSTUFF_AREA_LIMIT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4096\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mINSTANCE_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_MEAN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m123.675\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m116.28\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m103.53\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPIXEL_STD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m58.395\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.12\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m57.375\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPROPOSAL_GENERATOR\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRPN\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRESNETS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORM_MODULATED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORM_NUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORM_ON_PER_STAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEPTH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m50\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mFrozenBN\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_GROUPS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES2_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES4_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES5_DILATION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mRES5_MULTI_GRID\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSTEM_OUT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSTEM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mbasic\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSTRIDE_IN_1X1\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mWIDTH_PER_GROUP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m64\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRETINANET\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFOCAL_LOSS_ALPHA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFOCAL_LOSS_GAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp6\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mp7\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CONVS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRIOR_PROB\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSMOOTH_L1_LOSS_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTOPK_CANDIDATES_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_BOX_CASCADE_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m20.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
      "\u001b[38;5;15m      \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m15.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOUS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.6\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_BOX_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m10.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLS_AGNOSTIC_BBOX_REG\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFC_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1024\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_FC\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRAIN_ON_PRED_BOXES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mRes5ROIHeads\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNMS_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.25\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROPOSAL_APPEND_GT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSCORE_THRESH_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_KEYPOINT_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m512\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_KEYPOINTS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mKRCNNConvDeconvUpsampleHead\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_KEYPOINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m17\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mROI_MASK_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLS_AGNOSTIC_MASK\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMaskRCNNConvUpsampleHead\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;186m'\u001b[39m\u001b[38;5;186m'\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_RESOLUTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m14\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_SAMPLING_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOOLER_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mROIAlignV2\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mRPN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBATCH_SIZE_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141msmooth_l1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_LOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBBOX_REG_WEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mBOUNDARY_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONV_DIMS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mHEAD_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mStandardRPNHead\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_LABELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIOU_THRESHOLDS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNMS_THRESH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.7\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOSITIVE_FRACTION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOST_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPOST_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2000\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NMS_TOPK_TEST\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6000\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRE_NMS_TOPK_TRAIN\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12000\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSMOOTH_L1_BETA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSEM_SEG_HEAD\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPP_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPP_DILATIONS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mASPP_DROPOUT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCOMMON_STRIDE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCONVS_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m8\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIGNORE_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m255\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mIN_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mhard_pixel_mining\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mLOSS_WEIGHT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMASK_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m256\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mBranchMaskFormerHead\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mGN\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_CLASSES\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPIXEL_DECODER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mMSDeformAttnPixelFANDecoder\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROJECT_CHANNELS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPROJECT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mTRANSFORMER_ENC_LAYERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSWIN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mAPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mATTN_DROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDEPTHS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m18\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROP_PATH_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mDROP_RATE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mEMBED_DIM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m192\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMLP_RATIO\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4.0\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_HEADS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m6\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m24\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m48\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_SEM_BLOCKS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mOUT_FEATURES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres2\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres3\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mres5\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPATCH_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPATCH_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mPRETRAIN_IMG_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m384\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mQKV_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mQK_SCALE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mnull\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mSEM_WINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mUSE_CHECKPOINT\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mWINDOW_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m12\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./checkpoints/semask_large_mask2former_msfapn_ade20k.pth\u001b[39m\n",
      "\u001b[38;5;197mOUTPUT_DIR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m./output/potsdam_contrast_experiment_wrt_boundary_loss\u001b[39m\n",
      "\u001b[38;5;197mSEED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m-1\u001b[39m\n",
      "\u001b[38;5;197mSOLVER\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mAMP\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBACKBONE_MULTIPLIER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBASE_LR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mBIAS_LR_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCHECKPOINT_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m5000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mCLIP_GRADIENTS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfull_model\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mCLIP_VALUE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.01\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNORM_TYPE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mGAMMA\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.1\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mIMS_PER_BATCH\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mLR_SCHEDULER_NAME\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mWarmupPolyLR\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMAX_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m80000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mMOMENTUM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mNESTEROV\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mOPTIMIZER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mADAMW\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPOLY_LR_CONSTANT_ENDING\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPOLY_LR_POWER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.9\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mREFERENCE_WORLD_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mSTEPS\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m30000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_FACTOR\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_ITERS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWARMUP_METHOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mlinear\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.05\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_BIAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0001\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_EMBED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mWEIGHT_DECAY_NORM\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0.0\u001b[39m\n",
      "\u001b[38;5;197mTEST\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mAUG\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mFLIP\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mtrue\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMAX_SIZE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m4480\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mMIN_SIZES\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m320\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m480\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m640\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m800\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m960\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;15m-\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1120\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mDETECTIONS_PER_IMAGE\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m100\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mEVAL_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m1000\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mEXPECTED_RESULTS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mKEYPOINT_OKS_SIGMAS\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;15m[\u001b[39m\u001b[38;5;15m]\u001b[39m\n",
      "\u001b[38;5;15m  \u001b[39m\u001b[38;5;197mPRECISE_BN\u001b[39m\u001b[38;5;15m:\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mENABLED\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141mfalse\u001b[39m\n",
      "\u001b[38;5;15m    \u001b[39m\u001b[38;5;197mNUM_ITER\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m200\u001b[39m\n",
      "\u001b[38;5;197mVERSION\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m2\u001b[39m\n",
      "\u001b[38;5;197mVIS_PERIOD\u001b[39m\u001b[38;5;15m:\u001b[39m\u001b[38;5;15m \u001b[39m\u001b[38;5;141m0\u001b[39m\n",
      "\n",
      "\u001b[32m[10/12 11:18:36 detectron2]: \u001b[0mFull config saved to ./output/potsdam_contrast_experiment_wrt_boundary_loss/config.yaml\n",
      "\u001b[32m[10/12 11:18:36 d2.utils.env]: \u001b[0mUsing a generated random seed 36624164\n",
      "\u001b[32m[10/12 11:18:46 d2.engine.defaults]: \u001b[0mModel:\n",
      "SeMaskMaskFormer(\n",
      "  (backbone): D2SeMaskSwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (semantic_layer): SWSeMaskBlock(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (class_injection): ModuleList(\n",
      "            (0): SemanticAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (mlp_cls_q): Linear(in_features=192, out_features=6, bias=True)\n",
      "              (mlp_cls_k): Linear(in_features=192, out_features=6, bias=True)\n",
      "              (mlp_v): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (mlp_res): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (semantic_layer): SWSeMaskBlock(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (class_injection): ModuleList(\n",
      "            (0): SemanticAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (mlp_cls_q): Linear(in_features=384, out_features=6, bias=True)\n",
      "              (mlp_cls_k): Linear(in_features=384, out_features=6, bias=True)\n",
      "              (mlp_v): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (mlp_res): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (semantic_layer): SeMaskBlock(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (class_injection): ModuleList(\n",
      "            (0): SemanticAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (mlp_cls_q): Linear(in_features=768, out_features=6, bias=True)\n",
      "              (mlp_cls_k): Linear(in_features=768, out_features=6, bias=True)\n",
      "              (mlp_v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (mlp_res): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath()\n",
      "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              (act): GELU()\n",
      "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (semantic_layer): SeMaskBlock(\n",
      "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          (class_injection): ModuleList(\n",
      "            (0): SemanticAttention(\n",
      "              (softmax): Softmax(dim=-1)\n",
      "              (mlp_cls_q): Linear(in_features=1536, out_features=6, bias=True)\n",
      "              (mlp_cls_k): Linear(in_features=1536, out_features=6, bias=True)\n",
      "              (mlp_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (mlp_res): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sem_seg_head): BranchMaskFormerHead(\n",
      "    (sem_head): SemanticHead(\n",
      "      (scale_heads): ModuleList(\n",
      "        (0): Sequential()\n",
      "        (1): Sequential(\n",
      "          (0): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "          (1): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "          (1): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "          (2): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pixel_decoder): MSDeformAttnPixelFANDecoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (align_1): FeatureAlign(\n",
      "        (offset): Conv2d(\n",
      "          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (dcpack_L2): DCN(\n",
      "          (conv_offset_mask): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (transformer_self_attention_layers): ModuleList(\n",
      "        (0): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (2): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (3): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (4): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (5): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (6): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (7): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (8): SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_cross_attention_layers): ModuleList(\n",
      "        (0): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (1): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (2): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (3): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (4): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (5): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (6): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (7): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (8): CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_ffn_layers): ModuleList(\n",
      "        (0): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (query_feat): Embedding(100, 256)\n",
      "      (query_embed): Embedding(100, 256)\n",
      "      (level_embed): Embedding(3, 256)\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential()\n",
      "        (1): Sequential()\n",
      "        (2): Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=7, bias=True)\n",
      "      (class_binary_embed): Linear(in_features=256, out_features=3, bias=True)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lateral_convs): ModuleList(\n",
      "      (0): ConvModule(\n",
      "        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU()\n",
      "      )\n",
      "      (1): ConvModule(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU()\n",
      "      )\n",
      "      (2): ConvModule(\n",
      "        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU()\n",
      "      )\n",
      "      (3): ConvModule(\n",
      "        (conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): SeMaskCriterion SeMaskSetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 2.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'labels_cate', 'masks']\n",
      "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_cate': 0.4, 'loss_boundary': 2.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_cate_0': 0.4, 'loss_boundary_0': 2.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_cate_1': 0.4, 'loss_boundary_1': 2.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_cate_2': 0.4, 'loss_boundary_2': 2.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_cate_3': 0.4, 'loss_boundary_3': 2.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_cate_4': 0.4, 'loss_boundary_4': 2.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_cate_5': 0.4, 'loss_boundary_5': 2.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_cate_6': 0.4, 'loss_boundary_6': 2.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_cate_7': 0.4, 'loss_boundary_7': 2.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_cate_8': 0.4, 'loss_boundary_8': 2.0}\n",
      "      num_classes: 6\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "\u001b[32m[10/12 11:20:42 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[10/12 11:20:42 d2.data.common]: \u001b[0mSerializing 3456 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 11:20:42 d2.data.common]: \u001b[0mSerialized dataset takes 0.80 MiB\n",
      "\u001b[32m[10/12 11:20:50 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from ./checkpoints/semask_large_mask2former_msfapn_ade20k.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 192) in the checkpoint but (6, 192) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 192) in the checkpoint but (6, 192) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 384) in the checkpoint but (6, 384) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 384) in the checkpoint but (6, 384) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 768) in the checkpoint but (6, 768) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 768) in the checkpoint but (6, 768) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.weight' to the model due to incompatible shapes: (150, 1536) in the checkpoint but (6, 1536) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.weight' to the model due to incompatible shapes: (150, 1536) in the checkpoint but (6, 1536) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'backbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.bias' to the model due to incompatible shapes: (150,) in the checkpoint but (6,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (151, 256) in the checkpoint but (7, 256) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (151,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSkip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (151,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/12 11:20:51 fvcore.common.checkpoint]: \u001b[0mSome model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.layers.0.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.0.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.1.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.1.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.2.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.2.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.3.semantic_layer.class_injection.0.mlp_cls_k.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.layers.3.semantic_layer.class_injection.0.mlp_cls_q.{bias, weight}\u001b[0m\n",
      "\u001b[34mcriterion.empty_weight\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.0.bn.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.0.conv.weight\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.1.bn.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.1.conv.weight\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.2.bn.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.2.conv.weight\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.3.bn.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34msem_seg_head.lateral_convs.3.conv.weight\u001b[0m\n",
      "\u001b[34msem_seg_head.predictor.class_binary_embed.{bias, weight}\u001b[0m\n",
      "\u001b[34msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
      "\u001b[32m[10/12 11:20:51 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/conda/envs/semask/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/12 11:21:34 d2.utils.events]: \u001b[0m eta: 1 day, 0:13:25  iter: 19  total_loss: 62.45  loss_ce: 2.759  loss_cate: 1.253  loss_mask: 1.661  loss_dice: 1.488  loss_ce_0: 3.993  loss_cate_0: 0  loss_mask_0: 1.708  loss_dice_0: 1.899  loss_ce_1: 2.779  loss_cate_1: 0  loss_mask_1: 1.448  loss_dice_1: 1.563  loss_ce_2: 2.594  loss_cate_2: 0  loss_mask_2: 1.617  loss_dice_2: 1.557  loss_ce_3: 2.625  loss_cate_3: 0  loss_mask_3: 1.618  loss_dice_3: 1.53  loss_ce_4: 2.577  loss_cate_4: 0  loss_mask_4: 1.598  loss_dice_4: 1.509  loss_ce_5: 2.697  loss_cate_5: 0  loss_mask_5: 1.641  loss_dice_5: 1.591  loss_ce_6: 2.71  loss_cate_6: 0  loss_mask_6: 1.795  loss_dice_6: 1.526  loss_ce_7: 2.678  loss_cate_7: 0  loss_mask_7: 1.621  loss_dice_7: 1.532  loss_ce_8: 2.69  loss_cate_8: 0  loss_mask_8: 1.614  loss_dice_8: 1.535  time: 1.2243  data_time: 0.0387  lr: 9.9979e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:21:57 d2.utils.events]: \u001b[0m eta: 1 day, 0:17:29  iter: 39  total_loss: 52.09  loss_ce: 1.626  loss_cate: 1.275  loss_mask: 1.297  loss_dice: 1.595  loss_ce_0: 3.793  loss_cate_0: 0  loss_mask_0: 1.439  loss_dice_0: 1.593  loss_ce_1: 2.031  loss_cate_1: 0  loss_mask_1: 1.331  loss_dice_1: 1.45  loss_ce_2: 1.812  loss_cate_2: 0  loss_mask_2: 1.335  loss_dice_2: 1.509  loss_ce_3: 1.769  loss_cate_3: 0  loss_mask_3: 1.357  loss_dice_3: 1.481  loss_ce_4: 1.616  loss_cate_4: 0  loss_mask_4: 1.391  loss_dice_4: 1.528  loss_ce_5: 1.555  loss_cate_5: 0  loss_mask_5: 1.375  loss_dice_5: 1.553  loss_ce_6: 1.491  loss_cate_6: 0  loss_mask_6: 1.38  loss_dice_6: 1.57  loss_ce_7: 1.579  loss_cate_7: 0  loss_mask_7: 1.352  loss_dice_7: 1.639  loss_ce_8: 1.6  loss_cate_8: 0  loss_mask_8: 1.333  loss_dice_8: 1.59  time: 1.1590  data_time: 0.0140  lr: 9.9956e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:22:19 d2.utils.events]: \u001b[0m eta: 1 day, 0:19:13  iter: 59  total_loss: 43.23  loss_ce: 0.9161  loss_cate: 1.064  loss_mask: 1.302  loss_dice: 1.48  loss_ce_0: 3.546  loss_cate_0: 0  loss_mask_0: 1.326  loss_dice_0: 1.413  loss_ce_1: 1.678  loss_cate_1: 0  loss_mask_1: 1.218  loss_dice_1: 1.384  loss_ce_2: 1.322  loss_cate_2: 0  loss_mask_2: 1.282  loss_dice_2: 1.454  loss_ce_3: 1.128  loss_cate_3: 0  loss_mask_3: 1.279  loss_dice_3: 1.469  loss_ce_4: 1.019  loss_cate_4: 0  loss_mask_4: 1.368  loss_dice_4: 1.509  loss_ce_5: 1.027  loss_cate_5: 0  loss_mask_5: 1.347  loss_dice_5: 1.454  loss_ce_6: 1.017  loss_cate_6: 0  loss_mask_6: 1.359  loss_dice_6: 1.498  loss_ce_7: 0.9127  loss_cate_7: 0  loss_mask_7: 1.396  loss_dice_7: 1.472  loss_ce_8: 0.8736  loss_cate_8: 0  loss_mask_8: 1.293  loss_dice_8: 1.524  time: 1.1362  data_time: 0.0130  lr: 9.9934e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:22:41 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:22  iter: 79  total_loss: 39.16  loss_ce: 0.8568  loss_cate: 1.072  loss_mask: 1.354  loss_dice: 1.355  loss_ce_0: 3.31  loss_cate_0: 0  loss_mask_0: 1.271  loss_dice_0: 1.402  loss_ce_1: 1.402  loss_cate_1: 0  loss_mask_1: 1.134  loss_dice_1: 1.37  loss_ce_2: 1.097  loss_cate_2: 0  loss_mask_2: 1.206  loss_dice_2: 1.325  loss_ce_3: 0.9698  loss_cate_3: 0  loss_mask_3: 1.306  loss_dice_3: 1.364  loss_ce_4: 0.8714  loss_cate_4: 0  loss_mask_4: 1.332  loss_dice_4: 1.4  loss_ce_5: 0.8279  loss_cate_5: 0  loss_mask_5: 1.384  loss_dice_5: 1.388  loss_ce_6: 0.7831  loss_cate_6: 0  loss_mask_6: 1.31  loss_dice_6: 1.42  loss_ce_7: 0.8235  loss_cate_7: 0  loss_mask_7: 1.392  loss_dice_7: 1.402  loss_ce_8: 0.871  loss_cate_8: 0  loss_mask_8: 1.344  loss_dice_8: 1.41  time: 1.1235  data_time: 0.0130  lr: 9.9911e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:23:04 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:31  iter: 99  total_loss: 38.07  loss_ce: 0.6841  loss_cate: 0.9555  loss_mask: 1.191  loss_dice: 1.525  loss_ce_0: 3.159  loss_cate_0: 0  loss_mask_0: 1.127  loss_dice_0: 1.446  loss_ce_1: 1.161  loss_cate_1: 0  loss_mask_1: 1.158  loss_dice_1: 1.417  loss_ce_2: 0.8974  loss_cate_2: 0  loss_mask_2: 1.188  loss_dice_2: 1.448  loss_ce_3: 0.7968  loss_cate_3: 0  loss_mask_3: 1.211  loss_dice_3: 1.428  loss_ce_4: 0.7783  loss_cate_4: 0  loss_mask_4: 1.251  loss_dice_4: 1.482  loss_ce_5: 0.7044  loss_cate_5: 0  loss_mask_5: 1.215  loss_dice_5: 1.547  loss_ce_6: 0.6866  loss_cate_6: 0  loss_mask_6: 1.234  loss_dice_6: 1.475  loss_ce_7: 0.6819  loss_cate_7: 0  loss_mask_7: 1.208  loss_dice_7: 1.496  loss_ce_8: 0.6719  loss_cate_8: 0  loss_mask_8: 1.2  loss_dice_8: 1.544  time: 1.1189  data_time: 0.0135  lr: 9.9889e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:23:26 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:12  iter: 119  total_loss: 41.25  loss_ce: 0.6921  loss_cate: 0.9061  loss_mask: 1.267  loss_dice: 1.717  loss_ce_0: 3.039  loss_cate_0: 0  loss_mask_0: 1.179  loss_dice_0: 1.542  loss_ce_1: 1.086  loss_cate_1: 0  loss_mask_1: 1.217  loss_dice_1: 1.529  loss_ce_2: 0.8821  loss_cate_2: 0  loss_mask_2: 1.138  loss_dice_2: 1.584  loss_ce_3: 0.7694  loss_cate_3: 0  loss_mask_3: 1.3  loss_dice_3: 1.654  loss_ce_4: 0.6689  loss_cate_4: 0  loss_mask_4: 1.36  loss_dice_4: 1.741  loss_ce_5: 0.6889  loss_cate_5: 0  loss_mask_5: 1.362  loss_dice_5: 1.688  loss_ce_6: 0.7104  loss_cate_6: 0  loss_mask_6: 1.345  loss_dice_6: 1.603  loss_ce_7: 0.6384  loss_cate_7: 0  loss_mask_7: 1.352  loss_dice_7: 1.643  loss_ce_8: 0.7348  loss_cate_8: 0  loss_mask_8: 1.313  loss_dice_8: 1.646  time: 1.1152  data_time: 0.0128  lr: 9.9866e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:23:48 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:55  iter: 139  total_loss: 34.54  loss_ce: 0.6154  loss_cate: 0.8643  loss_mask: 1.301  loss_dice: 1.237  loss_ce_0: 2.949  loss_cate_0: 0  loss_mask_0: 1.089  loss_dice_0: 1.33  loss_ce_1: 0.9715  loss_cate_1: 0  loss_mask_1: 1.18  loss_dice_1: 1.283  loss_ce_2: 0.757  loss_cate_2: 0  loss_mask_2: 1.216  loss_dice_2: 1.301  loss_ce_3: 0.7136  loss_cate_3: 0  loss_mask_3: 1.276  loss_dice_3: 1.258  loss_ce_4: 0.6587  loss_cate_4: 0  loss_mask_4: 1.278  loss_dice_4: 1.291  loss_ce_5: 0.6215  loss_cate_5: 0  loss_mask_5: 1.302  loss_dice_5: 1.3  loss_ce_6: 0.6383  loss_cate_6: 0  loss_mask_6: 1.288  loss_dice_6: 1.252  loss_ce_7: 0.6428  loss_cate_7: 0  loss_mask_7: 1.305  loss_dice_7: 1.273  loss_ce_8: 0.6447  loss_cate_8: 0  loss_mask_8: 1.29  loss_dice_8: 1.235  time: 1.1126  data_time: 0.0126  lr: 9.9844e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:24:11 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:44  iter: 159  total_loss: 35.07  loss_ce: 0.677  loss_cate: 0.7785  loss_mask: 1.092  loss_dice: 1.391  loss_ce_0: 2.702  loss_cate_0: 0  loss_mask_0: 1.046  loss_dice_0: 1.307  loss_ce_1: 0.9598  loss_cate_1: 0  loss_mask_1: 1.099  loss_dice_1: 1.382  loss_ce_2: 0.8313  loss_cate_2: 0  loss_mask_2: 1.106  loss_dice_2: 1.28  loss_ce_3: 0.7326  loss_cate_3: 0  loss_mask_3: 1.143  loss_dice_3: 1.291  loss_ce_4: 0.717  loss_cate_4: 0  loss_mask_4: 1.08  loss_dice_4: 1.304  loss_ce_5: 0.7874  loss_cate_5: 0  loss_mask_5: 1.145  loss_dice_5: 1.32  loss_ce_6: 0.6837  loss_cate_6: 0  loss_mask_6: 1.124  loss_dice_6: 1.348  loss_ce_7: 0.6678  loss_cate_7: 0  loss_mask_7: 1.09  loss_dice_7: 1.351  loss_ce_8: 0.6572  loss_cate_8: 0  loss_mask_8: 1.103  loss_dice_8: 1.377  time: 1.1107  data_time: 0.0135  lr: 9.9821e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:24:33 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:05  iter: 179  total_loss: 33.98  loss_ce: 0.555  loss_cate: 0.7408  loss_mask: 1.159  loss_dice: 1.33  loss_ce_0: 2.522  loss_cate_0: 0  loss_mask_0: 1.167  loss_dice_0: 1.36  loss_ce_1: 0.8366  loss_cate_1: 0  loss_mask_1: 1.077  loss_dice_1: 1.348  loss_ce_2: 0.6462  loss_cate_2: 0  loss_mask_2: 1.106  loss_dice_2: 1.376  loss_ce_3: 0.6327  loss_cate_3: 0  loss_mask_3: 1.14  loss_dice_3: 1.35  loss_ce_4: 0.5768  loss_cate_4: 0  loss_mask_4: 1.148  loss_dice_4: 1.33  loss_ce_5: 0.6515  loss_cate_5: 0  loss_mask_5: 1.183  loss_dice_5: 1.362  loss_ce_6: 0.5789  loss_cate_6: 0  loss_mask_6: 1.178  loss_dice_6: 1.345  loss_ce_7: 0.5348  loss_cate_7: 0  loss_mask_7: 1.176  loss_dice_7: 1.376  loss_ce_8: 0.5937  loss_cate_8: 0  loss_mask_8: 1.175  loss_dice_8: 1.306  time: 1.1093  data_time: 0.0134  lr: 9.9799e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:24:56 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:37  iter: 199  total_loss: 35.3  loss_ce: 0.5669  loss_cate: 0.7156  loss_mask: 1.252  loss_dice: 1.375  loss_ce_0: 2.475  loss_cate_0: 0  loss_mask_0: 1.256  loss_dice_0: 1.355  loss_ce_1: 0.7438  loss_cate_1: 0  loss_mask_1: 1.253  loss_dice_1: 1.378  loss_ce_2: 0.6286  loss_cate_2: 0  loss_mask_2: 1.304  loss_dice_2: 1.376  loss_ce_3: 0.6269  loss_cate_3: 0  loss_mask_3: 1.237  loss_dice_3: 1.34  loss_ce_4: 0.5904  loss_cate_4: 0  loss_mask_4: 1.272  loss_dice_4: 1.43  loss_ce_5: 0.5682  loss_cate_5: 0  loss_mask_5: 1.353  loss_dice_5: 1.417  loss_ce_6: 0.5355  loss_cate_6: 0  loss_mask_6: 1.275  loss_dice_6: 1.366  loss_ce_7: 0.5821  loss_cate_7: 0  loss_mask_7: 1.273  loss_dice_7: 1.426  loss_ce_8: 0.5081  loss_cate_8: 0  loss_mask_8: 1.272  loss_dice_8: 1.418  time: 1.1080  data_time: 0.0139  lr: 9.9776e-06  max_mem: 28906M\n",
      "\u001b[32m[10/12 11:25:18 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:36  iter: 219  total_loss: 33.01  loss_ce: 0.5188  loss_cate: 0.6971  loss_mask: 1.171  loss_dice: 1.259  loss_ce_0: 2.35  loss_cate_0: 0  loss_mask_0: 1.213  loss_dice_0: 1.299  loss_ce_1: 0.7124  loss_cate_1: 0  loss_mask_1: 1.251  loss_dice_1: 1.335  loss_ce_2: 0.6034  loss_cate_2: 0  loss_mask_2: 1.185  loss_dice_2: 1.307  loss_ce_3: 0.5563  loss_cate_3: 0  loss_mask_3: 1.17  loss_dice_3: 1.282  loss_ce_4: 0.5829  loss_cate_4: 0  loss_mask_4: 1.204  loss_dice_4: 1.319  loss_ce_5: 0.5851  loss_cate_5: 0  loss_mask_5: 1.201  loss_dice_5: 1.284  loss_ce_6: 0.5176  loss_cate_6: 0  loss_mask_6: 1.196  loss_dice_6: 1.247  loss_ce_7: 0.5233  loss_cate_7: 0  loss_mask_7: 1.219  loss_dice_7: 1.276  loss_ce_8: 0.5133  loss_cate_8: 0  loss_mask_8: 1.175  loss_dice_8: 1.229  time: 1.1074  data_time: 0.0134  lr: 9.9754e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:25:40 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:08  iter: 239  total_loss: 32.19  loss_ce: 0.5292  loss_cate: 0.6311  loss_mask: 1.143  loss_dice: 1.346  loss_ce_0: 2.202  loss_cate_0: 0  loss_mask_0: 1.114  loss_dice_0: 1.298  loss_ce_1: 0.6871  loss_cate_1: 0  loss_mask_1: 1.089  loss_dice_1: 1.392  loss_ce_2: 0.5567  loss_cate_2: 0  loss_mask_2: 1.125  loss_dice_2: 1.375  loss_ce_3: 0.5755  loss_cate_3: 0  loss_mask_3: 1.113  loss_dice_3: 1.393  loss_ce_4: 0.5039  loss_cate_4: 0  loss_mask_4: 1.134  loss_dice_4: 1.41  loss_ce_5: 0.509  loss_cate_5: 0  loss_mask_5: 1.104  loss_dice_5: 1.363  loss_ce_6: 0.5667  loss_cate_6: 0  loss_mask_6: 1.134  loss_dice_6: 1.391  loss_ce_7: 0.5677  loss_cate_7: 0  loss_mask_7: 1.122  loss_dice_7: 1.382  loss_ce_8: 0.5592  loss_cate_8: 0  loss_mask_8: 1.155  loss_dice_8: 1.368  time: 1.1063  data_time: 0.0130  lr: 9.9731e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:26:02 d2.utils.events]: \u001b[0m eta: 1 day, 0:16:40  iter: 259  total_loss: 32.72  loss_ce: 0.5669  loss_cate: 0.6633  loss_mask: 1.034  loss_dice: 1.286  loss_ce_0: 2.071  loss_cate_0: 0  loss_mask_0: 1.093  loss_dice_0: 1.29  loss_ce_1: 0.6364  loss_cate_1: 0  loss_mask_1: 1.096  loss_dice_1: 1.425  loss_ce_2: 0.6326  loss_cate_2: 0  loss_mask_2: 1.011  loss_dice_2: 1.314  loss_ce_3: 0.596  loss_cate_3: 0  loss_mask_3: 0.9969  loss_dice_3: 1.328  loss_ce_4: 0.6309  loss_cate_4: 0  loss_mask_4: 1.01  loss_dice_4: 1.317  loss_ce_5: 0.5256  loss_cate_5: 0  loss_mask_5: 1.037  loss_dice_5: 1.3  loss_ce_6: 0.5565  loss_cate_6: 0  loss_mask_6: 1.001  loss_dice_6: 1.309  loss_ce_7: 0.5685  loss_cate_7: 0  loss_mask_7: 1.038  loss_dice_7: 1.313  loss_ce_8: 0.5644  loss_cate_8: 0  loss_mask_8: 1.027  loss_dice_8: 1.318  time: 1.1060  data_time: 0.0137  lr: 9.9709e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:26:25 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:51  iter: 279  total_loss: 33.57  loss_ce: 0.5526  loss_cate: 0.5894  loss_mask: 1.129  loss_dice: 1.386  loss_ce_0: 2.048  loss_cate_0: 0  loss_mask_0: 1.105  loss_dice_0: 1.428  loss_ce_1: 0.7084  loss_cate_1: 0  loss_mask_1: 1.172  loss_dice_1: 1.419  loss_ce_2: 0.6174  loss_cate_2: 0  loss_mask_2: 1.131  loss_dice_2: 1.402  loss_ce_3: 0.6017  loss_cate_3: 0  loss_mask_3: 1.049  loss_dice_3: 1.424  loss_ce_4: 0.5352  loss_cate_4: 0  loss_mask_4: 1.08  loss_dice_4: 1.442  loss_ce_5: 0.505  loss_cate_5: 0  loss_mask_5: 1.092  loss_dice_5: 1.462  loss_ce_6: 0.5502  loss_cate_6: 0  loss_mask_6: 1.102  loss_dice_6: 1.424  loss_ce_7: 0.5494  loss_cate_7: 0  loss_mask_7: 1.121  loss_dice_7: 1.401  loss_ce_8: 0.5634  loss_cate_8: 0  loss_mask_8: 1.103  loss_dice_8: 1.407  time: 1.1050  data_time: 0.0132  lr: 9.9686e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:26:47 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:40  iter: 299  total_loss: 33.64  loss_ce: 0.5211  loss_cate: 0.5934  loss_mask: 1.276  loss_dice: 1.315  loss_ce_0: 1.99  loss_cate_0: 0  loss_mask_0: 1.296  loss_dice_0: 1.325  loss_ce_1: 0.7123  loss_cate_1: 0  loss_mask_1: 1.256  loss_dice_1: 1.423  loss_ce_2: 0.7205  loss_cate_2: 0  loss_mask_2: 1.311  loss_dice_2: 1.385  loss_ce_3: 0.617  loss_cate_3: 0  loss_mask_3: 1.294  loss_dice_3: 1.358  loss_ce_4: 0.5747  loss_cate_4: 0  loss_mask_4: 1.265  loss_dice_4: 1.329  loss_ce_5: 0.5616  loss_cate_5: 0  loss_mask_5: 1.319  loss_dice_5: 1.319  loss_ce_6: 0.5342  loss_cate_6: 0  loss_mask_6: 1.29  loss_dice_6: 1.357  loss_ce_7: 0.5205  loss_cate_7: 0  loss_mask_7: 1.287  loss_dice_7: 1.341  loss_ce_8: 0.518  loss_cate_8: 0  loss_mask_8: 1.269  loss_dice_8: 1.333  time: 1.1048  data_time: 0.0139  lr: 9.9664e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:27:10 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:12  iter: 319  total_loss: 32.32  loss_ce: 0.507  loss_cate: 0.6277  loss_mask: 1.091  loss_dice: 1.295  loss_ce_0: 1.815  loss_cate_0: 0  loss_mask_0: 0.974  loss_dice_0: 1.274  loss_ce_1: 0.6061  loss_cate_1: 0  loss_mask_1: 1.149  loss_dice_1: 1.325  loss_ce_2: 0.4956  loss_cate_2: 0  loss_mask_2: 1.136  loss_dice_2: 1.261  loss_ce_3: 0.5221  loss_cate_3: 0  loss_mask_3: 1.136  loss_dice_3: 1.26  loss_ce_4: 0.4727  loss_cate_4: 0  loss_mask_4: 1.149  loss_dice_4: 1.306  loss_ce_5: 0.4391  loss_cate_5: 0  loss_mask_5: 1.155  loss_dice_5: 1.308  loss_ce_6: 0.5304  loss_cate_6: 0  loss_mask_6: 1.114  loss_dice_6: 1.316  loss_ce_7: 0.5067  loss_cate_7: 0  loss_mask_7: 1.166  loss_dice_7: 1.299  loss_ce_8: 0.5234  loss_cate_8: 0  loss_mask_8: 1.17  loss_dice_8: 1.285  time: 1.1044  data_time: 0.0136  lr: 9.9641e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:27:32 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:29  iter: 339  total_loss: 30.02  loss_ce: 0.3846  loss_cate: 0.5347  loss_mask: 1.073  loss_dice: 1.267  loss_ce_0: 1.69  loss_cate_0: 0  loss_mask_0: 1.031  loss_dice_0: 1.224  loss_ce_1: 0.5484  loss_cate_1: 0  loss_mask_1: 1.102  loss_dice_1: 1.301  loss_ce_2: 0.4254  loss_cate_2: 0  loss_mask_2: 1.093  loss_dice_2: 1.289  loss_ce_3: 0.4371  loss_cate_3: 0  loss_mask_3: 1.082  loss_dice_3: 1.24  loss_ce_4: 0.4153  loss_cate_4: 0  loss_mask_4: 1.047  loss_dice_4: 1.252  loss_ce_5: 0.3597  loss_cate_5: 0  loss_mask_5: 1.079  loss_dice_5: 1.275  loss_ce_6: 0.3723  loss_cate_6: 0  loss_mask_6: 1.051  loss_dice_6: 1.284  loss_ce_7: 0.371  loss_cate_7: 0  loss_mask_7: 1.072  loss_dice_7: 1.264  loss_ce_8: 0.3698  loss_cate_8: 0  loss_mask_8: 1.077  loss_dice_8: 1.268  time: 1.1041  data_time: 0.0128  lr: 9.9619e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:27:54 d2.utils.events]: \u001b[0m eta: 1 day, 0:14:28  iter: 359  total_loss: 31.06  loss_ce: 0.4435  loss_cate: 0.5137  loss_mask: 1.173  loss_dice: 1.412  loss_ce_0: 1.572  loss_cate_0: 0  loss_mask_0: 1.156  loss_dice_0: 1.242  loss_ce_1: 0.5613  loss_cate_1: 0  loss_mask_1: 1.111  loss_dice_1: 1.341  loss_ce_2: 0.4934  loss_cate_2: 0  loss_mask_2: 1.153  loss_dice_2: 1.352  loss_ce_3: 0.5202  loss_cate_3: 0  loss_mask_3: 1.139  loss_dice_3: 1.384  loss_ce_4: 0.5115  loss_cate_4: 0  loss_mask_4: 1.142  loss_dice_4: 1.39  loss_ce_5: 0.4967  loss_cate_5: 0  loss_mask_5: 1.135  loss_dice_5: 1.406  loss_ce_6: 0.5253  loss_cate_6: 0  loss_mask_6: 1.161  loss_dice_6: 1.397  loss_ce_7: 0.5023  loss_cate_7: 0  loss_mask_7: 1.159  loss_dice_7: 1.37  loss_ce_8: 0.4767  loss_cate_8: 0  loss_mask_8: 1.185  loss_dice_8: 1.414  time: 1.1032  data_time: 0.0133  lr: 9.9596e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:28:17 d2.utils.events]: \u001b[0m eta: 1 day, 0:15:18  iter: 379  total_loss: 31.78  loss_ce: 0.4112  loss_cate: 0.5121  loss_mask: 1.175  loss_dice: 1.451  loss_ce_0: 1.543  loss_cate_0: 0  loss_mask_0: 1.293  loss_dice_0: 1.442  loss_ce_1: 0.5155  loss_cate_1: 0  loss_mask_1: 1.138  loss_dice_1: 1.461  loss_ce_2: 0.5091  loss_cate_2: 0  loss_mask_2: 1.114  loss_dice_2: 1.402  loss_ce_3: 0.4386  loss_cate_3: 0  loss_mask_3: 1.145  loss_dice_3: 1.404  loss_ce_4: 0.4026  loss_cate_4: 0  loss_mask_4: 1.196  loss_dice_4: 1.443  loss_ce_5: 0.3643  loss_cate_5: 0  loss_mask_5: 1.189  loss_dice_5: 1.453  loss_ce_6: 0.3745  loss_cate_6: 0  loss_mask_6: 1.159  loss_dice_6: 1.458  loss_ce_7: 0.4268  loss_cate_7: 0  loss_mask_7: 1.153  loss_dice_7: 1.469  loss_ce_8: 0.4216  loss_cate_8: 0  loss_mask_8: 1.155  loss_dice_8: 1.461  time: 1.1034  data_time: 0.0138  lr: 9.9574e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:28:39 d2.utils.events]: \u001b[0m eta: 1 day, 0:14:37  iter: 399  total_loss: 30.85  loss_ce: 0.4074  loss_cate: 0.4925  loss_mask: 1.196  loss_dice: 1.375  loss_ce_0: 1.387  loss_cate_0: 0  loss_mask_0: 1.183  loss_dice_0: 1.425  loss_ce_1: 0.4991  loss_cate_1: 0  loss_mask_1: 1.141  loss_dice_1: 1.357  loss_ce_2: 0.3965  loss_cate_2: 0  loss_mask_2: 1.136  loss_dice_2: 1.365  loss_ce_3: 0.3787  loss_cate_3: 0  loss_mask_3: 1.13  loss_dice_3: 1.341  loss_ce_4: 0.4025  loss_cate_4: 0  loss_mask_4: 1.142  loss_dice_4: 1.34  loss_ce_5: 0.3994  loss_cate_5: 0  loss_mask_5: 1.169  loss_dice_5: 1.317  loss_ce_6: 0.3589  loss_cate_6: 0  loss_mask_6: 1.182  loss_dice_6: 1.376  loss_ce_7: 0.4165  loss_cate_7: 0  loss_mask_7: 1.178  loss_dice_7: 1.386  loss_ce_8: 0.3799  loss_cate_8: 0  loss_mask_8: 1.211  loss_dice_8: 1.346  time: 1.1033  data_time: 0.0135  lr: 9.9551e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:29:02 d2.utils.events]: \u001b[0m eta: 1 day, 0:14:34  iter: 419  total_loss: 30.61  loss_ce: 0.4789  loss_cate: 0.4259  loss_mask: 1.111  loss_dice: 1.297  loss_ce_0: 1.368  loss_cate_0: 0  loss_mask_0: 1.166  loss_dice_0: 1.269  loss_ce_1: 0.5352  loss_cate_1: 0  loss_mask_1: 1.126  loss_dice_1: 1.249  loss_ce_2: 0.4669  loss_cate_2: 0  loss_mask_2: 1.091  loss_dice_2: 1.345  loss_ce_3: 0.5108  loss_cate_3: 0  loss_mask_3: 1.095  loss_dice_3: 1.311  loss_ce_4: 0.4924  loss_cate_4: 0  loss_mask_4: 1.103  loss_dice_4: 1.28  loss_ce_5: 0.4504  loss_cate_5: 0  loss_mask_5: 1.087  loss_dice_5: 1.264  loss_ce_6: 0.4623  loss_cate_6: 0  loss_mask_6: 1.104  loss_dice_6: 1.266  loss_ce_7: 0.5068  loss_cate_7: 0  loss_mask_7: 1.065  loss_dice_7: 1.332  loss_ce_8: 0.5182  loss_cate_8: 0  loss_mask_8: 1.094  loss_dice_8: 1.283  time: 1.1030  data_time: 0.0127  lr: 9.9529e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:29:24 d2.utils.events]: \u001b[0m eta: 1 day, 0:14:23  iter: 439  total_loss: 30.47  loss_ce: 0.4747  loss_cate: 0.4668  loss_mask: 1.15  loss_dice: 1.259  loss_ce_0: 1.284  loss_cate_0: 0  loss_mask_0: 1.09  loss_dice_0: 1.328  loss_ce_1: 0.5294  loss_cate_1: 0  loss_mask_1: 1.12  loss_dice_1: 1.318  loss_ce_2: 0.4198  loss_cate_2: 0  loss_mask_2: 1.1  loss_dice_2: 1.294  loss_ce_3: 0.5134  loss_cate_3: 0  loss_mask_3: 1.142  loss_dice_3: 1.35  loss_ce_4: 0.4587  loss_cate_4: 0  loss_mask_4: 1.081  loss_dice_4: 1.372  loss_ce_5: 0.5082  loss_cate_5: 0  loss_mask_5: 1.028  loss_dice_5: 1.266  loss_ce_6: 0.5096  loss_cate_6: 0  loss_mask_6: 1.182  loss_dice_6: 1.249  loss_ce_7: 0.4697  loss_cate_7: 0  loss_mask_7: 1.165  loss_dice_7: 1.223  loss_ce_8: 0.449  loss_cate_8: 0  loss_mask_8: 1.2  loss_dice_8: 1.26  time: 1.1028  data_time: 0.0124  lr: 9.9506e-06  max_mem: 28910M\n",
      "\u001b[32m[10/12 11:29:46 d2.utils.events]: \u001b[0m eta: 1 day, 0:14:24  iter: 459  total_loss: 30.25  loss_ce: 0.4985  loss_cate: 0.4301  loss_mask: 1.027  loss_dice: 1.304  loss_ce_0: 1.327  loss_cate_0: 0  loss_mask_0: 0.9585  loss_dice_0: 1.34  loss_ce_1: 0.586  loss_cate_1: 0  loss_mask_1: 0.9962  loss_dice_1: 1.269  loss_ce_2: 0.5094  loss_cate_2: 0  loss_mask_2: 1.026  loss_dice_2: 1.334  loss_ce_3: 0.551  loss_cate_3: 0  loss_mask_3: 0.9944  loss_dice_3: 1.289  loss_ce_4: 0.5881  loss_cate_4: 0  loss_mask_4: 1.031  loss_dice_4: 1.328  loss_ce_5: 0.5136  loss_cate_5: 0  loss_mask_5: 1.001  loss_dice_5: 1.316  loss_ce_6: 0.5128  loss_cate_6: 0  loss_mask_6: 1.052  loss_dice_6: 1.297  loss_ce_7: 0.4736  loss_cate_7: 0  loss_mask_7: 1.025  loss_dice_7: 1.33  loss_ce_8: 0.497  loss_cate_8: 0  loss_mask_8: 1.01  loss_dice_8: 1.316  time: 1.1032  data_time: 0.0163  lr: 9.9483e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:30:09 d2.utils.events]: \u001b[0m eta: 1 day, 0:13:45  iter: 479  total_loss: 29.66  loss_ce: 0.4515  loss_cate: 0.4311  loss_mask: 1.114  loss_dice: 1.215  loss_ce_0: 1.225  loss_cate_0: 0  loss_mask_0: 1.008  loss_dice_0: 1.278  loss_ce_1: 0.5605  loss_cate_1: 0  loss_mask_1: 1.047  loss_dice_1: 1.336  loss_ce_2: 0.5689  loss_cate_2: 0  loss_mask_2: 1.112  loss_dice_2: 1.265  loss_ce_3: 0.5343  loss_cate_3: 0  loss_mask_3: 1.114  loss_dice_3: 1.285  loss_ce_4: 0.6001  loss_cate_4: 0  loss_mask_4: 1.049  loss_dice_4: 1.278  loss_ce_5: 0.5362  loss_cate_5: 0  loss_mask_5: 1.093  loss_dice_5: 1.209  loss_ce_6: 0.5122  loss_cate_6: 0  loss_mask_6: 1.117  loss_dice_6: 1.244  loss_ce_7: 0.4684  loss_cate_7: 0  loss_mask_7: 1.124  loss_dice_7: 1.227  loss_ce_8: 0.4739  loss_cate_8: 0  loss_mask_8: 1.104  loss_dice_8: 1.249  time: 1.1029  data_time: 0.0125  lr: 9.9461e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:30:31 d2.utils.events]: \u001b[0m eta: 1 day, 0:13:21  iter: 499  total_loss: 27.28  loss_ce: 0.4041  loss_cate: 0.4121  loss_mask: 1.023  loss_dice: 1.22  loss_ce_0: 1.195  loss_cate_0: 0  loss_mask_0: 1.048  loss_dice_0: 1.148  loss_ce_1: 0.3912  loss_cate_1: 0  loss_mask_1: 1.104  loss_dice_1: 1.183  loss_ce_2: 0.4395  loss_cate_2: 0  loss_mask_2: 1.003  loss_dice_2: 1.14  loss_ce_3: 0.4484  loss_cate_3: 0  loss_mask_3: 0.9988  loss_dice_3: 1.158  loss_ce_4: 0.4356  loss_cate_4: 0  loss_mask_4: 1.014  loss_dice_4: 1.165  loss_ce_5: 0.4502  loss_cate_5: 0  loss_mask_5: 1.004  loss_dice_5: 1.155  loss_ce_6: 0.4304  loss_cate_6: 0  loss_mask_6: 1.023  loss_dice_6: 1.154  loss_ce_7: 0.4417  loss_cate_7: 0  loss_mask_7: 1.025  loss_dice_7: 1.132  loss_ce_8: 0.3969  loss_cate_8: 0  loss_mask_8: 1.017  loss_dice_8: 1.213  time: 1.1024  data_time: 0.0119  lr: 9.9438e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:30:53 d2.utils.events]: \u001b[0m eta: 1 day, 0:12:52  iter: 519  total_loss: 29.44  loss_ce: 0.4837  loss_cate: 0.4104  loss_mask: 1.049  loss_dice: 1.237  loss_ce_0: 1.134  loss_cate_0: 0  loss_mask_0: 1.015  loss_dice_0: 1.252  loss_ce_1: 0.5513  loss_cate_1: 0  loss_mask_1: 1.008  loss_dice_1: 1.386  loss_ce_2: 0.4905  loss_cate_2: 0  loss_mask_2: 0.9963  loss_dice_2: 1.274  loss_ce_3: 0.4387  loss_cate_3: 0  loss_mask_3: 1.018  loss_dice_3: 1.269  loss_ce_4: 0.4317  loss_cate_4: 0  loss_mask_4: 1.057  loss_dice_4: 1.273  loss_ce_5: 0.4196  loss_cate_5: 0  loss_mask_5: 1.047  loss_dice_5: 1.302  loss_ce_6: 0.4951  loss_cate_6: 0  loss_mask_6: 1.064  loss_dice_6: 1.253  loss_ce_7: 0.4934  loss_cate_7: 0  loss_mask_7: 1.044  loss_dice_7: 1.242  loss_ce_8: 0.5078  loss_cate_8: 0  loss_mask_8: 1.036  loss_dice_8: 1.236  time: 1.1022  data_time: 0.0126  lr: 9.9416e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:31:17 d2.utils.events]: \u001b[0m eta: 1 day, 0:12:49  iter: 539  total_loss: 28.58  loss_ce: 0.4441  loss_cate: 0.3869  loss_mask: 1.009  loss_dice: 1.296  loss_ce_0: 1.082  loss_cate_0: 0  loss_mask_0: 1.018  loss_dice_0: 1.307  loss_ce_1: 0.5039  loss_cate_1: 0  loss_mask_1: 1.036  loss_dice_1: 1.369  loss_ce_2: 0.4624  loss_cate_2: 0  loss_mask_2: 1.041  loss_dice_2: 1.33  loss_ce_3: 0.4645  loss_cate_3: 0  loss_mask_3: 1.042  loss_dice_3: 1.307  loss_ce_4: 0.486  loss_cate_4: 0  loss_mask_4: 1.05  loss_dice_4: 1.331  loss_ce_5: 0.4486  loss_cate_5: 0  loss_mask_5: 1.023  loss_dice_5: 1.295  loss_ce_6: 0.4352  loss_cate_6: 0  loss_mask_6: 1.014  loss_dice_6: 1.258  loss_ce_7: 0.4659  loss_cate_7: 0  loss_mask_7: 1.012  loss_dice_7: 1.3  loss_ce_8: 0.464  loss_cate_8: 0  loss_mask_8: 1.02  loss_dice_8: 1.297  time: 1.1024  data_time: 0.0143  lr: 9.9393e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:31:39 d2.utils.events]: \u001b[0m eta: 1 day, 0:12:34  iter: 559  total_loss: 28.82  loss_ce: 0.3573  loss_cate: 0.3782  loss_mask: 1.012  loss_dice: 1.279  loss_ce_0: 1.087  loss_cate_0: 0  loss_mask_0: 0.9956  loss_dice_0: 1.231  loss_ce_1: 0.4425  loss_cate_1: 0  loss_mask_1: 1.043  loss_dice_1: 1.257  loss_ce_2: 0.4651  loss_cate_2: 0  loss_mask_2: 1.013  loss_dice_2: 1.203  loss_ce_3: 0.4697  loss_cate_3: 0  loss_mask_3: 0.9925  loss_dice_3: 1.176  loss_ce_4: 0.3796  loss_cate_4: 0  loss_mask_4: 0.9742  loss_dice_4: 1.206  loss_ce_5: 0.384  loss_cate_5: 0  loss_mask_5: 0.9967  loss_dice_5: 1.257  loss_ce_6: 0.3842  loss_cate_6: 0  loss_mask_6: 0.9885  loss_dice_6: 1.244  loss_ce_7: 0.4179  loss_cate_7: 0  loss_mask_7: 0.9993  loss_dice_7: 1.262  loss_ce_8: 0.3799  loss_cate_8: 0  loss_mask_8: 1.037  loss_dice_8: 1.237  time: 1.1023  data_time: 0.0130  lr: 9.9371e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:32:03 d2.utils.events]: \u001b[0m eta: 1 day, 0:12:13  iter: 579  total_loss: 29.17  loss_ce: 0.3225  loss_cate: 0.3684  loss_mask: 1.036  loss_dice: 1.347  loss_ce_0: 1.023  loss_cate_0: 0  loss_mask_0: 1.049  loss_dice_0: 1.296  loss_ce_1: 0.335  loss_cate_1: 0  loss_mask_1: 1.039  loss_dice_1: 1.41  loss_ce_2: 0.3628  loss_cate_2: 0  loss_mask_2: 1.033  loss_dice_2: 1.31  loss_ce_3: 0.3531  loss_cate_3: 0  loss_mask_3: 1.061  loss_dice_3: 1.364  loss_ce_4: 0.3291  loss_cate_4: 0  loss_mask_4: 1.034  loss_dice_4: 1.33  loss_ce_5: 0.3078  loss_cate_5: 0  loss_mask_5: 1.02  loss_dice_5: 1.387  loss_ce_6: 0.3355  loss_cate_6: 0  loss_mask_6: 1.021  loss_dice_6: 1.349  loss_ce_7: 0.3051  loss_cate_7: 0  loss_mask_7: 0.99  loss_dice_7: 1.301  loss_ce_8: 0.2905  loss_cate_8: 0  loss_mask_8: 0.9971  loss_dice_8: 1.398  time: 1.1023  data_time: 0.0127  lr: 9.9348e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:32:26 d2.utils.events]: \u001b[0m eta: 1 day, 0:12:04  iter: 599  total_loss: 30.48  loss_ce: 0.3591  loss_cate: 0.3667  loss_mask: 1.094  loss_dice: 1.269  loss_ce_0: 1.043  loss_cate_0: 0  loss_mask_0: 0.9929  loss_dice_0: 1.25  loss_ce_1: 0.5102  loss_cate_1: 0  loss_mask_1: 1.233  loss_dice_1: 1.341  loss_ce_2: 0.5074  loss_cate_2: 0  loss_mask_2: 1.078  loss_dice_2: 1.278  loss_ce_3: 0.4857  loss_cate_3: 0  loss_mask_3: 1.106  loss_dice_3: 1.288  loss_ce_4: 0.4294  loss_cate_4: 0  loss_mask_4: 1.176  loss_dice_4: 1.288  loss_ce_5: 0.4396  loss_cate_5: 0  loss_mask_5: 1.078  loss_dice_5: 1.266  loss_ce_6: 0.3837  loss_cate_6: 0  loss_mask_6: 1.137  loss_dice_6: 1.291  loss_ce_7: 0.3845  loss_cate_7: 0  loss_mask_7: 1.101  loss_dice_7: 1.299  loss_ce_8: 0.3625  loss_cate_8: 0  loss_mask_8: 1.126  loss_dice_8: 1.279  time: 1.1023  data_time: 0.0134  lr: 9.9326e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:32:49 d2.utils.events]: \u001b[0m eta: 1 day, 0:11:43  iter: 619  total_loss: 28.51  loss_ce: 0.3213  loss_cate: 0.3689  loss_mask: 1.089  loss_dice: 1.285  loss_ce_0: 0.9047  loss_cate_0: 0  loss_mask_0: 1.15  loss_dice_0: 1.365  loss_ce_1: 0.3343  loss_cate_1: 0  loss_mask_1: 1.119  loss_dice_1: 1.394  loss_ce_2: 0.3021  loss_cate_2: 0  loss_mask_2: 1.124  loss_dice_2: 1.369  loss_ce_3: 0.309  loss_cate_3: 0  loss_mask_3: 1.126  loss_dice_3: 1.327  loss_ce_4: 0.33  loss_cate_4: 0  loss_mask_4: 1.13  loss_dice_4: 1.276  loss_ce_5: 0.3276  loss_cate_5: 0  loss_mask_5: 1.114  loss_dice_5: 1.276  loss_ce_6: 0.3434  loss_cate_6: 0  loss_mask_6: 1.1  loss_dice_6: 1.309  loss_ce_7: 0.3251  loss_cate_7: 0  loss_mask_7: 1.113  loss_dice_7: 1.321  loss_ce_8: 0.3081  loss_cate_8: 0  loss_mask_8: 1.09  loss_dice_8: 1.316  time: 1.1021  data_time: 0.0127  lr: 9.9303e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:33:11 d2.utils.events]: \u001b[0m eta: 1 day, 0:11:25  iter: 639  total_loss: 30.74  loss_ce: 0.3623  loss_cate: 0.4032  loss_mask: 1.144  loss_dice: 1.341  loss_ce_0: 0.9427  loss_cate_0: 0  loss_mask_0: 1.081  loss_dice_0: 1.347  loss_ce_1: 0.4205  loss_cate_1: 0  loss_mask_1: 1.002  loss_dice_1: 1.385  loss_ce_2: 0.4674  loss_cate_2: 0  loss_mask_2: 1.076  loss_dice_2: 1.35  loss_ce_3: 0.5094  loss_cate_3: 0  loss_mask_3: 1.18  loss_dice_3: 1.359  loss_ce_4: 0.5057  loss_cate_4: 0  loss_mask_4: 1.008  loss_dice_4: 1.336  loss_ce_5: 0.4818  loss_cate_5: 0  loss_mask_5: 1.192  loss_dice_5: 1.382  loss_ce_6: 0.4844  loss_cate_6: 0  loss_mask_6: 1.009  loss_dice_6: 1.374  loss_ce_7: 0.4628  loss_cate_7: 0  loss_mask_7: 0.9798  loss_dice_7: 1.377  loss_ce_8: 0.3873  loss_cate_8: 0  loss_mask_8: 1.06  loss_dice_8: 1.385  time: 1.1022  data_time: 0.0134  lr: 9.9281e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:33:34 d2.utils.events]: \u001b[0m eta: 1 day, 0:11:28  iter: 659  total_loss: 29.82  loss_ce: 0.3502  loss_cate: 0.3578  loss_mask: 1.108  loss_dice: 1.317  loss_ce_0: 0.9588  loss_cate_0: 0  loss_mask_0: 1.142  loss_dice_0: 1.356  loss_ce_1: 0.5725  loss_cate_1: 0  loss_mask_1: 1.135  loss_dice_1: 1.315  loss_ce_2: 0.4585  loss_cate_2: 0  loss_mask_2: 1.143  loss_dice_2: 1.358  loss_ce_3: 0.4435  loss_cate_3: 0  loss_mask_3: 1.182  loss_dice_3: 1.282  loss_ce_4: 0.4443  loss_cate_4: 0  loss_mask_4: 1.132  loss_dice_4: 1.286  loss_ce_5: 0.4543  loss_cate_5: 0  loss_mask_5: 1.096  loss_dice_5: 1.278  loss_ce_6: 0.429  loss_cate_6: 0  loss_mask_6: 1.145  loss_dice_6: 1.268  loss_ce_7: 0.4104  loss_cate_7: 0  loss_mask_7: 1.149  loss_dice_7: 1.331  loss_ce_8: 0.3708  loss_cate_8: 0  loss_mask_8: 1.084  loss_dice_8: 1.34  time: 1.1022  data_time: 0.0139  lr: 9.9258e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:33:56 d2.utils.events]: \u001b[0m eta: 1 day, 0:11:10  iter: 679  total_loss: 31.26  loss_ce: 0.4613  loss_cate: 0.3819  loss_mask: 1.127  loss_dice: 1.429  loss_ce_0: 0.9654  loss_cate_0: 0  loss_mask_0: 1.08  loss_dice_0: 1.502  loss_ce_1: 0.4794  loss_cate_1: 0  loss_mask_1: 1.074  loss_dice_1: 1.375  loss_ce_2: 0.5129  loss_cate_2: 0  loss_mask_2: 1.077  loss_dice_2: 1.322  loss_ce_3: 0.5837  loss_cate_3: 0  loss_mask_3: 1.117  loss_dice_3: 1.429  loss_ce_4: 0.5225  loss_cate_4: 0  loss_mask_4: 1.099  loss_dice_4: 1.372  loss_ce_5: 0.4734  loss_cate_5: 0  loss_mask_5: 1.075  loss_dice_5: 1.368  loss_ce_6: 0.5544  loss_cate_6: 0  loss_mask_6: 1.038  loss_dice_6: 1.349  loss_ce_7: 0.4585  loss_cate_7: 0  loss_mask_7: 1.063  loss_dice_7: 1.437  loss_ce_8: 0.4313  loss_cate_8: 0  loss_mask_8: 1.113  loss_dice_8: 1.45  time: 1.1022  data_time: 0.0142  lr: 9.9236e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:34:18 d2.utils.events]: \u001b[0m eta: 1 day, 0:10:41  iter: 699  total_loss: 26.64  loss_ce: 0.3089  loss_cate: 0.3223  loss_mask: 1.027  loss_dice: 1.155  loss_ce_0: 0.8447  loss_cate_0: 0  loss_mask_0: 1.073  loss_dice_0: 1.181  loss_ce_1: 0.283  loss_cate_1: 0  loss_mask_1: 1.048  loss_dice_1: 1.2  loss_ce_2: 0.3171  loss_cate_2: 0  loss_mask_2: 1.029  loss_dice_2: 1.182  loss_ce_3: 0.3273  loss_cate_3: 0  loss_mask_3: 1.026  loss_dice_3: 1.149  loss_ce_4: 0.3183  loss_cate_4: 0  loss_mask_4: 1.018  loss_dice_4: 1.149  loss_ce_5: 0.3183  loss_cate_5: 0  loss_mask_5: 0.9855  loss_dice_5: 1.136  loss_ce_6: 0.2881  loss_cate_6: 0  loss_mask_6: 1.038  loss_dice_6: 1.155  loss_ce_7: 0.2829  loss_cate_7: 0  loss_mask_7: 1.073  loss_dice_7: 1.155  loss_ce_8: 0.2852  loss_cate_8: 0  loss_mask_8: 0.9999  loss_dice_8: 1.156  time: 1.1018  data_time: 0.0127  lr: 9.9213e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:34:41 d2.utils.events]: \u001b[0m eta: 1 day, 0:10:26  iter: 719  total_loss: 27.23  loss_ce: 0.3907  loss_cate: 0.3448  loss_mask: 1.089  loss_dice: 1.193  loss_ce_0: 0.8226  loss_cate_0: 0  loss_mask_0: 1.09  loss_dice_0: 1.228  loss_ce_1: 0.4493  loss_cate_1: 0  loss_mask_1: 1.078  loss_dice_1: 1.22  loss_ce_2: 0.455  loss_cate_2: 0  loss_mask_2: 1.045  loss_dice_2: 1.182  loss_ce_3: 0.508  loss_cate_3: 0  loss_mask_3: 1.049  loss_dice_3: 1.197  loss_ce_4: 0.4226  loss_cate_4: 0  loss_mask_4: 1.035  loss_dice_4: 1.237  loss_ce_5: 0.4176  loss_cate_5: 0  loss_mask_5: 1.076  loss_dice_5: 1.225  loss_ce_6: 0.3894  loss_cate_6: 0  loss_mask_6: 1.052  loss_dice_6: 1.231  loss_ce_7: 0.406  loss_cate_7: 0  loss_mask_7: 1.055  loss_dice_7: 1.234  loss_ce_8: 0.388  loss_cate_8: 0  loss_mask_8: 1.019  loss_dice_8: 1.226  time: 1.1018  data_time: 0.0143  lr: 9.9191e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:35:03 d2.utils.events]: \u001b[0m eta: 1 day, 0:09:58  iter: 739  total_loss: 27.9  loss_ce: 0.4424  loss_cate: 0.393  loss_mask: 1.031  loss_dice: 1.182  loss_ce_0: 0.8095  loss_cate_0: 0  loss_mask_0: 1.02  loss_dice_0: 1.15  loss_ce_1: 0.519  loss_cate_1: 0  loss_mask_1: 0.999  loss_dice_1: 1.099  loss_ce_2: 0.4516  loss_cate_2: 0  loss_mask_2: 1.05  loss_dice_2: 1.129  loss_ce_3: 0.4285  loss_cate_3: 0  loss_mask_3: 1.062  loss_dice_3: 1.165  loss_ce_4: 0.4156  loss_cate_4: 0  loss_mask_4: 1.083  loss_dice_4: 1.182  loss_ce_5: 0.3818  loss_cate_5: 0  loss_mask_5: 1.047  loss_dice_5: 1.2  loss_ce_6: 0.438  loss_cate_6: 0  loss_mask_6: 1.051  loss_dice_6: 1.173  loss_ce_7: 0.433  loss_cate_7: 0  loss_mask_7: 1.061  loss_dice_7: 1.265  loss_ce_8: 0.377  loss_cate_8: 0  loss_mask_8: 1.014  loss_dice_8: 1.249  time: 1.1016  data_time: 0.0129  lr: 9.9168e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:35:26 d2.utils.events]: \u001b[0m eta: 1 day, 0:09:13  iter: 759  total_loss: 27.59  loss_ce: 0.3919  loss_cate: 0.3118  loss_mask: 0.8965  loss_dice: 1.218  loss_ce_0: 0.8955  loss_cate_0: 0  loss_mask_0: 0.9875  loss_dice_0: 1.312  loss_ce_1: 0.5187  loss_cate_1: 0  loss_mask_1: 1.005  loss_dice_1: 1.205  loss_ce_2: 0.418  loss_cate_2: 0  loss_mask_2: 1.018  loss_dice_2: 1.273  loss_ce_3: 0.3994  loss_cate_3: 0  loss_mask_3: 0.9987  loss_dice_3: 1.246  loss_ce_4: 0.3901  loss_cate_4: 0  loss_mask_4: 0.9664  loss_dice_4: 1.259  loss_ce_5: 0.3566  loss_cate_5: 0  loss_mask_5: 0.9396  loss_dice_5: 1.273  loss_ce_6: 0.3912  loss_cate_6: 0  loss_mask_6: 0.9249  loss_dice_6: 1.288  loss_ce_7: 0.385  loss_cate_7: 0  loss_mask_7: 0.9209  loss_dice_7: 1.228  loss_ce_8: 0.3903  loss_cate_8: 0  loss_mask_8: 0.8923  loss_dice_8: 1.261  time: 1.1014  data_time: 0.0137  lr: 9.9146e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:35:48 d2.utils.events]: \u001b[0m eta: 1 day, 0:08:48  iter: 779  total_loss: 26.08  loss_ce: 0.3967  loss_cate: 0.3296  loss_mask: 0.979  loss_dice: 1.109  loss_ce_0: 0.7829  loss_cate_0: 0  loss_mask_0: 0.8961  loss_dice_0: 1.149  loss_ce_1: 0.4377  loss_cate_1: 0  loss_mask_1: 0.9929  loss_dice_1: 1.17  loss_ce_2: 0.4003  loss_cate_2: 0  loss_mask_2: 0.9519  loss_dice_2: 1.117  loss_ce_3: 0.4304  loss_cate_3: 0  loss_mask_3: 1.002  loss_dice_3: 1.113  loss_ce_4: 0.4453  loss_cate_4: 0  loss_mask_4: 0.9702  loss_dice_4: 1.105  loss_ce_5: 0.441  loss_cate_5: 0  loss_mask_5: 0.9466  loss_dice_5: 1.119  loss_ce_6: 0.388  loss_cate_6: 0  loss_mask_6: 0.9577  loss_dice_6: 1.102  loss_ce_7: 0.405  loss_cate_7: 0  loss_mask_7: 0.9533  loss_dice_7: 1.109  loss_ce_8: 0.4329  loss_cate_8: 0  loss_mask_8: 0.9716  loss_dice_8: 1.117  time: 1.1014  data_time: 0.0144  lr: 9.9123e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:36:11 d2.utils.events]: \u001b[0m eta: 1 day, 0:08:35  iter: 799  total_loss: 29.49  loss_ce: 0.3974  loss_cate: 0.3156  loss_mask: 1.096  loss_dice: 1.353  loss_ce_0: 0.7222  loss_cate_0: 0  loss_mask_0: 1.122  loss_dice_0: 1.341  loss_ce_1: 0.5253  loss_cate_1: 0  loss_mask_1: 1.083  loss_dice_1: 1.344  loss_ce_2: 0.482  loss_cate_2: 0  loss_mask_2: 1.13  loss_dice_2: 1.358  loss_ce_3: 0.4307  loss_cate_3: 0  loss_mask_3: 1.106  loss_dice_3: 1.383  loss_ce_4: 0.4849  loss_cate_4: 0  loss_mask_4: 1.108  loss_dice_4: 1.372  loss_ce_5: 0.4355  loss_cate_5: 0  loss_mask_5: 1.094  loss_dice_5: 1.334  loss_ce_6: 0.4787  loss_cate_6: 0  loss_mask_6: 1.115  loss_dice_6: 1.324  loss_ce_7: 0.3904  loss_cate_7: 0  loss_mask_7: 1.095  loss_dice_7: 1.364  loss_ce_8: 0.4115  loss_cate_8: 0  loss_mask_8: 1.104  loss_dice_8: 1.365  time: 1.1014  data_time: 0.0134  lr: 9.9101e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:36:33 d2.utils.events]: \u001b[0m eta: 1 day, 0:08:38  iter: 819  total_loss: 26.46  loss_ce: 0.3628  loss_cate: 0.3125  loss_mask: 0.956  loss_dice: 1.147  loss_ce_0: 0.7302  loss_cate_0: 0  loss_mask_0: 0.9837  loss_dice_0: 1.215  loss_ce_1: 0.4507  loss_cate_1: 0  loss_mask_1: 0.9894  loss_dice_1: 1.171  loss_ce_2: 0.4427  loss_cate_2: 0  loss_mask_2: 1.066  loss_dice_2: 1.205  loss_ce_3: 0.3838  loss_cate_3: 0  loss_mask_3: 0.9999  loss_dice_3: 1.185  loss_ce_4: 0.3627  loss_cate_4: 0  loss_mask_4: 1.01  loss_dice_4: 1.165  loss_ce_5: 0.3881  loss_cate_5: 0  loss_mask_5: 0.959  loss_dice_5: 1.142  loss_ce_6: 0.4243  loss_cate_6: 0  loss_mask_6: 0.9835  loss_dice_6: 1.136  loss_ce_7: 0.3784  loss_cate_7: 0  loss_mask_7: 1.017  loss_dice_7: 1.148  loss_ce_8: 0.414  loss_cate_8: 0  loss_mask_8: 0.9637  loss_dice_8: 1.135  time: 1.1016  data_time: 0.0144  lr: 9.9078e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:36:56 d2.utils.events]: \u001b[0m eta: 1 day, 0:08:18  iter: 839  total_loss: 23.52  loss_ce: 0.4052  loss_cate: 0.2624  loss_mask: 0.8933  loss_dice: 0.9965  loss_ce_0: 0.7489  loss_cate_0: 0  loss_mask_0: 0.8438  loss_dice_0: 1.161  loss_ce_1: 0.4073  loss_cate_1: 0  loss_mask_1: 0.8792  loss_dice_1: 1.143  loss_ce_2: 0.4021  loss_cate_2: 0  loss_mask_2: 0.8477  loss_dice_2: 1.06  loss_ce_3: 0.4093  loss_cate_3: 0  loss_mask_3: 0.8425  loss_dice_3: 0.9987  loss_ce_4: 0.3814  loss_cate_4: 0  loss_mask_4: 0.8721  loss_dice_4: 1.041  loss_ce_5: 0.3744  loss_cate_5: 0  loss_mask_5: 0.8192  loss_dice_5: 1.044  loss_ce_6: 0.4516  loss_cate_6: 0  loss_mask_6: 0.8046  loss_dice_6: 1.001  loss_ce_7: 0.4026  loss_cate_7: 0  loss_mask_7: 0.8914  loss_dice_7: 0.9968  loss_ce_8: 0.3946  loss_cate_8: 0  loss_mask_8: 0.9014  loss_dice_8: 1.017  time: 1.1017  data_time: 0.0126  lr: 9.9056e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:37:18 d2.utils.events]: \u001b[0m eta: 1 day, 0:07:58  iter: 859  total_loss: 26.73  loss_ce: 0.4553  loss_cate: 0.2991  loss_mask: 0.9416  loss_dice: 1.053  loss_ce_0: 0.8222  loss_cate_0: 0  loss_mask_0: 0.987  loss_dice_0: 1.172  loss_ce_1: 0.5122  loss_cate_1: 0  loss_mask_1: 0.964  loss_dice_1: 1.134  loss_ce_2: 0.4853  loss_cate_2: 0  loss_mask_2: 0.9271  loss_dice_2: 1.099  loss_ce_3: 0.481  loss_cate_3: 0  loss_mask_3: 0.9486  loss_dice_3: 1.085  loss_ce_4: 0.4429  loss_cate_4: 0  loss_mask_4: 0.9392  loss_dice_4: 1.142  loss_ce_5: 0.4681  loss_cate_5: 0  loss_mask_5: 0.9169  loss_dice_5: 1.134  loss_ce_6: 0.4543  loss_cate_6: 0  loss_mask_6: 0.9148  loss_dice_6: 1.101  loss_ce_7: 0.4441  loss_cate_7: 0  loss_mask_7: 0.9253  loss_dice_7: 1.124  loss_ce_8: 0.4458  loss_cate_8: 0  loss_mask_8: 0.9185  loss_dice_8: 1.054  time: 1.1018  data_time: 0.0137  lr: 9.9033e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:37:42 d2.utils.events]: \u001b[0m eta: 1 day, 0:07:38  iter: 879  total_loss: 26.33  loss_ce: 0.3105  loss_cate: 0.2885  loss_mask: 1.08  loss_dice: 1.271  loss_ce_0: 0.7107  loss_cate_0: 0  loss_mask_0: 0.9924  loss_dice_0: 1.139  loss_ce_1: 0.4164  loss_cate_1: 0  loss_mask_1: 0.9708  loss_dice_1: 1.171  loss_ce_2: 0.341  loss_cate_2: 0  loss_mask_2: 1.018  loss_dice_2: 1.211  loss_ce_3: 0.334  loss_cate_3: 0  loss_mask_3: 1.122  loss_dice_3: 1.198  loss_ce_4: 0.3603  loss_cate_4: 0  loss_mask_4: 1.073  loss_dice_4: 1.183  loss_ce_5: 0.3543  loss_cate_5: 0  loss_mask_5: 1.099  loss_dice_5: 1.226  loss_ce_6: 0.3268  loss_cate_6: 0  loss_mask_6: 1.078  loss_dice_6: 1.214  loss_ce_7: 0.3174  loss_cate_7: 0  loss_mask_7: 1.059  loss_dice_7: 1.251  loss_ce_8: 0.3072  loss_cate_8: 0  loss_mask_8: 1.016  loss_dice_8: 1.248  time: 1.1041  data_time: 0.1250  lr: 9.9011e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:38:05 d2.utils.events]: \u001b[0m eta: 1 day, 0:07:08  iter: 899  total_loss: 27.86  loss_ce: 0.3546  loss_cate: 0.3129  loss_mask: 0.992  loss_dice: 1.338  loss_ce_0: 0.746  loss_cate_0: 0  loss_mask_0: 1.124  loss_dice_0: 1.384  loss_ce_1: 0.4784  loss_cate_1: 0  loss_mask_1: 1.115  loss_dice_1: 1.345  loss_ce_2: 0.4088  loss_cate_2: 0  loss_mask_2: 1.068  loss_dice_2: 1.328  loss_ce_3: 0.3262  loss_cate_3: 0  loss_mask_3: 1.078  loss_dice_3: 1.322  loss_ce_4: 0.3949  loss_cate_4: 0  loss_mask_4: 1.046  loss_dice_4: 1.309  loss_ce_5: 0.3529  loss_cate_5: 0  loss_mask_5: 1.14  loss_dice_5: 1.291  loss_ce_6: 0.3277  loss_cate_6: 0  loss_mask_6: 1.131  loss_dice_6: 1.383  loss_ce_7: 0.3967  loss_cate_7: 0  loss_mask_7: 1.109  loss_dice_7: 1.346  loss_ce_8: 0.3615  loss_cate_8: 0  loss_mask_8: 1.036  loss_dice_8: 1.309  time: 1.1040  data_time: 0.0134  lr: 9.8988e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:38:27 d2.utils.events]: \u001b[0m eta: 1 day, 0:06:46  iter: 919  total_loss: 27.26  loss_ce: 0.2739  loss_cate: 0.311  loss_mask: 0.9447  loss_dice: 1.256  loss_ce_0: 0.6837  loss_cate_0: 0  loss_mask_0: 0.9941  loss_dice_0: 1.282  loss_ce_1: 0.38  loss_cate_1: 0  loss_mask_1: 0.9311  loss_dice_1: 1.231  loss_ce_2: 0.327  loss_cate_2: 0  loss_mask_2: 0.9196  loss_dice_2: 1.272  loss_ce_3: 0.3031  loss_cate_3: 0  loss_mask_3: 0.9407  loss_dice_3: 1.207  loss_ce_4: 0.3195  loss_cate_4: 0  loss_mask_4: 0.9403  loss_dice_4: 1.162  loss_ce_5: 0.2801  loss_cate_5: 0  loss_mask_5: 0.9465  loss_dice_5: 1.193  loss_ce_6: 0.3113  loss_cate_6: 0  loss_mask_6: 0.9398  loss_dice_6: 1.214  loss_ce_7: 0.2981  loss_cate_7: 0  loss_mask_7: 0.9381  loss_dice_7: 1.23  loss_ce_8: 0.3008  loss_cate_8: 0  loss_mask_8: 0.9362  loss_dice_8: 1.294  time: 1.1038  data_time: 0.0134  lr: 9.8966e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:38:50 d2.utils.events]: \u001b[0m eta: 1 day, 0:06:21  iter: 939  total_loss: 23.77  loss_ce: 0.3355  loss_cate: 0.2961  loss_mask: 0.9884  loss_dice: 1.138  loss_ce_0: 0.7284  loss_cate_0: 0  loss_mask_0: 1.001  loss_dice_0: 1.158  loss_ce_1: 0.3592  loss_cate_1: 0  loss_mask_1: 1.065  loss_dice_1: 1.192  loss_ce_2: 0.2994  loss_cate_2: 0  loss_mask_2: 1.071  loss_dice_2: 1.125  loss_ce_3: 0.3248  loss_cate_3: 0  loss_mask_3: 0.9727  loss_dice_3: 1.089  loss_ce_4: 0.3198  loss_cate_4: 0  loss_mask_4: 0.9566  loss_dice_4: 1.156  loss_ce_5: 0.2982  loss_cate_5: 0  loss_mask_5: 1.022  loss_dice_5: 1.153  loss_ce_6: 0.3329  loss_cate_6: 0  loss_mask_6: 1.021  loss_dice_6: 1.155  loss_ce_7: 0.3211  loss_cate_7: 0  loss_mask_7: 1.017  loss_dice_7: 1.155  loss_ce_8: 0.3318  loss_cate_8: 0  loss_mask_8: 1.032  loss_dice_8: 1.188  time: 1.1037  data_time: 0.0139  lr: 9.8943e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:39:12 d2.utils.events]: \u001b[0m eta: 1 day, 0:05:52  iter: 959  total_loss: 25.36  loss_ce: 0.5056  loss_cate: 0.3402  loss_mask: 0.8744  loss_dice: 1.143  loss_ce_0: 0.7522  loss_cate_0: 0  loss_mask_0: 0.9149  loss_dice_0: 1.183  loss_ce_1: 0.5934  loss_cate_1: 0  loss_mask_1: 0.8874  loss_dice_1: 1.168  loss_ce_2: 0.5342  loss_cate_2: 0  loss_mask_2: 0.8774  loss_dice_2: 1.177  loss_ce_3: 0.5732  loss_cate_3: 0  loss_mask_3: 0.8606  loss_dice_3: 1.154  loss_ce_4: 0.5058  loss_cate_4: 0  loss_mask_4: 0.9004  loss_dice_4: 1.161  loss_ce_5: 0.4842  loss_cate_5: 0  loss_mask_5: 0.8592  loss_dice_5: 1.181  loss_ce_6: 0.441  loss_cate_6: 0  loss_mask_6: 0.9039  loss_dice_6: 1.126  loss_ce_7: 0.4684  loss_cate_7: 0  loss_mask_7: 0.893  loss_dice_7: 1.165  loss_ce_8: 0.4822  loss_cate_8: 0  loss_mask_8: 0.8774  loss_dice_8: 1.143  time: 1.1035  data_time: 0.0130  lr: 9.892e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:39:34 d2.utils.events]: \u001b[0m eta: 1 day, 0:05:33  iter: 979  total_loss: 27.15  loss_ce: 0.4101  loss_cate: 0.2734  loss_mask: 0.9212  loss_dice: 1.393  loss_ce_0: 0.6579  loss_cate_0: 0  loss_mask_0: 0.9918  loss_dice_0: 1.388  loss_ce_1: 0.4304  loss_cate_1: 0  loss_mask_1: 0.9904  loss_dice_1: 1.313  loss_ce_2: 0.4652  loss_cate_2: 0  loss_mask_2: 0.9948  loss_dice_2: 1.346  loss_ce_3: 0.4091  loss_cate_3: 0  loss_mask_3: 0.9826  loss_dice_3: 1.372  loss_ce_4: 0.4183  loss_cate_4: 0  loss_mask_4: 1.03  loss_dice_4: 1.356  loss_ce_5: 0.4378  loss_cate_5: 0  loss_mask_5: 0.9666  loss_dice_5: 1.35  loss_ce_6: 0.4457  loss_cate_6: 0  loss_mask_6: 0.9517  loss_dice_6: 1.333  loss_ce_7: 0.4406  loss_cate_7: 0  loss_mask_7: 0.9834  loss_dice_7: 1.356  loss_ce_8: 0.4206  loss_cate_8: 0  loss_mask_8: 0.9596  loss_dice_8: 1.342  time: 1.1034  data_time: 0.0147  lr: 9.8898e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:41:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 11:41:06 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 11:41:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 11:42:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 11:42:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0014 s/iter. Inference: 0.1632 s/iter. Eval: 0.0141 s/iter. Total: 0.1788 s/iter. ETA=0:05:58\n",
      "\u001b[32m[10/12 11:42:16 d2.evaluation.evaluator]: \u001b[0mInference done 37/2016. Dataloading: 0.0021 s/iter. Inference: 0.1622 s/iter. Eval: 0.0300 s/iter. Total: 0.1946 s/iter. ETA=0:06:25\n",
      "\u001b[32m[10/12 11:42:21 d2.evaluation.evaluator]: \u001b[0mInference done 64/2016. Dataloading: 0.0022 s/iter. Inference: 0.1617 s/iter. Eval: 0.0265 s/iter. Total: 0.1905 s/iter. ETA=0:06:11\n",
      "\u001b[32m[10/12 11:42:26 d2.evaluation.evaluator]: \u001b[0mInference done 91/2016. Dataloading: 0.0022 s/iter. Inference: 0.1614 s/iter. Eval: 0.0260 s/iter. Total: 0.1898 s/iter. ETA=0:06:05\n",
      "\u001b[32m[10/12 11:42:31 d2.evaluation.evaluator]: \u001b[0mInference done 117/2016. Dataloading: 0.0022 s/iter. Inference: 0.1617 s/iter. Eval: 0.0265 s/iter. Total: 0.1906 s/iter. ETA=0:06:01\n",
      "\u001b[32m[10/12 11:42:37 d2.evaluation.evaluator]: \u001b[0mInference done 142/2016. Dataloading: 0.0022 s/iter. Inference: 0.1619 s/iter. Eval: 0.0288 s/iter. Total: 0.1930 s/iter. ETA=0:06:01\n",
      "\u001b[32m[10/12 11:42:42 d2.evaluation.evaluator]: \u001b[0mInference done 169/2016. Dataloading: 0.0022 s/iter. Inference: 0.1622 s/iter. Eval: 0.0278 s/iter. Total: 0.1924 s/iter. ETA=0:05:55\n",
      "\u001b[32m[10/12 11:42:47 d2.evaluation.evaluator]: \u001b[0mInference done 195/2016. Dataloading: 0.0023 s/iter. Inference: 0.1624 s/iter. Eval: 0.0283 s/iter. Total: 0.1931 s/iter. ETA=0:05:51\n",
      "\u001b[32m[10/12 11:42:52 d2.evaluation.evaluator]: \u001b[0mInference done 223/2016. Dataloading: 0.0023 s/iter. Inference: 0.1625 s/iter. Eval: 0.0270 s/iter. Total: 0.1920 s/iter. ETA=0:05:44\n",
      "\u001b[32m[10/12 11:42:57 d2.evaluation.evaluator]: \u001b[0mInference done 248/2016. Dataloading: 0.0023 s/iter. Inference: 0.1627 s/iter. Eval: 0.0277 s/iter. Total: 0.1929 s/iter. ETA=0:05:40\n",
      "\u001b[32m[10/12 11:43:02 d2.evaluation.evaluator]: \u001b[0mInference done 275/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0272 s/iter. Total: 0.1924 s/iter. ETA=0:05:35\n",
      "\u001b[32m[10/12 11:43:07 d2.evaluation.evaluator]: \u001b[0mInference done 301/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0276 s/iter. Total: 0.1929 s/iter. ETA=0:05:30\n",
      "\u001b[32m[10/12 11:43:12 d2.evaluation.evaluator]: \u001b[0mInference done 329/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0268 s/iter. Total: 0.1921 s/iter. ETA=0:05:24\n",
      "\u001b[32m[10/12 11:43:17 d2.evaluation.evaluator]: \u001b[0mInference done 356/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0268 s/iter. Total: 0.1920 s/iter. ETA=0:05:18\n",
      "\u001b[32m[10/12 11:43:23 d2.evaluation.evaluator]: \u001b[0mInference done 383/2016. Dataloading: 0.0023 s/iter. Inference: 0.1627 s/iter. Eval: 0.0265 s/iter. Total: 0.1916 s/iter. ETA=0:05:12\n",
      "\u001b[32m[10/12 11:43:28 d2.evaluation.evaluator]: \u001b[0mInference done 409/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0266 s/iter. Total: 0.1919 s/iter. ETA=0:05:08\n",
      "\u001b[32m[10/12 11:43:33 d2.evaluation.evaluator]: \u001b[0mInference done 436/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0266 s/iter. Total: 0.1918 s/iter. ETA=0:05:03\n",
      "\u001b[32m[10/12 11:43:39 d2.evaluation.evaluator]: \u001b[0mInference done 461/2016. Dataloading: 0.0047 s/iter. Inference: 0.1629 s/iter. Eval: 0.0283 s/iter. Total: 0.1961 s/iter. ETA=0:05:04\n",
      "\u001b[32m[10/12 11:43:45 d2.evaluation.evaluator]: \u001b[0mInference done 473/2016. Dataloading: 0.0104 s/iter. Inference: 0.1629 s/iter. Eval: 0.0289 s/iter. Total: 0.2024 s/iter. ETA=0:05:12\n",
      "\u001b[32m[10/12 11:43:50 d2.evaluation.evaluator]: \u001b[0mInference done 499/2016. Dataloading: 0.0100 s/iter. Inference: 0.1629 s/iter. Eval: 0.0289 s/iter. Total: 0.2019 s/iter. ETA=0:05:06\n",
      "\u001b[32m[10/12 11:43:55 d2.evaluation.evaluator]: \u001b[0mInference done 526/2016. Dataloading: 0.0096 s/iter. Inference: 0.1628 s/iter. Eval: 0.0285 s/iter. Total: 0.2011 s/iter. ETA=0:04:59\n",
      "\u001b[32m[10/12 11:44:00 d2.evaluation.evaluator]: \u001b[0mInference done 552/2016. Dataloading: 0.0092 s/iter. Inference: 0.1628 s/iter. Eval: 0.0286 s/iter. Total: 0.2008 s/iter. ETA=0:04:53\n",
      "\u001b[32m[10/12 11:44:05 d2.evaluation.evaluator]: \u001b[0mInference done 580/2016. Dataloading: 0.0089 s/iter. Inference: 0.1628 s/iter. Eval: 0.0281 s/iter. Total: 0.2000 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 11:44:10 d2.evaluation.evaluator]: \u001b[0mInference done 607/2016. Dataloading: 0.0086 s/iter. Inference: 0.1628 s/iter. Eval: 0.0280 s/iter. Total: 0.1995 s/iter. ETA=0:04:41\n",
      "\u001b[32m[10/12 11:44:15 d2.evaluation.evaluator]: \u001b[0mInference done 633/2016. Dataloading: 0.0083 s/iter. Inference: 0.1629 s/iter. Eval: 0.0281 s/iter. Total: 0.1995 s/iter. ETA=0:04:35\n",
      "\u001b[32m[10/12 11:44:20 d2.evaluation.evaluator]: \u001b[0mInference done 660/2016. Dataloading: 0.0080 s/iter. Inference: 0.1630 s/iter. Eval: 0.0277 s/iter. Total: 0.1989 s/iter. ETA=0:04:29\n",
      "\u001b[32m[10/12 11:44:25 d2.evaluation.evaluator]: \u001b[0mInference done 686/2016. Dataloading: 0.0078 s/iter. Inference: 0.1630 s/iter. Eval: 0.0278 s/iter. Total: 0.1987 s/iter. ETA=0:04:24\n",
      "\u001b[32m[10/12 11:44:31 d2.evaluation.evaluator]: \u001b[0mInference done 713/2016. Dataloading: 0.0076 s/iter. Inference: 0.1630 s/iter. Eval: 0.0277 s/iter. Total: 0.1984 s/iter. ETA=0:04:18\n",
      "\u001b[32m[10/12 11:44:36 d2.evaluation.evaluator]: \u001b[0mInference done 740/2016. Dataloading: 0.0074 s/iter. Inference: 0.1630 s/iter. Eval: 0.0276 s/iter. Total: 0.1982 s/iter. ETA=0:04:12\n",
      "\u001b[32m[10/12 11:44:41 d2.evaluation.evaluator]: \u001b[0mInference done 767/2016. Dataloading: 0.0072 s/iter. Inference: 0.1630 s/iter. Eval: 0.0273 s/iter. Total: 0.1977 s/iter. ETA=0:04:06\n",
      "\u001b[32m[10/12 11:44:46 d2.evaluation.evaluator]: \u001b[0mInference done 793/2016. Dataloading: 0.0071 s/iter. Inference: 0.1631 s/iter. Eval: 0.0272 s/iter. Total: 0.1976 s/iter. ETA=0:04:01\n",
      "\u001b[32m[10/12 11:44:51 d2.evaluation.evaluator]: \u001b[0mInference done 819/2016. Dataloading: 0.0069 s/iter. Inference: 0.1631 s/iter. Eval: 0.0273 s/iter. Total: 0.1975 s/iter. ETA=0:03:56\n",
      "\u001b[32m[10/12 11:44:56 d2.evaluation.evaluator]: \u001b[0mInference done 846/2016. Dataloading: 0.0068 s/iter. Inference: 0.1631 s/iter. Eval: 0.0271 s/iter. Total: 0.1972 s/iter. ETA=0:03:50\n",
      "\u001b[32m[10/12 11:45:01 d2.evaluation.evaluator]: \u001b[0mInference done 871/2016. Dataloading: 0.0066 s/iter. Inference: 0.1631 s/iter. Eval: 0.0274 s/iter. Total: 0.1973 s/iter. ETA=0:03:45\n",
      "\u001b[32m[10/12 11:45:06 d2.evaluation.evaluator]: \u001b[0mInference done 898/2016. Dataloading: 0.0065 s/iter. Inference: 0.1631 s/iter. Eval: 0.0272 s/iter. Total: 0.1970 s/iter. ETA=0:03:40\n",
      "\u001b[32m[10/12 11:45:11 d2.evaluation.evaluator]: \u001b[0mInference done 923/2016. Dataloading: 0.0064 s/iter. Inference: 0.1632 s/iter. Eval: 0.0275 s/iter. Total: 0.1972 s/iter. ETA=0:03:35\n",
      "\u001b[32m[10/12 11:45:16 d2.evaluation.evaluator]: \u001b[0mInference done 950/2016. Dataloading: 0.0063 s/iter. Inference: 0.1631 s/iter. Eval: 0.0274 s/iter. Total: 0.1969 s/iter. ETA=0:03:29\n",
      "\u001b[32m[10/12 11:45:21 d2.evaluation.evaluator]: \u001b[0mInference done 975/2016. Dataloading: 0.0062 s/iter. Inference: 0.1631 s/iter. Eval: 0.0276 s/iter. Total: 0.1970 s/iter. ETA=0:03:25\n",
      "\u001b[32m[10/12 11:45:26 d2.evaluation.evaluator]: \u001b[0mInference done 1001/2016. Dataloading: 0.0061 s/iter. Inference: 0.1631 s/iter. Eval: 0.0276 s/iter. Total: 0.1970 s/iter. ETA=0:03:19\n",
      "\u001b[32m[10/12 11:45:31 d2.evaluation.evaluator]: \u001b[0mInference done 1026/2016. Dataloading: 0.0060 s/iter. Inference: 0.1631 s/iter. Eval: 0.0279 s/iter. Total: 0.1972 s/iter. ETA=0:03:15\n",
      "\u001b[32m[10/12 11:45:36 d2.evaluation.evaluator]: \u001b[0mInference done 1053/2016. Dataloading: 0.0059 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1969 s/iter. ETA=0:03:09\n",
      "\u001b[32m[10/12 11:45:42 d2.evaluation.evaluator]: \u001b[0mInference done 1079/2016. Dataloading: 0.0058 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1970 s/iter. ETA=0:03:04\n",
      "\u001b[32m[10/12 11:45:47 d2.evaluation.evaluator]: \u001b[0mInference done 1106/2016. Dataloading: 0.0057 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1968 s/iter. ETA=0:02:59\n",
      "\u001b[32m[10/12 11:45:52 d2.evaluation.evaluator]: \u001b[0mInference done 1132/2016. Dataloading: 0.0056 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1967 s/iter. ETA=0:02:53\n",
      "\u001b[32m[10/12 11:45:57 d2.evaluation.evaluator]: \u001b[0mInference done 1159/2016. Dataloading: 0.0055 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1965 s/iter. ETA=0:02:48\n",
      "\u001b[32m[10/12 11:46:02 d2.evaluation.evaluator]: \u001b[0mInference done 1185/2016. Dataloading: 0.0055 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1965 s/iter. ETA=0:02:43\n",
      "\u001b[32m[10/12 11:46:07 d2.evaluation.evaluator]: \u001b[0mInference done 1212/2016. Dataloading: 0.0054 s/iter. Inference: 0.1632 s/iter. Eval: 0.0275 s/iter. Total: 0.1963 s/iter. ETA=0:02:37\n",
      "\u001b[32m[10/12 11:46:12 d2.evaluation.evaluator]: \u001b[0mInference done 1238/2016. Dataloading: 0.0053 s/iter. Inference: 0.1632 s/iter. Eval: 0.0275 s/iter. Total: 0.1962 s/iter. ETA=0:02:32\n",
      "\u001b[32m[10/12 11:46:17 d2.evaluation.evaluator]: \u001b[0mInference done 1265/2016. Dataloading: 0.0053 s/iter. Inference: 0.1632 s/iter. Eval: 0.0274 s/iter. Total: 0.1960 s/iter. ETA=0:02:27\n",
      "\u001b[32m[10/12 11:46:22 d2.evaluation.evaluator]: \u001b[0mInference done 1291/2016. Dataloading: 0.0052 s/iter. Inference: 0.1632 s/iter. Eval: 0.0274 s/iter. Total: 0.1960 s/iter. ETA=0:02:22\n",
      "\u001b[32m[10/12 11:46:27 d2.evaluation.evaluator]: \u001b[0mInference done 1318/2016. Dataloading: 0.0051 s/iter. Inference: 0.1632 s/iter. Eval: 0.0274 s/iter. Total: 0.1959 s/iter. ETA=0:02:16\n",
      "\u001b[32m[10/12 11:46:37 d2.evaluation.evaluator]: \u001b[0mInference done 1369/2016. Dataloading: 0.0050 s/iter. Inference: 0.1633 s/iter. Eval: 0.0276 s/iter. Total: 0.1960 s/iter. ETA=0:02:06\n",
      "\u001b[32m[10/12 11:46:43 d2.evaluation.evaluator]: \u001b[0mInference done 1393/2016. Dataloading: 0.0050 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1963 s/iter. ETA=0:02:02\n",
      "\u001b[32m[10/12 11:46:48 d2.evaluation.evaluator]: \u001b[0mInference done 1420/2016. Dataloading: 0.0050 s/iter. Inference: 0.1633 s/iter. Eval: 0.0278 s/iter. Total: 0.1962 s/iter. ETA=0:01:56\n",
      "\u001b[32m[10/12 11:46:53 d2.evaluation.evaluator]: \u001b[0mInference done 1446/2016. Dataloading: 0.0049 s/iter. Inference: 0.1633 s/iter. Eval: 0.0278 s/iter. Total: 0.1962 s/iter. ETA=0:01:51\n",
      "\u001b[32m[10/12 11:46:58 d2.evaluation.evaluator]: \u001b[0mInference done 1473/2016. Dataloading: 0.0049 s/iter. Inference: 0.1633 s/iter. Eval: 0.0277 s/iter. Total: 0.1960 s/iter. ETA=0:01:46\n",
      "\u001b[32m[10/12 11:47:03 d2.evaluation.evaluator]: \u001b[0mInference done 1498/2016. Dataloading: 0.0048 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1961 s/iter. ETA=0:01:41\n",
      "\u001b[32m[10/12 11:47:08 d2.evaluation.evaluator]: \u001b[0mInference done 1525/2016. Dataloading: 0.0048 s/iter. Inference: 0.1633 s/iter. Eval: 0.0278 s/iter. Total: 0.1960 s/iter. ETA=0:01:36\n",
      "\u001b[32m[10/12 11:47:13 d2.evaluation.evaluator]: \u001b[0mInference done 1552/2016. Dataloading: 0.0047 s/iter. Inference: 0.1633 s/iter. Eval: 0.0278 s/iter. Total: 0.1959 s/iter. ETA=0:01:30\n",
      "\u001b[32m[10/12 11:47:18 d2.evaluation.evaluator]: \u001b[0mInference done 1578/2016. Dataloading: 0.0047 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1959 s/iter. ETA=0:01:25\n",
      "\u001b[32m[10/12 11:47:23 d2.evaluation.evaluator]: \u001b[0mInference done 1605/2016. Dataloading: 0.0046 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1958 s/iter. ETA=0:01:20\n",
      "\u001b[32m[10/12 11:47:29 d2.evaluation.evaluator]: \u001b[0mInference done 1631/2016. Dataloading: 0.0046 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1959 s/iter. ETA=0:01:15\n",
      "\u001b[32m[10/12 11:47:34 d2.evaluation.evaluator]: \u001b[0mInference done 1659/2016. Dataloading: 0.0046 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1957 s/iter. ETA=0:01:09\n",
      "\u001b[32m[10/12 11:47:39 d2.evaluation.evaluator]: \u001b[0mInference done 1685/2016. Dataloading: 0.0045 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1956 s/iter. ETA=0:01:04\n",
      "\u001b[32m[10/12 11:47:44 d2.evaluation.evaluator]: \u001b[0mInference done 1712/2016. Dataloading: 0.0045 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1956 s/iter. ETA=0:00:59\n",
      "\u001b[32m[10/12 11:47:49 d2.evaluation.evaluator]: \u001b[0mInference done 1738/2016. Dataloading: 0.0044 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1955 s/iter. ETA=0:00:54\n",
      "\u001b[32m[10/12 11:47:54 d2.evaluation.evaluator]: \u001b[0mInference done 1765/2016. Dataloading: 0.0044 s/iter. Inference: 0.1633 s/iter. Eval: 0.0276 s/iter. Total: 0.1954 s/iter. ETA=0:00:49\n",
      "\u001b[32m[10/12 11:47:59 d2.evaluation.evaluator]: \u001b[0mInference done 1792/2016. Dataloading: 0.0044 s/iter. Inference: 0.1633 s/iter. Eval: 0.0275 s/iter. Total: 0.1953 s/iter. ETA=0:00:43\n",
      "\u001b[32m[10/12 11:48:04 d2.evaluation.evaluator]: \u001b[0mInference done 1818/2016. Dataloading: 0.0043 s/iter. Inference: 0.1633 s/iter. Eval: 0.0277 s/iter. Total: 0.1955 s/iter. ETA=0:00:38\n",
      "\u001b[32m[10/12 11:48:10 d2.evaluation.evaluator]: \u001b[0mInference done 1845/2016. Dataloading: 0.0043 s/iter. Inference: 0.1633 s/iter. Eval: 0.0277 s/iter. Total: 0.1954 s/iter. ETA=0:00:33\n",
      "\u001b[32m[10/12 11:48:15 d2.evaluation.evaluator]: \u001b[0mInference done 1873/2016. Dataloading: 0.0043 s/iter. Inference: 0.1633 s/iter. Eval: 0.0275 s/iter. Total: 0.1953 s/iter. ETA=0:00:27\n",
      "\u001b[32m[10/12 11:48:20 d2.evaluation.evaluator]: \u001b[0mInference done 1900/2016. Dataloading: 0.0043 s/iter. Inference: 0.1633 s/iter. Eval: 0.0274 s/iter. Total: 0.1951 s/iter. ETA=0:00:22\n",
      "\u001b[32m[10/12 11:48:25 d2.evaluation.evaluator]: \u001b[0mInference done 1926/2016. Dataloading: 0.0042 s/iter. Inference: 0.1633 s/iter. Eval: 0.0274 s/iter. Total: 0.1951 s/iter. ETA=0:00:17\n",
      "\u001b[32m[10/12 11:48:30 d2.evaluation.evaluator]: \u001b[0mInference done 1952/2016. Dataloading: 0.0042 s/iter. Inference: 0.1633 s/iter. Eval: 0.0274 s/iter. Total: 0.1951 s/iter. ETA=0:00:12\n",
      "\u001b[32m[10/12 11:48:35 d2.evaluation.evaluator]: \u001b[0mInference done 1977/2016. Dataloading: 0.0042 s/iter. Inference: 0.1633 s/iter. Eval: 0.0275 s/iter. Total: 0.1952 s/iter. ETA=0:00:07\n",
      "\u001b[32m[10/12 11:48:40 d2.evaluation.evaluator]: \u001b[0mInference done 2004/2016. Dataloading: 0.0042 s/iter. Inference: 0.1633 s/iter. Eval: 0.0275 s/iter. Total: 0.1951 s/iter. ETA=0:00:02\n",
      "\u001b[32m[10/12 11:48:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:32.221936 (0.195038 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 11:48:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:28 (0.163303 s / iter per device, on 1 devices)\n",
      "miou = 70.46283496592805\n",
      "OA = 86.10867999848865\n",
      "Kappa = 81.92492520980086\n",
      "F1_score = 65.90976490772503\n",
      "\u001b[32m[10/12 11:48:43 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 70.46283496592805, 'fwIoU': 76.9637116800726, 'IoU-Background': 32.67655053891998, 'IoU-Surfaces': 81.7359269537907, 'IoU-Building': 85.41825267488299, 'IoU-Low vegetation': 73.02387850997603, 'IoU-tree': 73.27030437979546, 'IoU-Car': 76.65209673820318, 'mACC': 80.86444979297399, 'pACC': 86.10867999848865, 'ACC-Background': 58.02873559479872, 'ACC-Surfaces': 93.09808209532973, 'ACC-Building': 87.18831318503779, 'ACC-Low vegetation': 85.69422417624192, 'ACC-tree': 80.53859375171861, 'ACC-Car': 80.63874995471726})])\n",
      "\u001b[32m[10/12 11:48:43 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 11:48:43 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 11:48:43 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 11:48:43 d2.evaluation.testing]: \u001b[0mcopypaste: 70.4628,76.9637,80.8644,86.1087\n",
      "\u001b[32m[10/12 11:48:43 d2.utils.events]: \u001b[0m eta: 1 day, 0:04:55  iter: 999  total_loss: 29.35  loss_ce: 0.3623  loss_cate: 0.2809  loss_mask: 1.058  loss_dice: 1.183  loss_ce_0: 0.6448  loss_cate_0: 0  loss_mask_0: 1.065  loss_dice_0: 1.233  loss_ce_1: 0.4639  loss_cate_1: 0  loss_mask_1: 1.172  loss_dice_1: 1.263  loss_ce_2: 0.4112  loss_cate_2: 0  loss_mask_2: 1.104  loss_dice_2: 1.244  loss_ce_3: 0.3702  loss_cate_3: 0  loss_mask_3: 1.102  loss_dice_3: 1.219  loss_ce_4: 0.3499  loss_cate_4: 0  loss_mask_4: 1.086  loss_dice_4: 1.203  loss_ce_5: 0.3364  loss_cate_5: 0  loss_mask_5: 1.086  loss_dice_5: 1.181  loss_ce_6: 0.3289  loss_cate_6: 0  loss_mask_6: 1.135  loss_dice_6: 1.161  loss_ce_7: 0.3307  loss_cate_7: 0  loss_mask_7: 1.144  loss_dice_7: 1.141  loss_ce_8: 0.3583  loss_cate_8: 0  loss_mask_8: 1.117  loss_dice_8: 1.167  time: 1.1030  data_time: 0.0127  lr: 9.8875e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:49:05 d2.utils.events]: \u001b[0m eta: 1 day, 0:04:50  iter: 1019  total_loss: 26.42  loss_ce: 0.3865  loss_cate: 0.3014  loss_mask: 1.007  loss_dice: 1.21  loss_ce_0: 0.7125  loss_cate_0: 0  loss_mask_0: 1.033  loss_dice_0: 1.29  loss_ce_1: 0.4194  loss_cate_1: 0  loss_mask_1: 0.9911  loss_dice_1: 1.192  loss_ce_2: 0.3909  loss_cate_2: 0  loss_mask_2: 0.9527  loss_dice_2: 1.146  loss_ce_3: 0.446  loss_cate_3: 0  loss_mask_3: 0.957  loss_dice_3: 1.2  loss_ce_4: 0.4225  loss_cate_4: 0  loss_mask_4: 0.9986  loss_dice_4: 1.13  loss_ce_5: 0.4082  loss_cate_5: 0  loss_mask_5: 0.9855  loss_dice_5: 1.21  loss_ce_6: 0.4417  loss_cate_6: 0  loss_mask_6: 0.9446  loss_dice_6: 1.18  loss_ce_7: 0.4515  loss_cate_7: 0  loss_mask_7: 0.9538  loss_dice_7: 1.21  loss_ce_8: 0.4437  loss_cate_8: 0  loss_mask_8: 0.9953  loss_dice_8: 1.236  time: 1.1030  data_time: 0.0130  lr: 9.8853e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:49:27 d2.utils.events]: \u001b[0m eta: 1 day, 0:04:30  iter: 1039  total_loss: 27.55  loss_ce: 0.3458  loss_cate: 0.3069  loss_mask: 1.1  loss_dice: 1.317  loss_ce_0: 0.656  loss_cate_0: 0  loss_mask_0: 0.9064  loss_dice_0: 1.294  loss_ce_1: 0.4136  loss_cate_1: 0  loss_mask_1: 1.029  loss_dice_1: 1.371  loss_ce_2: 0.3566  loss_cate_2: 0  loss_mask_2: 1.038  loss_dice_2: 1.277  loss_ce_3: 0.3983  loss_cate_3: 0  loss_mask_3: 1.063  loss_dice_3: 1.273  loss_ce_4: 0.4206  loss_cate_4: 0  loss_mask_4: 1.053  loss_dice_4: 1.262  loss_ce_5: 0.4117  loss_cate_5: 0  loss_mask_5: 0.9949  loss_dice_5: 1.24  loss_ce_6: 0.3884  loss_cate_6: 0  loss_mask_6: 0.9693  loss_dice_6: 1.293  loss_ce_7: 0.3931  loss_cate_7: 0  loss_mask_7: 1.033  loss_dice_7: 1.298  loss_ce_8: 0.3568  loss_cate_8: 0  loss_mask_8: 1.054  loss_dice_8: 1.316  time: 1.1028  data_time: 0.0145  lr: 9.883e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:49:50 d2.utils.events]: \u001b[0m eta: 1 day, 0:04:15  iter: 1059  total_loss: 25.64  loss_ce: 0.3064  loss_cate: 0.2512  loss_mask: 0.9488  loss_dice: 1.273  loss_ce_0: 0.5436  loss_cate_0: 0  loss_mask_0: 1.018  loss_dice_0: 1.315  loss_ce_1: 0.3359  loss_cate_1: 0  loss_mask_1: 0.9  loss_dice_1: 1.255  loss_ce_2: 0.3863  loss_cate_2: 0  loss_mask_2: 0.899  loss_dice_2: 1.257  loss_ce_3: 0.3711  loss_cate_3: 0  loss_mask_3: 0.99  loss_dice_3: 1.263  loss_ce_4: 0.3614  loss_cate_4: 0  loss_mask_4: 0.9895  loss_dice_4: 1.259  loss_ce_5: 0.3347  loss_cate_5: 0  loss_mask_5: 0.9673  loss_dice_5: 1.169  loss_ce_6: 0.3562  loss_cate_6: 0  loss_mask_6: 0.9997  loss_dice_6: 1.24  loss_ce_7: 0.3196  loss_cate_7: 0  loss_mask_7: 0.9756  loss_dice_7: 1.233  loss_ce_8: 0.2714  loss_cate_8: 0  loss_mask_8: 0.9773  loss_dice_8: 1.24  time: 1.1028  data_time: 0.0136  lr: 9.8808e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:50:13 d2.utils.events]: \u001b[0m eta: 1 day, 0:03:59  iter: 1079  total_loss: 26.31  loss_ce: 0.3397  loss_cate: 0.2152  loss_mask: 1.006  loss_dice: 1.179  loss_ce_0: 0.6731  loss_cate_0: 0  loss_mask_0: 0.9541  loss_dice_0: 1.251  loss_ce_1: 0.4914  loss_cate_1: 0  loss_mask_1: 0.9612  loss_dice_1: 1.201  loss_ce_2: 0.4292  loss_cate_2: 0  loss_mask_2: 1.01  loss_dice_2: 1.215  loss_ce_3: 0.3798  loss_cate_3: 0  loss_mask_3: 1.02  loss_dice_3: 1.202  loss_ce_4: 0.3941  loss_cate_4: 0  loss_mask_4: 1  loss_dice_4: 1.178  loss_ce_5: 0.3695  loss_cate_5: 0  loss_mask_5: 0.9953  loss_dice_5: 1.167  loss_ce_6: 0.3889  loss_cate_6: 0  loss_mask_6: 0.9801  loss_dice_6: 1.143  loss_ce_7: 0.3518  loss_cate_7: 0  loss_mask_7: 0.9704  loss_dice_7: 1.181  loss_ce_8: 0.3452  loss_cate_8: 0  loss_mask_8: 0.9604  loss_dice_8: 1.154  time: 1.1027  data_time: 0.0131  lr: 9.8785e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:50:35 d2.utils.events]: \u001b[0m eta: 1 day, 0:03:44  iter: 1099  total_loss: 23.92  loss_ce: 0.3244  loss_cate: 0.2716  loss_mask: 0.9316  loss_dice: 1.101  loss_ce_0: 0.6773  loss_cate_0: 0  loss_mask_0: 0.9203  loss_dice_0: 1.149  loss_ce_1: 0.3835  loss_cate_1: 0  loss_mask_1: 1.009  loss_dice_1: 1.062  loss_ce_2: 0.3398  loss_cate_2: 0  loss_mask_2: 0.9266  loss_dice_2: 1.082  loss_ce_3: 0.3324  loss_cate_3: 0  loss_mask_3: 0.909  loss_dice_3: 1.11  loss_ce_4: 0.3229  loss_cate_4: 0  loss_mask_4: 0.908  loss_dice_4: 1.091  loss_ce_5: 0.3318  loss_cate_5: 0  loss_mask_5: 0.9499  loss_dice_5: 1.116  loss_ce_6: 0.3364  loss_cate_6: 0  loss_mask_6: 0.9943  loss_dice_6: 1.104  loss_ce_7: 0.3217  loss_cate_7: 0  loss_mask_7: 0.9436  loss_dice_7: 1.11  loss_ce_8: 0.3221  loss_cate_8: 0  loss_mask_8: 0.9441  loss_dice_8: 1.129  time: 1.1029  data_time: 0.0142  lr: 9.8763e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:50:57 d2.utils.events]: \u001b[0m eta: 1 day, 0:03:24  iter: 1119  total_loss: 24.78  loss_ce: 0.3411  loss_cate: 0.2828  loss_mask: 0.9355  loss_dice: 1.183  loss_ce_0: 0.6328  loss_cate_0: 0  loss_mask_0: 0.9423  loss_dice_0: 1.241  loss_ce_1: 0.39  loss_cate_1: 0  loss_mask_1: 0.9595  loss_dice_1: 1.257  loss_ce_2: 0.3966  loss_cate_2: 0  loss_mask_2: 0.9522  loss_dice_2: 1.233  loss_ce_3: 0.3688  loss_cate_3: 0  loss_mask_3: 0.9449  loss_dice_3: 1.18  loss_ce_4: 0.2972  loss_cate_4: 0  loss_mask_4: 0.9489  loss_dice_4: 1.192  loss_ce_5: 0.4184  loss_cate_5: 0  loss_mask_5: 0.9333  loss_dice_5: 1.157  loss_ce_6: 0.3186  loss_cate_6: 0  loss_mask_6: 0.9487  loss_dice_6: 1.171  loss_ce_7: 0.2993  loss_cate_7: 0  loss_mask_7: 0.9473  loss_dice_7: 1.164  loss_ce_8: 0.3214  loss_cate_8: 0  loss_mask_8: 0.9486  loss_dice_8: 1.164  time: 1.1028  data_time: 0.0123  lr: 9.874e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:51:20 d2.utils.events]: \u001b[0m eta: 1 day, 0:03:01  iter: 1139  total_loss: 26.37  loss_ce: 0.2766  loss_cate: 0.2884  loss_mask: 1.023  loss_dice: 1.256  loss_ce_0: 0.5934  loss_cate_0: 0  loss_mask_0: 1.002  loss_dice_0: 1.239  loss_ce_1: 0.3491  loss_cate_1: 0  loss_mask_1: 0.9676  loss_dice_1: 1.168  loss_ce_2: 0.3525  loss_cate_2: 0  loss_mask_2: 0.9605  loss_dice_2: 1.183  loss_ce_3: 0.2451  loss_cate_3: 0  loss_mask_3: 1.001  loss_dice_3: 1.243  loss_ce_4: 0.2645  loss_cate_4: 0  loss_mask_4: 0.9976  loss_dice_4: 1.232  loss_ce_5: 0.3055  loss_cate_5: 0  loss_mask_5: 0.9765  loss_dice_5: 1.166  loss_ce_6: 0.336  loss_cate_6: 0  loss_mask_6: 0.952  loss_dice_6: 1.187  loss_ce_7: 0.2841  loss_cate_7: 0  loss_mask_7: 0.9884  loss_dice_7: 1.227  loss_ce_8: 0.2759  loss_cate_8: 0  loss_mask_8: 0.9905  loss_dice_8: 1.181  time: 1.1028  data_time: 0.0127  lr: 9.8718e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:51:42 d2.utils.events]: \u001b[0m eta: 1 day, 0:02:42  iter: 1159  total_loss: 25.32  loss_ce: 0.2634  loss_cate: 0.2682  loss_mask: 0.9587  loss_dice: 1.267  loss_ce_0: 0.61  loss_cate_0: 0  loss_mask_0: 0.9576  loss_dice_0: 1.269  loss_ce_1: 0.3929  loss_cate_1: 0  loss_mask_1: 0.9215  loss_dice_1: 1.287  loss_ce_2: 0.3823  loss_cate_2: 0  loss_mask_2: 0.9391  loss_dice_2: 1.287  loss_ce_3: 0.3321  loss_cate_3: 0  loss_mask_3: 0.9461  loss_dice_3: 1.237  loss_ce_4: 0.343  loss_cate_4: 0  loss_mask_4: 0.9117  loss_dice_4: 1.225  loss_ce_5: 0.2942  loss_cate_5: 0  loss_mask_5: 0.9108  loss_dice_5: 1.25  loss_ce_6: 0.2684  loss_cate_6: 0  loss_mask_6: 0.9759  loss_dice_6: 1.257  loss_ce_7: 0.2631  loss_cate_7: 0  loss_mask_7: 0.9618  loss_dice_7: 1.281  loss_ce_8: 0.2547  loss_cate_8: 0  loss_mask_8: 0.9659  loss_dice_8: 1.265  time: 1.1028  data_time: 0.0132  lr: 9.8695e-06  max_mem: 28911M\n",
      "\u001b[32m[10/12 11:52:05 d2.utils.events]: \u001b[0m eta: 1 day, 0:02:20  iter: 1179  total_loss: 28.34  loss_ce: 0.3587  loss_cate: 0.3179  loss_mask: 1.051  loss_dice: 1.361  loss_ce_0: 0.6844  loss_cate_0: 0  loss_mask_0: 1.127  loss_dice_0: 1.357  loss_ce_1: 0.444  loss_cate_1: 0  loss_mask_1: 1.055  loss_dice_1: 1.305  loss_ce_2: 0.3934  loss_cate_2: 0  loss_mask_2: 1.016  loss_dice_2: 1.388  loss_ce_3: 0.365  loss_cate_3: 0  loss_mask_3: 1.035  loss_dice_3: 1.376  loss_ce_4: 0.3701  loss_cate_4: 0  loss_mask_4: 1.034  loss_dice_4: 1.38  loss_ce_5: 0.3754  loss_cate_5: 0  loss_mask_5: 1.005  loss_dice_5: 1.358  loss_ce_6: 0.3671  loss_cate_6: 0  loss_mask_6: 1.026  loss_dice_6: 1.34  loss_ce_7: 0.3915  loss_cate_7: 0  loss_mask_7: 1.041  loss_dice_7: 1.357  loss_ce_8: 0.3545  loss_cate_8: 0  loss_mask_8: 1.073  loss_dice_8: 1.356  time: 1.1027  data_time: 0.0129  lr: 9.8673e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:52:27 d2.utils.events]: \u001b[0m eta: 1 day, 0:01:57  iter: 1199  total_loss: 29.88  loss_ce: 0.3998  loss_cate: 0.2736  loss_mask: 1.127  loss_dice: 1.269  loss_ce_0: 0.5934  loss_cate_0: 0  loss_mask_0: 1.086  loss_dice_0: 1.378  loss_ce_1: 0.446  loss_cate_1: 0  loss_mask_1: 1.123  loss_dice_1: 1.291  loss_ce_2: 0.4522  loss_cate_2: 0  loss_mask_2: 1.06  loss_dice_2: 1.284  loss_ce_3: 0.4545  loss_cate_3: 0  loss_mask_3: 1.074  loss_dice_3: 1.315  loss_ce_4: 0.39  loss_cate_4: 0  loss_mask_4: 1.121  loss_dice_4: 1.319  loss_ce_5: 0.4131  loss_cate_5: 0  loss_mask_5: 1.122  loss_dice_5: 1.323  loss_ce_6: 0.4025  loss_cate_6: 0  loss_mask_6: 1.082  loss_dice_6: 1.255  loss_ce_7: 0.4827  loss_cate_7: 0  loss_mask_7: 1.178  loss_dice_7: 1.293  loss_ce_8: 0.4376  loss_cate_8: 0  loss_mask_8: 1.151  loss_dice_8: 1.317  time: 1.1026  data_time: 0.0127  lr: 9.865e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:52:49 d2.utils.events]: \u001b[0m eta: 1 day, 0:01:32  iter: 1219  total_loss: 26.09  loss_ce: 0.317  loss_cate: 0.2607  loss_mask: 1.004  loss_dice: 1.24  loss_ce_0: 0.5247  loss_cate_0: 0  loss_mask_0: 1.069  loss_dice_0: 1.351  loss_ce_1: 0.4277  loss_cate_1: 0  loss_mask_1: 1.062  loss_dice_1: 1.322  loss_ce_2: 0.3385  loss_cate_2: 0  loss_mask_2: 1.012  loss_dice_2: 1.293  loss_ce_3: 0.4087  loss_cate_3: 0  loss_mask_3: 0.9904  loss_dice_3: 1.211  loss_ce_4: 0.3975  loss_cate_4: 0  loss_mask_4: 0.9878  loss_dice_4: 1.246  loss_ce_5: 0.3526  loss_cate_5: 0  loss_mask_5: 0.993  loss_dice_5: 1.28  loss_ce_6: 0.3295  loss_cate_6: 0  loss_mask_6: 0.9866  loss_dice_6: 1.248  loss_ce_7: 0.3517  loss_cate_7: 0  loss_mask_7: 0.9954  loss_dice_7: 1.26  loss_ce_8: 0.303  loss_cate_8: 0  loss_mask_8: 1.014  loss_dice_8: 1.268  time: 1.1025  data_time: 0.0129  lr: 9.8628e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:53:12 d2.utils.events]: \u001b[0m eta: 1 day, 0:01:10  iter: 1239  total_loss: 23.83  loss_ce: 0.2794  loss_cate: 0.2657  loss_mask: 1.045  loss_dice: 1.094  loss_ce_0: 0.5748  loss_cate_0: 0  loss_mask_0: 1.033  loss_dice_0: 1.206  loss_ce_1: 0.2882  loss_cate_1: 0  loss_mask_1: 1.029  loss_dice_1: 1.164  loss_ce_2: 0.2379  loss_cate_2: 0  loss_mask_2: 1.066  loss_dice_2: 1.133  loss_ce_3: 0.242  loss_cate_3: 0  loss_mask_3: 1.038  loss_dice_3: 1.12  loss_ce_4: 0.272  loss_cate_4: 0  loss_mask_4: 1.073  loss_dice_4: 1.083  loss_ce_5: 0.3488  loss_cate_5: 0  loss_mask_5: 1.034  loss_dice_5: 1.112  loss_ce_6: 0.2688  loss_cate_6: 0  loss_mask_6: 1.023  loss_dice_6: 1.123  loss_ce_7: 0.2902  loss_cate_7: 0  loss_mask_7: 1.04  loss_dice_7: 1.091  loss_ce_8: 0.2694  loss_cate_8: 0  loss_mask_8: 1.05  loss_dice_8: 1.12  time: 1.1024  data_time: 0.0120  lr: 9.8605e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:53:34 d2.utils.events]: \u001b[0m eta: 1 day, 0:00:47  iter: 1259  total_loss: 29.17  loss_ce: 0.3934  loss_cate: 0.2691  loss_mask: 1.17  loss_dice: 1.325  loss_ce_0: 0.5374  loss_cate_0: 0  loss_mask_0: 1.146  loss_dice_0: 1.335  loss_ce_1: 0.387  loss_cate_1: 0  loss_mask_1: 1.138  loss_dice_1: 1.288  loss_ce_2: 0.3671  loss_cate_2: 0  loss_mask_2: 1.185  loss_dice_2: 1.241  loss_ce_3: 0.3711  loss_cate_3: 0  loss_mask_3: 1.142  loss_dice_3: 1.24  loss_ce_4: 0.3007  loss_cate_4: 0  loss_mask_4: 1.132  loss_dice_4: 1.205  loss_ce_5: 0.3489  loss_cate_5: 0  loss_mask_5: 1.127  loss_dice_5: 1.26  loss_ce_6: 0.3709  loss_cate_6: 0  loss_mask_6: 1.166  loss_dice_6: 1.226  loss_ce_7: 0.3604  loss_cate_7: 0  loss_mask_7: 1.168  loss_dice_7: 1.236  loss_ce_8: 0.3836  loss_cate_8: 0  loss_mask_8: 1.135  loss_dice_8: 1.253  time: 1.1023  data_time: 0.0126  lr: 9.8583e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:53:56 d2.utils.events]: \u001b[0m eta: 1 day, 0:00:26  iter: 1279  total_loss: 24.67  loss_ce: 0.2396  loss_cate: 0.2763  loss_mask: 0.9444  loss_dice: 1.141  loss_ce_0: 0.5783  loss_cate_0: 0  loss_mask_0: 0.9565  loss_dice_0: 1.242  loss_ce_1: 0.3725  loss_cate_1: 0  loss_mask_1: 0.9286  loss_dice_1: 1.198  loss_ce_2: 0.2855  loss_cate_2: 0  loss_mask_2: 0.9356  loss_dice_2: 1.141  loss_ce_3: 0.272  loss_cate_3: 0  loss_mask_3: 0.9139  loss_dice_3: 1.163  loss_ce_4: 0.2237  loss_cate_4: 0  loss_mask_4: 0.9332  loss_dice_4: 1.159  loss_ce_5: 0.2271  loss_cate_5: 0  loss_mask_5: 0.9219  loss_dice_5: 1.191  loss_ce_6: 0.2716  loss_cate_6: 0  loss_mask_6: 0.9584  loss_dice_6: 1.142  loss_ce_7: 0.258  loss_cate_7: 0  loss_mask_7: 0.9569  loss_dice_7: 1.13  loss_ce_8: 0.2179  loss_cate_8: 0  loss_mask_8: 0.9668  loss_dice_8: 1.161  time: 1.1022  data_time: 0.0119  lr: 9.856e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:54:19 d2.utils.events]: \u001b[0m eta: 1 day, 0:00:04  iter: 1299  total_loss: 25.55  loss_ce: 0.1886  loss_cate: 0.2614  loss_mask: 1.084  loss_dice: 1.257  loss_ce_0: 0.5207  loss_cate_0: 0  loss_mask_0: 0.9843  loss_dice_0: 1.267  loss_ce_1: 0.2647  loss_cate_1: 0  loss_mask_1: 1.009  loss_dice_1: 1.249  loss_ce_2: 0.2797  loss_cate_2: 0  loss_mask_2: 0.9993  loss_dice_2: 1.236  loss_ce_3: 0.2475  loss_cate_3: 0  loss_mask_3: 1.04  loss_dice_3: 1.298  loss_ce_4: 0.2589  loss_cate_4: 0  loss_mask_4: 1.057  loss_dice_4: 1.255  loss_ce_5: 0.2352  loss_cate_5: 0  loss_mask_5: 1.05  loss_dice_5: 1.248  loss_ce_6: 0.2209  loss_cate_6: 0  loss_mask_6: 1.064  loss_dice_6: 1.238  loss_ce_7: 0.1829  loss_cate_7: 0  loss_mask_7: 1.081  loss_dice_7: 1.263  loss_ce_8: 0.1894  loss_cate_8: 0  loss_mask_8: 1.072  loss_dice_8: 1.275  time: 1.1022  data_time: 0.0140  lr: 9.8537e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:54:41 d2.utils.events]: \u001b[0m eta: 23:59:43  iter: 1319  total_loss: 27.2  loss_ce: 0.4554  loss_cate: 0.2596  loss_mask: 0.8796  loss_dice: 1.254  loss_ce_0: 0.6905  loss_cate_0: 0  loss_mask_0: 0.9785  loss_dice_0: 1.298  loss_ce_1: 0.4834  loss_cate_1: 0  loss_mask_1: 0.9237  loss_dice_1: 1.258  loss_ce_2: 0.5505  loss_cate_2: 0  loss_mask_2: 0.9387  loss_dice_2: 1.274  loss_ce_3: 0.4228  loss_cate_3: 0  loss_mask_3: 0.916  loss_dice_3: 1.263  loss_ce_4: 0.4787  loss_cate_4: 0  loss_mask_4: 0.8797  loss_dice_4: 1.248  loss_ce_5: 0.3923  loss_cate_5: 0  loss_mask_5: 0.9155  loss_dice_5: 1.255  loss_ce_6: 0.4229  loss_cate_6: 0  loss_mask_6: 0.9047  loss_dice_6: 1.241  loss_ce_7: 0.4428  loss_cate_7: 0  loss_mask_7: 0.8947  loss_dice_7: 1.266  loss_ce_8: 0.4289  loss_cate_8: 0  loss_mask_8: 0.865  loss_dice_8: 1.196  time: 1.1021  data_time: 0.0127  lr: 9.8515e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:55:03 d2.utils.events]: \u001b[0m eta: 23:59:20  iter: 1339  total_loss: 22.48  loss_ce: 0.304  loss_cate: 0.2687  loss_mask: 0.9168  loss_dice: 1.091  loss_ce_0: 0.6123  loss_cate_0: 0  loss_mask_0: 0.9029  loss_dice_0: 1.062  loss_ce_1: 0.432  loss_cate_1: 0  loss_mask_1: 0.8306  loss_dice_1: 1.022  loss_ce_2: 0.3546  loss_cate_2: 0  loss_mask_2: 0.9384  loss_dice_2: 1.065  loss_ce_3: 0.3311  loss_cate_3: 0  loss_mask_3: 0.9337  loss_dice_3: 1.106  loss_ce_4: 0.2948  loss_cate_4: 0  loss_mask_4: 0.9374  loss_dice_4: 1.114  loss_ce_5: 0.2992  loss_cate_5: 0  loss_mask_5: 0.9058  loss_dice_5: 1.152  loss_ce_6: 0.3144  loss_cate_6: 0  loss_mask_6: 0.8885  loss_dice_6: 1.109  loss_ce_7: 0.3082  loss_cate_7: 0  loss_mask_7: 0.9175  loss_dice_7: 1.099  loss_ce_8: 0.3151  loss_cate_8: 0  loss_mask_8: 0.9033  loss_dice_8: 1.083  time: 1.1020  data_time: 0.0128  lr: 9.8492e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:55:26 d2.utils.events]: \u001b[0m eta: 23:59:10  iter: 1359  total_loss: 27.37  loss_ce: 0.458  loss_cate: 0.2541  loss_mask: 0.9794  loss_dice: 1.161  loss_ce_0: 0.5958  loss_cate_0: 0  loss_mask_0: 1.023  loss_dice_0: 1.259  loss_ce_1: 0.4916  loss_cate_1: 0  loss_mask_1: 0.9875  loss_dice_1: 1.198  loss_ce_2: 0.4877  loss_cate_2: 0  loss_mask_2: 1.025  loss_dice_2: 1.199  loss_ce_3: 0.3805  loss_cate_3: 0  loss_mask_3: 0.9811  loss_dice_3: 1.175  loss_ce_4: 0.3879  loss_cate_4: 0  loss_mask_4: 0.9835  loss_dice_4: 1.185  loss_ce_5: 0.4174  loss_cate_5: 0  loss_mask_5: 0.9753  loss_dice_5: 1.176  loss_ce_6: 0.4317  loss_cate_6: 0  loss_mask_6: 0.9667  loss_dice_6: 1.155  loss_ce_7: 0.3767  loss_cate_7: 0  loss_mask_7: 0.943  loss_dice_7: 1.131  loss_ce_8: 0.3862  loss_cate_8: 0  loss_mask_8: 0.9669  loss_dice_8: 1.132  time: 1.1021  data_time: 0.0142  lr: 9.847e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:55:48 d2.utils.events]: \u001b[0m eta: 23:58:39  iter: 1379  total_loss: 26.11  loss_ce: 0.2926  loss_cate: 0.276  loss_mask: 1.085  loss_dice: 1.208  loss_ce_0: 0.6273  loss_cate_0: 0  loss_mask_0: 0.9982  loss_dice_0: 1.183  loss_ce_1: 0.3312  loss_cate_1: 0  loss_mask_1: 1.021  loss_dice_1: 1.22  loss_ce_2: 0.3203  loss_cate_2: 0  loss_mask_2: 1.034  loss_dice_2: 1.138  loss_ce_3: 0.2807  loss_cate_3: 0  loss_mask_3: 1.029  loss_dice_3: 1.157  loss_ce_4: 0.2965  loss_cate_4: 0  loss_mask_4: 1.045  loss_dice_4: 1.212  loss_ce_5: 0.3156  loss_cate_5: 0  loss_mask_5: 1.063  loss_dice_5: 1.147  loss_ce_6: 0.3176  loss_cate_6: 0  loss_mask_6: 1.037  loss_dice_6: 1.144  loss_ce_7: 0.3377  loss_cate_7: 0  loss_mask_7: 1.027  loss_dice_7: 1.155  loss_ce_8: 0.3069  loss_cate_8: 0  loss_mask_8: 1.043  loss_dice_8: 1.257  time: 1.1021  data_time: 0.0134  lr: 9.8447e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:56:11 d2.utils.events]: \u001b[0m eta: 23:58:25  iter: 1399  total_loss: 27.29  loss_ce: 0.3453  loss_cate: 0.2809  loss_mask: 1.004  loss_dice: 1.258  loss_ce_0: 0.5642  loss_cate_0: 0  loss_mask_0: 1.089  loss_dice_0: 1.314  loss_ce_1: 0.3996  loss_cate_1: 0  loss_mask_1: 1.042  loss_dice_1: 1.268  loss_ce_2: 0.3535  loss_cate_2: 0  loss_mask_2: 1.064  loss_dice_2: 1.309  loss_ce_3: 0.3408  loss_cate_3: 0  loss_mask_3: 1.07  loss_dice_3: 1.252  loss_ce_4: 0.2672  loss_cate_4: 0  loss_mask_4: 1.098  loss_dice_4: 1.269  loss_ce_5: 0.3004  loss_cate_5: 0  loss_mask_5: 1.098  loss_dice_5: 1.259  loss_ce_6: 0.3113  loss_cate_6: 0  loss_mask_6: 1.053  loss_dice_6: 1.29  loss_ce_7: 0.269  loss_cate_7: 0  loss_mask_7: 1.005  loss_dice_7: 1.276  loss_ce_8: 0.3507  loss_cate_8: 0  loss_mask_8: 1.016  loss_dice_8: 1.331  time: 1.1021  data_time: 0.0130  lr: 9.8425e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:56:33 d2.utils.events]: \u001b[0m eta: 23:58:08  iter: 1419  total_loss: 25.96  loss_ce: 0.3868  loss_cate: 0.2482  loss_mask: 0.9996  loss_dice: 1.132  loss_ce_0: 0.6401  loss_cate_0: 0  loss_mask_0: 0.9749  loss_dice_0: 1.204  loss_ce_1: 0.4011  loss_cate_1: 0  loss_mask_1: 0.9788  loss_dice_1: 1.162  loss_ce_2: 0.3825  loss_cate_2: 0  loss_mask_2: 0.9524  loss_dice_2: 1.126  loss_ce_3: 0.3475  loss_cate_3: 0  loss_mask_3: 0.9705  loss_dice_3: 1.073  loss_ce_4: 0.3479  loss_cate_4: 0  loss_mask_4: 1.028  loss_dice_4: 1.099  loss_ce_5: 0.3482  loss_cate_5: 0  loss_mask_5: 1.002  loss_dice_5: 1.121  loss_ce_6: 0.3597  loss_cate_6: 0  loss_mask_6: 0.981  loss_dice_6: 1.134  loss_ce_7: 0.34  loss_cate_7: 0  loss_mask_7: 0.9782  loss_dice_7: 1.155  loss_ce_8: 0.3817  loss_cate_8: 0  loss_mask_8: 1.008  loss_dice_8: 1.169  time: 1.1021  data_time: 0.0127  lr: 9.8402e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:56:55 d2.utils.events]: \u001b[0m eta: 23:58:01  iter: 1439  total_loss: 26.15  loss_ce: 0.3136  loss_cate: 0.2636  loss_mask: 1.001  loss_dice: 1.138  loss_ce_0: 0.5438  loss_cate_0: 0  loss_mask_0: 1.066  loss_dice_0: 1.171  loss_ce_1: 0.3864  loss_cate_1: 0  loss_mask_1: 0.9776  loss_dice_1: 1.146  loss_ce_2: 0.3752  loss_cate_2: 0  loss_mask_2: 0.9816  loss_dice_2: 1.174  loss_ce_3: 0.3396  loss_cate_3: 0  loss_mask_3: 0.9781  loss_dice_3: 1.165  loss_ce_4: 0.3189  loss_cate_4: 0  loss_mask_4: 0.9962  loss_dice_4: 1.165  loss_ce_5: 0.3413  loss_cate_5: 0  loss_mask_5: 0.9736  loss_dice_5: 1.151  loss_ce_6: 0.3116  loss_cate_6: 0  loss_mask_6: 0.9821  loss_dice_6: 1.113  loss_ce_7: 0.3134  loss_cate_7: 0  loss_mask_7: 0.9971  loss_dice_7: 1.148  loss_ce_8: 0.3061  loss_cate_8: 0  loss_mask_8: 0.9905  loss_dice_8: 1.173  time: 1.1021  data_time: 0.0141  lr: 9.838e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:57:19 d2.utils.events]: \u001b[0m eta: 23:57:22  iter: 1459  total_loss: 29.25  loss_ce: 0.4232  loss_cate: 0.2634  loss_mask: 1.107  loss_dice: 1.274  loss_ce_0: 0.6157  loss_cate_0: 0  loss_mask_0: 1.152  loss_dice_0: 1.254  loss_ce_1: 0.3862  loss_cate_1: 0  loss_mask_1: 1.12  loss_dice_1: 1.362  loss_ce_2: 0.4396  loss_cate_2: 0  loss_mask_2: 1.15  loss_dice_2: 1.267  loss_ce_3: 0.4564  loss_cate_3: 0  loss_mask_3: 1.069  loss_dice_3: 1.257  loss_ce_4: 0.4227  loss_cate_4: 0  loss_mask_4: 1.145  loss_dice_4: 1.24  loss_ce_5: 0.4258  loss_cate_5: 0  loss_mask_5: 1.102  loss_dice_5: 1.235  loss_ce_6: 0.4445  loss_cate_6: 0  loss_mask_6: 1.011  loss_dice_6: 1.244  loss_ce_7: 0.4399  loss_cate_7: 0  loss_mask_7: 1.03  loss_dice_7: 1.25  loss_ce_8: 0.4528  loss_cate_8: 0  loss_mask_8: 1.081  loss_dice_8: 1.251  time: 1.1021  data_time: 0.0131  lr: 9.8357e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:57:41 d2.utils.events]: \u001b[0m eta: 23:57:02  iter: 1479  total_loss: 25.69  loss_ce: 0.3877  loss_cate: 0.2619  loss_mask: 0.9089  loss_dice: 1.097  loss_ce_0: 0.6037  loss_cate_0: 0  loss_mask_0: 0.9384  loss_dice_0: 1.158  loss_ce_1: 0.3772  loss_cate_1: 0  loss_mask_1: 0.9357  loss_dice_1: 1.248  loss_ce_2: 0.3709  loss_cate_2: 0  loss_mask_2: 0.9041  loss_dice_2: 1.12  loss_ce_3: 0.3981  loss_cate_3: 0  loss_mask_3: 0.8876  loss_dice_3: 1.016  loss_ce_4: 0.4117  loss_cate_4: 0  loss_mask_4: 0.8988  loss_dice_4: 1.178  loss_ce_5: 0.3765  loss_cate_5: 0  loss_mask_5: 0.9154  loss_dice_5: 1.052  loss_ce_6: 0.392  loss_cate_6: 0  loss_mask_6: 0.867  loss_dice_6: 1.113  loss_ce_7: 0.373  loss_cate_7: 0  loss_mask_7: 0.9142  loss_dice_7: 1.113  loss_ce_8: 0.3936  loss_cate_8: 0  loss_mask_8: 0.9067  loss_dice_8: 1.121  time: 1.1020  data_time: 0.0125  lr: 9.8335e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:58:03 d2.utils.events]: \u001b[0m eta: 23:56:42  iter: 1499  total_loss: 27.08  loss_ce: 0.4266  loss_cate: 0.2688  loss_mask: 0.9664  loss_dice: 1.257  loss_ce_0: 0.7395  loss_cate_0: 0  loss_mask_0: 0.9591  loss_dice_0: 1.271  loss_ce_1: 0.489  loss_cate_1: 0  loss_mask_1: 1.052  loss_dice_1: 1.338  loss_ce_2: 0.4813  loss_cate_2: 0  loss_mask_2: 0.9601  loss_dice_2: 1.276  loss_ce_3: 0.4395  loss_cate_3: 0  loss_mask_3: 0.9386  loss_dice_3: 1.218  loss_ce_4: 0.4411  loss_cate_4: 0  loss_mask_4: 0.9462  loss_dice_4: 1.212  loss_ce_5: 0.4116  loss_cate_5: 0  loss_mask_5: 1.004  loss_dice_5: 1.222  loss_ce_6: 0.4639  loss_cate_6: 0  loss_mask_6: 0.9169  loss_dice_6: 1.241  loss_ce_7: 0.4329  loss_cate_7: 0  loss_mask_7: 0.9952  loss_dice_7: 1.235  loss_ce_8: 0.3861  loss_cate_8: 0  loss_mask_8: 0.9447  loss_dice_8: 1.246  time: 1.1020  data_time: 0.0132  lr: 9.8312e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:58:26 d2.utils.events]: \u001b[0m eta: 23:56:42  iter: 1519  total_loss: 26.5  loss_ce: 0.4132  loss_cate: 0.2564  loss_mask: 0.8731  loss_dice: 1.175  loss_ce_0: 0.6462  loss_cate_0: 0  loss_mask_0: 0.8422  loss_dice_0: 1.207  loss_ce_1: 0.4635  loss_cate_1: 0  loss_mask_1: 0.9011  loss_dice_1: 1.214  loss_ce_2: 0.4449  loss_cate_2: 0  loss_mask_2: 0.8714  loss_dice_2: 1.215  loss_ce_3: 0.4928  loss_cate_3: 0  loss_mask_3: 0.8423  loss_dice_3: 1.176  loss_ce_4: 0.468  loss_cate_4: 0  loss_mask_4: 0.8794  loss_dice_4: 1.155  loss_ce_5: 0.4571  loss_cate_5: 0  loss_mask_5: 0.8766  loss_dice_5: 1.135  loss_ce_6: 0.4294  loss_cate_6: 0  loss_mask_6: 0.8622  loss_dice_6: 1.099  loss_ce_7: 0.4265  loss_cate_7: 0  loss_mask_7: 0.8705  loss_dice_7: 1.197  loss_ce_8: 0.4834  loss_cate_8: 0  loss_mask_8: 0.8437  loss_dice_8: 1.148  time: 1.1020  data_time: 0.0131  lr: 9.8289e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:58:48 d2.utils.events]: \u001b[0m eta: 23:55:52  iter: 1539  total_loss: 27.26  loss_ce: 0.3898  loss_cate: 0.2623  loss_mask: 1.07  loss_dice: 1.273  loss_ce_0: 0.5713  loss_cate_0: 0  loss_mask_0: 1.037  loss_dice_0: 1.342  loss_ce_1: 0.3581  loss_cate_1: 0  loss_mask_1: 1.057  loss_dice_1: 1.289  loss_ce_2: 0.4164  loss_cate_2: 0  loss_mask_2: 1.028  loss_dice_2: 1.272  loss_ce_3: 0.3885  loss_cate_3: 0  loss_mask_3: 1.039  loss_dice_3: 1.212  loss_ce_4: 0.4451  loss_cate_4: 0  loss_mask_4: 1.069  loss_dice_4: 1.267  loss_ce_5: 0.4059  loss_cate_5: 0  loss_mask_5: 1.034  loss_dice_5: 1.265  loss_ce_6: 0.4206  loss_cate_6: 0  loss_mask_6: 1.044  loss_dice_6: 1.271  loss_ce_7: 0.371  loss_cate_7: 0  loss_mask_7: 1.03  loss_dice_7: 1.25  loss_ce_8: 0.2786  loss_cate_8: 0  loss_mask_8: 1.046  loss_dice_8: 1.209  time: 1.1019  data_time: 0.0129  lr: 9.8267e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:59:10 d2.utils.events]: \u001b[0m eta: 23:55:27  iter: 1559  total_loss: 26.07  loss_ce: 0.2646  loss_cate: 0.3295  loss_mask: 0.9756  loss_dice: 1.204  loss_ce_0: 0.4854  loss_cate_0: 0  loss_mask_0: 0.9862  loss_dice_0: 1.318  loss_ce_1: 0.323  loss_cate_1: 0  loss_mask_1: 0.9699  loss_dice_1: 1.286  loss_ce_2: 0.3568  loss_cate_2: 0  loss_mask_2: 0.9765  loss_dice_2: 1.217  loss_ce_3: 0.3546  loss_cate_3: 0  loss_mask_3: 0.9404  loss_dice_3: 1.193  loss_ce_4: 0.3381  loss_cate_4: 0  loss_mask_4: 0.9379  loss_dice_4: 1.228  loss_ce_5: 0.3177  loss_cate_5: 0  loss_mask_5: 0.9727  loss_dice_5: 1.21  loss_ce_6: 0.289  loss_cate_6: 0  loss_mask_6: 0.9705  loss_dice_6: 1.189  loss_ce_7: 0.3343  loss_cate_7: 0  loss_mask_7: 0.9523  loss_dice_7: 1.185  loss_ce_8: 0.3313  loss_cate_8: 0  loss_mask_8: 0.9722  loss_dice_8: 1.148  time: 1.1020  data_time: 0.0261  lr: 9.8244e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:59:33 d2.utils.events]: \u001b[0m eta: 23:55:13  iter: 1579  total_loss: 27.02  loss_ce: 0.3369  loss_cate: 0.2605  loss_mask: 1.021  loss_dice: 1.216  loss_ce_0: 0.6248  loss_cate_0: 0  loss_mask_0: 1.042  loss_dice_0: 1.348  loss_ce_1: 0.4236  loss_cate_1: 0  loss_mask_1: 1.028  loss_dice_1: 1.314  loss_ce_2: 0.3882  loss_cate_2: 0  loss_mask_2: 1.044  loss_dice_2: 1.221  loss_ce_3: 0.3944  loss_cate_3: 0  loss_mask_3: 1.041  loss_dice_3: 1.222  loss_ce_4: 0.4023  loss_cate_4: 0  loss_mask_4: 1.041  loss_dice_4: 1.292  loss_ce_5: 0.4196  loss_cate_5: 0  loss_mask_5: 1.023  loss_dice_5: 1.284  loss_ce_6: 0.403  loss_cate_6: 0  loss_mask_6: 1.045  loss_dice_6: 1.252  loss_ce_7: 0.357  loss_cate_7: 0  loss_mask_7: 1.011  loss_dice_7: 1.241  loss_ce_8: 0.3845  loss_cate_8: 0  loss_mask_8: 1.025  loss_dice_8: 1.226  time: 1.1021  data_time: 0.0133  lr: 9.8222e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 11:59:56 d2.utils.events]: \u001b[0m eta: 23:55:02  iter: 1599  total_loss: 23.3  loss_ce: 0.3228  loss_cate: 0.2178  loss_mask: 0.8341  loss_dice: 1.122  loss_ce_0: 0.5228  loss_cate_0: 0  loss_mask_0: 0.7948  loss_dice_0: 1.144  loss_ce_1: 0.4124  loss_cate_1: 0  loss_mask_1: 0.8145  loss_dice_1: 1.169  loss_ce_2: 0.4149  loss_cate_2: 0  loss_mask_2: 0.7774  loss_dice_2: 1.123  loss_ce_3: 0.4012  loss_cate_3: 0  loss_mask_3: 0.7482  loss_dice_3: 1.061  loss_ce_4: 0.3948  loss_cate_4: 0  loss_mask_4: 0.7514  loss_dice_4: 1.092  loss_ce_5: 0.3892  loss_cate_5: 0  loss_mask_5: 0.7499  loss_dice_5: 1.107  loss_ce_6: 0.3995  loss_cate_6: 0  loss_mask_6: 0.7592  loss_dice_6: 1.05  loss_ce_7: 0.3858  loss_cate_7: 0  loss_mask_7: 0.7538  loss_dice_7: 1.081  loss_ce_8: 0.3769  loss_cate_8: 0  loss_mask_8: 0.7721  loss_dice_8: 1.084  time: 1.1023  data_time: 0.0213  lr: 9.8199e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:00:18 d2.utils.events]: \u001b[0m eta: 23:54:56  iter: 1619  total_loss: 25.93  loss_ce: 0.3296  loss_cate: 0.2332  loss_mask: 0.9753  loss_dice: 1.18  loss_ce_0: 0.5411  loss_cate_0: 0  loss_mask_0: 0.9474  loss_dice_0: 1.207  loss_ce_1: 0.4351  loss_cate_1: 0  loss_mask_1: 1.001  loss_dice_1: 1.19  loss_ce_2: 0.4414  loss_cate_2: 0  loss_mask_2: 0.9496  loss_dice_2: 1.178  loss_ce_3: 0.4049  loss_cate_3: 0  loss_mask_3: 0.9378  loss_dice_3: 1.147  loss_ce_4: 0.3734  loss_cate_4: 0  loss_mask_4: 0.9512  loss_dice_4: 1.15  loss_ce_5: 0.3323  loss_cate_5: 0  loss_mask_5: 0.9458  loss_dice_5: 1.171  loss_ce_6: 0.3038  loss_cate_6: 0  loss_mask_6: 0.9659  loss_dice_6: 1.167  loss_ce_7: 0.33  loss_cate_7: 0  loss_mask_7: 0.9631  loss_dice_7: 1.149  loss_ce_8: 0.2996  loss_cate_8: 0  loss_mask_8: 1.016  loss_dice_8: 1.168  time: 1.1023  data_time: 0.0132  lr: 9.8177e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:00:41 d2.utils.events]: \u001b[0m eta: 23:54:45  iter: 1639  total_loss: 25.47  loss_ce: 0.3369  loss_cate: 0.2196  loss_mask: 1.002  loss_dice: 1.214  loss_ce_0: 0.5003  loss_cate_0: 0  loss_mask_0: 1.008  loss_dice_0: 1.287  loss_ce_1: 0.3461  loss_cate_1: 0  loss_mask_1: 1.026  loss_dice_1: 1.292  loss_ce_2: 0.3465  loss_cate_2: 0  loss_mask_2: 1.054  loss_dice_2: 1.243  loss_ce_3: 0.3339  loss_cate_3: 0  loss_mask_3: 1.019  loss_dice_3: 1.245  loss_ce_4: 0.3547  loss_cate_4: 0  loss_mask_4: 1.009  loss_dice_4: 1.238  loss_ce_5: 0.348  loss_cate_5: 0  loss_mask_5: 0.9952  loss_dice_5: 1.23  loss_ce_6: 0.3154  loss_cate_6: 0  loss_mask_6: 0.9997  loss_dice_6: 1.2  loss_ce_7: 0.3299  loss_cate_7: 0  loss_mask_7: 1.01  loss_dice_7: 1.222  loss_ce_8: 0.3104  loss_cate_8: 0  loss_mask_8: 0.9592  loss_dice_8: 1.196  time: 1.1025  data_time: 0.0139  lr: 9.8154e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:01:03 d2.utils.events]: \u001b[0m eta: 23:54:23  iter: 1659  total_loss: 29.47  loss_ce: 0.3678  loss_cate: 0.24  loss_mask: 1.075  loss_dice: 1.265  loss_ce_0: 0.6058  loss_cate_0: 0  loss_mask_0: 1.09  loss_dice_0: 1.324  loss_ce_1: 0.4188  loss_cate_1: 0  loss_mask_1: 1.078  loss_dice_1: 1.327  loss_ce_2: 0.4404  loss_cate_2: 0  loss_mask_2: 1.06  loss_dice_2: 1.328  loss_ce_3: 0.3743  loss_cate_3: 0  loss_mask_3: 1.073  loss_dice_3: 1.308  loss_ce_4: 0.3515  loss_cate_4: 0  loss_mask_4: 1.06  loss_dice_4: 1.271  loss_ce_5: 0.4132  loss_cate_5: 0  loss_mask_5: 1.065  loss_dice_5: 1.3  loss_ce_6: 0.3831  loss_cate_6: 0  loss_mask_6: 1.105  loss_dice_6: 1.265  loss_ce_7: 0.3943  loss_cate_7: 0  loss_mask_7: 1.114  loss_dice_7: 1.344  loss_ce_8: 0.4017  loss_cate_8: 0  loss_mask_8: 1.09  loss_dice_8: 1.327  time: 1.1026  data_time: 0.0158  lr: 9.8132e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:01:26 d2.utils.events]: \u001b[0m eta: 23:54:10  iter: 1679  total_loss: 23.98  loss_ce: 0.2628  loss_cate: 0.2573  loss_mask: 0.8583  loss_dice: 1.079  loss_ce_0: 0.6057  loss_cate_0: 0  loss_mask_0: 0.8609  loss_dice_0: 1.199  loss_ce_1: 0.3086  loss_cate_1: 0  loss_mask_1: 0.8681  loss_dice_1: 1.254  loss_ce_2: 0.3221  loss_cate_2: 0  loss_mask_2: 0.8631  loss_dice_2: 1.115  loss_ce_3: 0.3045  loss_cate_3: 0  loss_mask_3: 0.8542  loss_dice_3: 1.094  loss_ce_4: 0.3441  loss_cate_4: 0  loss_mask_4: 0.8214  loss_dice_4: 1.121  loss_ce_5: 0.2817  loss_cate_5: 0  loss_mask_5: 0.8474  loss_dice_5: 1.145  loss_ce_6: 0.289  loss_cate_6: 0  loss_mask_6: 0.8595  loss_dice_6: 1.116  loss_ce_7: 0.295  loss_cate_7: 0  loss_mask_7: 0.8709  loss_dice_7: 1.132  loss_ce_8: 0.2802  loss_cate_8: 0  loss_mask_8: 0.8463  loss_dice_8: 1.108  time: 1.1028  data_time: 0.0155  lr: 9.8109e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:01:49 d2.utils.events]: \u001b[0m eta: 23:54:01  iter: 1699  total_loss: 24.93  loss_ce: 0.2873  loss_cate: 0.2174  loss_mask: 1.004  loss_dice: 1.103  loss_ce_0: 0.4402  loss_cate_0: 0  loss_mask_0: 0.9703  loss_dice_0: 1.249  loss_ce_1: 0.368  loss_cate_1: 0  loss_mask_1: 0.9607  loss_dice_1: 1.058  loss_ce_2: 0.3401  loss_cate_2: 0  loss_mask_2: 1.024  loss_dice_2: 1.109  loss_ce_3: 0.2764  loss_cate_3: 0  loss_mask_3: 1.001  loss_dice_3: 1.113  loss_ce_4: 0.2971  loss_cate_4: 0  loss_mask_4: 1.008  loss_dice_4: 1.045  loss_ce_5: 0.2749  loss_cate_5: 0  loss_mask_5: 1.02  loss_dice_5: 1.133  loss_ce_6: 0.2653  loss_cate_6: 0  loss_mask_6: 0.9923  loss_dice_6: 1.106  loss_ce_7: 0.258  loss_cate_7: 0  loss_mask_7: 0.9994  loss_dice_7: 1.126  loss_ce_8: 0.271  loss_cate_8: 0  loss_mask_8: 0.9788  loss_dice_8: 1.157  time: 1.1029  data_time: 0.0139  lr: 9.8087e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:02:11 d2.utils.events]: \u001b[0m eta: 23:53:45  iter: 1719  total_loss: 24.46  loss_ce: 0.198  loss_cate: 0.2553  loss_mask: 0.983  loss_dice: 1.102  loss_ce_0: 0.4692  loss_cate_0: 0  loss_mask_0: 0.9603  loss_dice_0: 1.244  loss_ce_1: 0.3141  loss_cate_1: 0  loss_mask_1: 0.9426  loss_dice_1: 1.157  loss_ce_2: 0.2423  loss_cate_2: 0  loss_mask_2: 0.9816  loss_dice_2: 1.061  loss_ce_3: 0.2645  loss_cate_3: 0  loss_mask_3: 0.9465  loss_dice_3: 1.047  loss_ce_4: 0.2347  loss_cate_4: 0  loss_mask_4: 0.94  loss_dice_4: 1.092  loss_ce_5: 0.2242  loss_cate_5: 0  loss_mask_5: 0.9714  loss_dice_5: 1.108  loss_ce_6: 0.1624  loss_cate_6: 0  loss_mask_6: 1.007  loss_dice_6: 1.113  loss_ce_7: 0.2437  loss_cate_7: 0  loss_mask_7: 0.9666  loss_dice_7: 1.153  loss_ce_8: 0.1436  loss_cate_8: 0  loss_mask_8: 0.9189  loss_dice_8: 1.14  time: 1.1030  data_time: 0.0164  lr: 9.8064e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:02:34 d2.utils.events]: \u001b[0m eta: 23:53:50  iter: 1739  total_loss: 27.46  loss_ce: 0.2745  loss_cate: 0.2469  loss_mask: 1.091  loss_dice: 1.362  loss_ce_0: 0.4093  loss_cate_0: 0  loss_mask_0: 1.11  loss_dice_0: 1.35  loss_ce_1: 0.2898  loss_cate_1: 0  loss_mask_1: 1.072  loss_dice_1: 1.378  loss_ce_2: 0.3048  loss_cate_2: 0  loss_mask_2: 1.059  loss_dice_2: 1.334  loss_ce_3: 0.2458  loss_cate_3: 0  loss_mask_3: 1.055  loss_dice_3: 1.386  loss_ce_4: 0.2899  loss_cate_4: 0  loss_mask_4: 1.093  loss_dice_4: 1.367  loss_ce_5: 0.2868  loss_cate_5: 0  loss_mask_5: 1.048  loss_dice_5: 1.347  loss_ce_6: 0.2977  loss_cate_6: 0  loss_mask_6: 1.068  loss_dice_6: 1.362  loss_ce_7: 0.2788  loss_cate_7: 0  loss_mask_7: 1.085  loss_dice_7: 1.368  loss_ce_8: 0.2776  loss_cate_8: 0  loss_mask_8: 1.103  loss_dice_8: 1.358  time: 1.1031  data_time: 0.0149  lr: 9.8041e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:02:56 d2.utils.events]: \u001b[0m eta: 23:53:44  iter: 1759  total_loss: 24.33  loss_ce: 0.3232  loss_cate: 0.2039  loss_mask: 0.956  loss_dice: 1.009  loss_ce_0: 0.4704  loss_cate_0: 0  loss_mask_0: 0.9402  loss_dice_0: 1.095  loss_ce_1: 0.3213  loss_cate_1: 0  loss_mask_1: 1.035  loss_dice_1: 1.031  loss_ce_2: 0.3056  loss_cate_2: 0  loss_mask_2: 1.011  loss_dice_2: 1.102  loss_ce_3: 0.2569  loss_cate_3: 0  loss_mask_3: 1.035  loss_dice_3: 1.077  loss_ce_4: 0.256  loss_cate_4: 0  loss_mask_4: 1.002  loss_dice_4: 1.055  loss_ce_5: 0.3207  loss_cate_5: 0  loss_mask_5: 0.9774  loss_dice_5: 1.071  loss_ce_6: 0.2883  loss_cate_6: 0  loss_mask_6: 1.006  loss_dice_6: 1.061  loss_ce_7: 0.3099  loss_cate_7: 0  loss_mask_7: 0.9758  loss_dice_7: 1.08  loss_ce_8: 0.3576  loss_cate_8: 0  loss_mask_8: 0.985  loss_dice_8: 1.001  time: 1.1031  data_time: 0.0129  lr: 9.8019e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:03:19 d2.utils.events]: \u001b[0m eta: 23:53:28  iter: 1779  total_loss: 25  loss_ce: 0.3054  loss_cate: 0.2417  loss_mask: 0.9299  loss_dice: 1.081  loss_ce_0: 0.4901  loss_cate_0: 0  loss_mask_0: 0.9232  loss_dice_0: 1.209  loss_ce_1: 0.3472  loss_cate_1: 0  loss_mask_1: 0.9321  loss_dice_1: 1.158  loss_ce_2: 0.3423  loss_cate_2: 0  loss_mask_2: 0.9456  loss_dice_2: 1.089  loss_ce_3: 0.3244  loss_cate_3: 0  loss_mask_3: 0.9572  loss_dice_3: 1.152  loss_ce_4: 0.3312  loss_cate_4: 0  loss_mask_4: 0.9539  loss_dice_4: 1.092  loss_ce_5: 0.2499  loss_cate_5: 0  loss_mask_5: 0.9649  loss_dice_5: 1.122  loss_ce_6: 0.2693  loss_cate_6: 0  loss_mask_6: 0.9638  loss_dice_6: 1.159  loss_ce_7: 0.2735  loss_cate_7: 0  loss_mask_7: 0.9762  loss_dice_7: 1.188  loss_ce_8: 0.2857  loss_cate_8: 0  loss_mask_8: 0.9295  loss_dice_8: 1.087  time: 1.1031  data_time: 0.0147  lr: 9.7996e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:03:42 d2.utils.events]: \u001b[0m eta: 23:53:06  iter: 1799  total_loss: 25.23  loss_ce: 0.3878  loss_cate: 0.2247  loss_mask: 0.9714  loss_dice: 1.218  loss_ce_0: 0.552  loss_cate_0: 0  loss_mask_0: 0.9541  loss_dice_0: 1.281  loss_ce_1: 0.3909  loss_cate_1: 0  loss_mask_1: 1.007  loss_dice_1: 1.166  loss_ce_2: 0.3666  loss_cate_2: 0  loss_mask_2: 1.006  loss_dice_2: 1.232  loss_ce_3: 0.4277  loss_cate_3: 0  loss_mask_3: 0.9843  loss_dice_3: 1.193  loss_ce_4: 0.3608  loss_cate_4: 0  loss_mask_4: 0.9687  loss_dice_4: 1.234  loss_ce_5: 0.3102  loss_cate_5: 0  loss_mask_5: 0.987  loss_dice_5: 1.214  loss_ce_6: 0.3217  loss_cate_6: 0  loss_mask_6: 0.9546  loss_dice_6: 1.222  loss_ce_7: 0.3778  loss_cate_7: 0  loss_mask_7: 0.9537  loss_dice_7: 1.176  loss_ce_8: 0.3419  loss_cate_8: 0  loss_mask_8: 0.9912  loss_dice_8: 1.163  time: 1.1032  data_time: 0.0161  lr: 9.7974e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:04:05 d2.utils.events]: \u001b[0m eta: 23:53:01  iter: 1819  total_loss: 24.13  loss_ce: 0.2684  loss_cate: 0.2487  loss_mask: 0.9292  loss_dice: 1.022  loss_ce_0: 0.47  loss_cate_0: 0  loss_mask_0: 0.9678  loss_dice_0: 1.145  loss_ce_1: 0.3314  loss_cate_1: 0  loss_mask_1: 0.9635  loss_dice_1: 1.089  loss_ce_2: 0.2715  loss_cate_2: 0  loss_mask_2: 0.9404  loss_dice_2: 1.081  loss_ce_3: 0.3698  loss_cate_3: 0  loss_mask_3: 0.9319  loss_dice_3: 1.024  loss_ce_4: 0.2817  loss_cate_4: 0  loss_mask_4: 0.9372  loss_dice_4: 1.005  loss_ce_5: 0.2421  loss_cate_5: 0  loss_mask_5: 0.9466  loss_dice_5: 1.038  loss_ce_6: 0.2741  loss_cate_6: 0  loss_mask_6: 0.918  loss_dice_6: 1.07  loss_ce_7: 0.2302  loss_cate_7: 0  loss_mask_7: 0.9555  loss_dice_7: 1.071  loss_ce_8: 0.2994  loss_cate_8: 0  loss_mask_8: 0.9295  loss_dice_8: 1.033  time: 1.1033  data_time: 0.0142  lr: 9.7951e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:04:27 d2.utils.events]: \u001b[0m eta: 23:52:44  iter: 1839  total_loss: 24.98  loss_ce: 0.2752  loss_cate: 0.1933  loss_mask: 0.9107  loss_dice: 1.307  loss_ce_0: 0.4842  loss_cate_0: 0  loss_mask_0: 0.9358  loss_dice_0: 1.372  loss_ce_1: 0.3549  loss_cate_1: 0  loss_mask_1: 0.9454  loss_dice_1: 1.257  loss_ce_2: 0.3619  loss_cate_2: 0  loss_mask_2: 0.9175  loss_dice_2: 1.261  loss_ce_3: 0.3186  loss_cate_3: 0  loss_mask_3: 0.9073  loss_dice_3: 1.267  loss_ce_4: 0.3693  loss_cate_4: 0  loss_mask_4: 0.906  loss_dice_4: 1.243  loss_ce_5: 0.2757  loss_cate_5: 0  loss_mask_5: 0.8925  loss_dice_5: 1.272  loss_ce_6: 0.2933  loss_cate_6: 0  loss_mask_6: 0.91  loss_dice_6: 1.232  loss_ce_7: 0.3335  loss_cate_7: 0  loss_mask_7: 0.9278  loss_dice_7: 1.231  loss_ce_8: 0.3546  loss_cate_8: 0  loss_mask_8: 0.9316  loss_dice_8: 1.231  time: 1.1033  data_time: 0.0130  lr: 9.7929e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:04:50 d2.utils.events]: \u001b[0m eta: 23:52:22  iter: 1859  total_loss: 24.76  loss_ce: 0.3283  loss_cate: 0.2338  loss_mask: 0.9931  loss_dice: 1.223  loss_ce_0: 0.5701  loss_cate_0: 0  loss_mask_0: 0.9095  loss_dice_0: 1.193  loss_ce_1: 0.4193  loss_cate_1: 0  loss_mask_1: 0.9303  loss_dice_1: 1.192  loss_ce_2: 0.3343  loss_cate_2: 0  loss_mask_2: 1.025  loss_dice_2: 1.219  loss_ce_3: 0.3334  loss_cate_3: 0  loss_mask_3: 0.9493  loss_dice_3: 1.175  loss_ce_4: 0.3163  loss_cate_4: 0  loss_mask_4: 0.9618  loss_dice_4: 1.195  loss_ce_5: 0.3159  loss_cate_5: 0  loss_mask_5: 1.027  loss_dice_5: 1.205  loss_ce_6: 0.3656  loss_cate_6: 0  loss_mask_6: 0.996  loss_dice_6: 1.182  loss_ce_7: 0.3144  loss_cate_7: 0  loss_mask_7: 0.9654  loss_dice_7: 1.243  loss_ce_8: 0.3586  loss_cate_8: 0  loss_mask_8: 0.9826  loss_dice_8: 1.219  time: 1.1033  data_time: 0.0129  lr: 9.7906e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:05:12 d2.utils.events]: \u001b[0m eta: 23:51:58  iter: 1879  total_loss: 27.53  loss_ce: 0.3394  loss_cate: 0.2513  loss_mask: 0.9548  loss_dice: 1.171  loss_ce_0: 0.4158  loss_cate_0: 0  loss_mask_0: 1.031  loss_dice_0: 1.321  loss_ce_1: 0.3149  loss_cate_1: 0  loss_mask_1: 1.029  loss_dice_1: 1.257  loss_ce_2: 0.2782  loss_cate_2: 0  loss_mask_2: 1.013  loss_dice_2: 1.238  loss_ce_3: 0.3151  loss_cate_3: 0  loss_mask_3: 0.9979  loss_dice_3: 1.208  loss_ce_4: 0.3107  loss_cate_4: 0  loss_mask_4: 0.998  loss_dice_4: 1.239  loss_ce_5: 0.2835  loss_cate_5: 0  loss_mask_5: 1.016  loss_dice_5: 1.214  loss_ce_6: 0.3515  loss_cate_6: 0  loss_mask_6: 0.9795  loss_dice_6: 1.185  loss_ce_7: 0.2825  loss_cate_7: 0  loss_mask_7: 0.9878  loss_dice_7: 1.235  loss_ce_8: 0.3034  loss_cate_8: 0  loss_mask_8: 0.9875  loss_dice_8: 1.254  time: 1.1032  data_time: 0.0129  lr: 9.7884e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:05:34 d2.utils.events]: \u001b[0m eta: 23:51:40  iter: 1899  total_loss: 25.02  loss_ce: 0.2331  loss_cate: 0.2352  loss_mask: 0.9986  loss_dice: 1.151  loss_ce_0: 0.4535  loss_cate_0: 0  loss_mask_0: 1.031  loss_dice_0: 1.318  loss_ce_1: 0.3241  loss_cate_1: 0  loss_mask_1: 1.016  loss_dice_1: 1.184  loss_ce_2: 0.2674  loss_cate_2: 0  loss_mask_2: 1.009  loss_dice_2: 1.189  loss_ce_3: 0.3147  loss_cate_3: 0  loss_mask_3: 0.9914  loss_dice_3: 1.153  loss_ce_4: 0.2531  loss_cate_4: 0  loss_mask_4: 1.002  loss_dice_4: 1.134  loss_ce_5: 0.2472  loss_cate_5: 0  loss_mask_5: 1.018  loss_dice_5: 1.138  loss_ce_6: 0.2586  loss_cate_6: 0  loss_mask_6: 1.009  loss_dice_6: 1.141  loss_ce_7: 0.2366  loss_cate_7: 0  loss_mask_7: 1.022  loss_dice_7: 1.172  loss_ce_8: 0.2324  loss_cate_8: 0  loss_mask_8: 1.02  loss_dice_8: 1.152  time: 1.1031  data_time: 0.0135  lr: 9.7861e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:05:56 d2.utils.events]: \u001b[0m eta: 23:51:32  iter: 1919  total_loss: 25.51  loss_ce: 0.3159  loss_cate: 0.2422  loss_mask: 1.002  loss_dice: 1.273  loss_ce_0: 0.5399  loss_cate_0: 0  loss_mask_0: 1.02  loss_dice_0: 1.339  loss_ce_1: 0.3831  loss_cate_1: 0  loss_mask_1: 1.005  loss_dice_1: 1.246  loss_ce_2: 0.3207  loss_cate_2: 0  loss_mask_2: 0.9837  loss_dice_2: 1.295  loss_ce_3: 0.3312  loss_cate_3: 0  loss_mask_3: 0.9875  loss_dice_3: 1.252  loss_ce_4: 0.2888  loss_cate_4: 0  loss_mask_4: 1.011  loss_dice_4: 1.267  loss_ce_5: 0.3231  loss_cate_5: 0  loss_mask_5: 0.9999  loss_dice_5: 1.253  loss_ce_6: 0.3346  loss_cate_6: 0  loss_mask_6: 0.994  loss_dice_6: 1.295  loss_ce_7: 0.3056  loss_cate_7: 0  loss_mask_7: 1.031  loss_dice_7: 1.285  loss_ce_8: 0.3075  loss_cate_8: 0  loss_mask_8: 0.9641  loss_dice_8: 1.26  time: 1.1031  data_time: 0.0131  lr: 9.7839e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:06:19 d2.utils.events]: \u001b[0m eta: 23:51:15  iter: 1939  total_loss: 23.69  loss_ce: 0.2117  loss_cate: 0.2197  loss_mask: 0.8883  loss_dice: 1.046  loss_ce_0: 0.3974  loss_cate_0: 0  loss_mask_0: 0.9398  loss_dice_0: 1.093  loss_ce_1: 0.2811  loss_cate_1: 0  loss_mask_1: 0.8979  loss_dice_1: 1.108  loss_ce_2: 0.239  loss_cate_2: 0  loss_mask_2: 0.8816  loss_dice_2: 1.064  loss_ce_3: 0.2433  loss_cate_3: 0  loss_mask_3: 0.8889  loss_dice_3: 1.045  loss_ce_4: 0.2354  loss_cate_4: 0  loss_mask_4: 0.8775  loss_dice_4: 1.09  loss_ce_5: 0.2217  loss_cate_5: 0  loss_mask_5: 0.8753  loss_dice_5: 1.053  loss_ce_6: 0.2012  loss_cate_6: 0  loss_mask_6: 0.8969  loss_dice_6: 1.092  loss_ce_7: 0.2191  loss_cate_7: 0  loss_mask_7: 0.9026  loss_dice_7: 1.058  loss_ce_8: 0.229  loss_cate_8: 0  loss_mask_8: 0.8814  loss_dice_8: 1.036  time: 1.1031  data_time: 0.0126  lr: 9.7816e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:06:41 d2.utils.events]: \u001b[0m eta: 23:50:56  iter: 1959  total_loss: 24  loss_ce: 0.2344  loss_cate: 0.221  loss_mask: 0.8994  loss_dice: 1.219  loss_ce_0: 0.4411  loss_cate_0: 0  loss_mask_0: 0.9097  loss_dice_0: 1.211  loss_ce_1: 0.2982  loss_cate_1: 0  loss_mask_1: 0.893  loss_dice_1: 1.229  loss_ce_2: 0.2173  loss_cate_2: 0  loss_mask_2: 0.8627  loss_dice_2: 1.181  loss_ce_3: 0.2873  loss_cate_3: 0  loss_mask_3: 0.8568  loss_dice_3: 1.171  loss_ce_4: 0.2304  loss_cate_4: 0  loss_mask_4: 0.8798  loss_dice_4: 1.214  loss_ce_5: 0.2352  loss_cate_5: 0  loss_mask_5: 0.8775  loss_dice_5: 1.239  loss_ce_6: 0.2923  loss_cate_6: 0  loss_mask_6: 0.9101  loss_dice_6: 1.185  loss_ce_7: 0.2648  loss_cate_7: 0  loss_mask_7: 0.8763  loss_dice_7: 1.219  loss_ce_8: 0.2652  loss_cate_8: 0  loss_mask_8: 0.8679  loss_dice_8: 1.255  time: 1.1031  data_time: 0.0128  lr: 9.7793e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:07:03 d2.utils.events]: \u001b[0m eta: 23:50:36  iter: 1979  total_loss: 26.58  loss_ce: 0.3597  loss_cate: 0.2431  loss_mask: 0.9905  loss_dice: 1.312  loss_ce_0: 0.5289  loss_cate_0: 0  loss_mask_0: 1.018  loss_dice_0: 1.288  loss_ce_1: 0.3617  loss_cate_1: 0  loss_mask_1: 0.9682  loss_dice_1: 1.28  loss_ce_2: 0.3842  loss_cate_2: 0  loss_mask_2: 0.939  loss_dice_2: 1.284  loss_ce_3: 0.3592  loss_cate_3: 0  loss_mask_3: 0.8997  loss_dice_3: 1.263  loss_ce_4: 0.3308  loss_cate_4: 0  loss_mask_4: 0.9577  loss_dice_4: 1.258  loss_ce_5: 0.3577  loss_cate_5: 0  loss_mask_5: 0.9128  loss_dice_5: 1.25  loss_ce_6: 0.3555  loss_cate_6: 0  loss_mask_6: 0.9929  loss_dice_6: 1.27  loss_ce_7: 0.322  loss_cate_7: 0  loss_mask_7: 0.979  loss_dice_7: 1.26  loss_ce_8: 0.4009  loss_cate_8: 0  loss_mask_8: 0.9887  loss_dice_8: 1.336  time: 1.1030  data_time: 0.0136  lr: 9.7771e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:08:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 12:08:44 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 12:08:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 12:10:06 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 12:10:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0014 s/iter. Inference: 0.1618 s/iter. Eval: 0.0117 s/iter. Total: 0.1749 s/iter. ETA=0:05:50\n",
      "\u001b[32m[10/12 12:10:13 d2.evaluation.evaluator]: \u001b[0mInference done 37/2016. Dataloading: 0.0026 s/iter. Inference: 0.1633 s/iter. Eval: 0.0284 s/iter. Total: 0.1945 s/iter. ETA=0:06:24\n",
      "\u001b[32m[10/12 12:10:18 d2.evaluation.evaluator]: \u001b[0mInference done 63/2016. Dataloading: 0.0026 s/iter. Inference: 0.1636 s/iter. Eval: 0.0281 s/iter. Total: 0.1945 s/iter. ETA=0:06:19\n",
      "\u001b[32m[10/12 12:10:23 d2.evaluation.evaluator]: \u001b[0mInference done 90/2016. Dataloading: 0.0025 s/iter. Inference: 0.1633 s/iter. Eval: 0.0276 s/iter. Total: 0.1936 s/iter. ETA=0:06:12\n",
      "\u001b[32m[10/12 12:10:28 d2.evaluation.evaluator]: \u001b[0mInference done 117/2016. Dataloading: 0.0025 s/iter. Inference: 0.1631 s/iter. Eval: 0.0260 s/iter. Total: 0.1918 s/iter. ETA=0:06:04\n",
      "\u001b[32m[10/12 12:10:33 d2.evaluation.evaluator]: \u001b[0mInference done 142/2016. Dataloading: 0.0025 s/iter. Inference: 0.1633 s/iter. Eval: 0.0278 s/iter. Total: 0.1937 s/iter. ETA=0:06:03\n",
      "\u001b[32m[10/12 12:10:39 d2.evaluation.evaluator]: \u001b[0mInference done 169/2016. Dataloading: 0.0025 s/iter. Inference: 0.1633 s/iter. Eval: 0.0271 s/iter. Total: 0.1929 s/iter. ETA=0:05:56\n",
      "\u001b[32m[10/12 12:10:44 d2.evaluation.evaluator]: \u001b[0mInference done 195/2016. Dataloading: 0.0025 s/iter. Inference: 0.1632 s/iter. Eval: 0.0277 s/iter. Total: 0.1936 s/iter. ETA=0:05:52\n",
      "\u001b[32m[10/12 12:10:49 d2.evaluation.evaluator]: \u001b[0mInference done 222/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0273 s/iter. Total: 0.1930 s/iter. ETA=0:05:46\n",
      "\u001b[32m[10/12 12:10:54 d2.evaluation.evaluator]: \u001b[0mInference done 247/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0281 s/iter. Total: 0.1938 s/iter. ETA=0:05:42\n",
      "\u001b[32m[10/12 12:10:59 d2.evaluation.evaluator]: \u001b[0mInference done 274/2016. Dataloading: 0.0024 s/iter. Inference: 0.1630 s/iter. Eval: 0.0276 s/iter. Total: 0.1931 s/iter. ETA=0:05:36\n",
      "\u001b[32m[10/12 12:11:04 d2.evaluation.evaluator]: \u001b[0mInference done 299/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0280 s/iter. Total: 0.1938 s/iter. ETA=0:05:32\n",
      "\u001b[32m[10/12 12:11:09 d2.evaluation.evaluator]: \u001b[0mInference done 326/2016. Dataloading: 0.0024 s/iter. Inference: 0.1630 s/iter. Eval: 0.0274 s/iter. Total: 0.1931 s/iter. ETA=0:05:26\n",
      "\u001b[32m[10/12 12:11:14 d2.evaluation.evaluator]: \u001b[0mInference done 352/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0276 s/iter. Total: 0.1934 s/iter. ETA=0:05:21\n",
      "\u001b[32m[10/12 12:11:19 d2.evaluation.evaluator]: \u001b[0mInference done 379/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0273 s/iter. Total: 0.1930 s/iter. ETA=0:05:16\n",
      "\u001b[32m[10/12 12:11:24 d2.evaluation.evaluator]: \u001b[0mInference done 405/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0272 s/iter. Total: 0.1930 s/iter. ETA=0:05:10\n",
      "\u001b[32m[10/12 12:11:29 d2.evaluation.evaluator]: \u001b[0mInference done 432/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0268 s/iter. Total: 0.1926 s/iter. ETA=0:05:05\n",
      "\u001b[32m[10/12 12:11:39 d2.evaluation.evaluator]: \u001b[0mInference done 484/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0271 s/iter. Total: 0.1929 s/iter. ETA=0:04:55\n",
      "\u001b[32m[10/12 12:11:44 d2.evaluation.evaluator]: \u001b[0mInference done 510/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0273 s/iter. Total: 0.1930 s/iter. ETA=0:04:50\n",
      "\u001b[32m[10/12 12:11:49 d2.evaluation.evaluator]: \u001b[0mInference done 537/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0270 s/iter. Total: 0.1927 s/iter. ETA=0:04:45\n",
      "\u001b[32m[10/12 12:11:55 d2.evaluation.evaluator]: \u001b[0mInference done 563/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0272 s/iter. Total: 0.1929 s/iter. ETA=0:04:40\n",
      "\u001b[32m[10/12 12:12:00 d2.evaluation.evaluator]: \u001b[0mInference done 590/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0270 s/iter. Total: 0.1928 s/iter. ETA=0:04:34\n",
      "\u001b[32m[10/12 12:12:05 d2.evaluation.evaluator]: \u001b[0mInference done 616/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0272 s/iter. Total: 0.1930 s/iter. ETA=0:04:30\n",
      "\u001b[32m[10/12 12:12:10 d2.evaluation.evaluator]: \u001b[0mInference done 642/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1937 s/iter. ETA=0:04:26\n",
      "\u001b[32m[10/12 12:12:16 d2.evaluation.evaluator]: \u001b[0mInference done 668/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1939 s/iter. ETA=0:04:21\n",
      "\u001b[32m[10/12 12:12:21 d2.evaluation.evaluator]: \u001b[0mInference done 694/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1940 s/iter. ETA=0:04:16\n",
      "\u001b[32m[10/12 12:12:26 d2.evaluation.evaluator]: \u001b[0mInference done 720/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1940 s/iter. ETA=0:04:11\n",
      "\u001b[32m[10/12 12:12:31 d2.evaluation.evaluator]: \u001b[0mInference done 747/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1939 s/iter. ETA=0:04:06\n",
      "\u001b[32m[10/12 12:12:36 d2.evaluation.evaluator]: \u001b[0mInference done 773/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1939 s/iter. ETA=0:04:01\n",
      "\u001b[32m[10/12 12:12:41 d2.evaluation.evaluator]: \u001b[0mInference done 799/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1939 s/iter. ETA=0:03:55\n",
      "\u001b[32m[10/12 12:12:46 d2.evaluation.evaluator]: \u001b[0mInference done 825/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0282 s/iter. Total: 0.1940 s/iter. ETA=0:03:51\n",
      "\u001b[32m[10/12 12:19:43 d2.utils.events]: \u001b[0m eta: 23:48:28  iter: 2159  total_loss: 22.93  loss_ce: 0.3088  loss_cate: 0.2361  loss_mask: 0.8871  loss_dice: 1.093  loss_ce_0: 0.4505  loss_cate_0: 0  loss_mask_0: 0.9383  loss_dice_0: 1.272  loss_ce_1: 0.392  loss_cate_1: 0  loss_mask_1: 0.9069  loss_dice_1: 1.149  loss_ce_2: 0.3121  loss_cate_2: 0  loss_mask_2: 0.8916  loss_dice_2: 1.058  loss_ce_3: 0.328  loss_cate_3: 0  loss_mask_3: 0.9063  loss_dice_3: 1.079  loss_ce_4: 0.2829  loss_cate_4: 0  loss_mask_4: 0.8882  loss_dice_4: 1.118  loss_ce_5: 0.3036  loss_cate_5: 0  loss_mask_5: 0.8821  loss_dice_5: 1.151  loss_ce_6: 0.2947  loss_cate_6: 0  loss_mask_6: 0.9022  loss_dice_6: 1.054  loss_ce_7: 0.2915  loss_cate_7: 0  loss_mask_7: 0.8918  loss_dice_7: 1.119  loss_ce_8: 0.3089  loss_cate_8: 0  loss_mask_8: 0.8891  loss_dice_8: 1.052  time: 1.1032  data_time: 0.0126  lr: 9.7568e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:20:05 d2.utils.events]: \u001b[0m eta: 23:48:02  iter: 2179  total_loss: 24.31  loss_ce: 0.2595  loss_cate: 0.2303  loss_mask: 0.8698  loss_dice: 1.133  loss_ce_0: 0.3854  loss_cate_0: 0  loss_mask_0: 0.8932  loss_dice_0: 1.188  loss_ce_1: 0.2388  loss_cate_1: 0  loss_mask_1: 0.8434  loss_dice_1: 1.135  loss_ce_2: 0.2583  loss_cate_2: 0  loss_mask_2: 0.8682  loss_dice_2: 1.114  loss_ce_3: 0.3006  loss_cate_3: 0  loss_mask_3: 0.8573  loss_dice_3: 1.068  loss_ce_4: 0.2233  loss_cate_4: 0  loss_mask_4: 0.8467  loss_dice_4: 1.092  loss_ce_5: 0.2519  loss_cate_5: 0  loss_mask_5: 0.8259  loss_dice_5: 1.091  loss_ce_6: 0.2199  loss_cate_6: 0  loss_mask_6: 0.8287  loss_dice_6: 1.1  loss_ce_7: 0.1789  loss_cate_7: 0  loss_mask_7: 0.8514  loss_dice_7: 1.124  loss_ce_8: 0.2358  loss_cate_8: 0  loss_mask_8: 0.8576  loss_dice_8: 1.115  time: 1.1032  data_time: 0.0138  lr: 9.7545e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:20:28 d2.utils.events]: \u001b[0m eta: 23:47:49  iter: 2199  total_loss: 26.76  loss_ce: 0.2773  loss_cate: 0.2563  loss_mask: 1.13  loss_dice: 1.182  loss_ce_0: 0.4425  loss_cate_0: 0  loss_mask_0: 1.141  loss_dice_0: 1.353  loss_ce_1: 0.2671  loss_cate_1: 0  loss_mask_1: 1.173  loss_dice_1: 1.339  loss_ce_2: 0.2497  loss_cate_2: 0  loss_mask_2: 1.075  loss_dice_2: 1.18  loss_ce_3: 0.2427  loss_cate_3: 0  loss_mask_3: 1.1  loss_dice_3: 1.241  loss_ce_4: 0.2781  loss_cate_4: 0  loss_mask_4: 1.077  loss_dice_4: 1.218  loss_ce_5: 0.2736  loss_cate_5: 0  loss_mask_5: 1.127  loss_dice_5: 1.238  loss_ce_6: 0.2835  loss_cate_6: 0  loss_mask_6: 1.082  loss_dice_6: 1.195  loss_ce_7: 0.2554  loss_cate_7: 0  loss_mask_7: 1.133  loss_dice_7: 1.209  loss_ce_8: 0.3207  loss_cate_8: 0  loss_mask_8: 1.154  loss_dice_8: 1.205  time: 1.1032  data_time: 0.0147  lr: 9.7523e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:20:50 d2.utils.events]: \u001b[0m eta: 23:47:27  iter: 2219  total_loss: 24.39  loss_ce: 0.2042  loss_cate: 0.21  loss_mask: 0.9253  loss_dice: 1.155  loss_ce_0: 0.3596  loss_cate_0: 0  loss_mask_0: 0.9558  loss_dice_0: 1.284  loss_ce_1: 0.3262  loss_cate_1: 0  loss_mask_1: 0.9265  loss_dice_1: 1.247  loss_ce_2: 0.2751  loss_cate_2: 0  loss_mask_2: 0.9194  loss_dice_2: 1.216  loss_ce_3: 0.2494  loss_cate_3: 0  loss_mask_3: 0.9318  loss_dice_3: 1.118  loss_ce_4: 0.2158  loss_cate_4: 0  loss_mask_4: 0.941  loss_dice_4: 1.186  loss_ce_5: 0.2676  loss_cate_5: 0  loss_mask_5: 0.915  loss_dice_5: 1.204  loss_ce_6: 0.2568  loss_cate_6: 0  loss_mask_6: 0.9326  loss_dice_6: 1.163  loss_ce_7: 0.2734  loss_cate_7: 0  loss_mask_7: 0.9332  loss_dice_7: 1.148  loss_ce_8: 0.2649  loss_cate_8: 0  loss_mask_8: 0.9381  loss_dice_8: 1.137  time: 1.1032  data_time: 0.0123  lr: 9.75e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:21:14 d2.utils.events]: \u001b[0m eta: 23:47:36  iter: 2239  total_loss: 22.89  loss_ce: 0.2529  loss_cate: 0.2443  loss_mask: 0.7692  loss_dice: 1.156  loss_ce_0: 0.3901  loss_cate_0: 0  loss_mask_0: 0.8083  loss_dice_0: 1.133  loss_ce_1: 0.3078  loss_cate_1: 0  loss_mask_1: 0.7711  loss_dice_1: 1.111  loss_ce_2: 0.2877  loss_cate_2: 0  loss_mask_2: 0.8382  loss_dice_2: 1.125  loss_ce_3: 0.2718  loss_cate_3: 0  loss_mask_3: 0.8349  loss_dice_3: 1.098  loss_ce_4: 0.2502  loss_cate_4: 0  loss_mask_4: 0.861  loss_dice_4: 1.084  loss_ce_5: 0.269  loss_cate_5: 0  loss_mask_5: 0.8331  loss_dice_5: 1.12  loss_ce_6: 0.2577  loss_cate_6: 0  loss_mask_6: 0.7628  loss_dice_6: 1.098  loss_ce_7: 0.2265  loss_cate_7: 0  loss_mask_7: 0.8449  loss_dice_7: 1.116  loss_ce_8: 0.2506  loss_cate_8: 0  loss_mask_8: 0.7916  loss_dice_8: 1.164  time: 1.1032  data_time: 0.0132  lr: 9.7478e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:21:36 d2.utils.events]: \u001b[0m eta: 23:47:17  iter: 2259  total_loss: 22.95  loss_ce: 0.2059  loss_cate: 0.2199  loss_mask: 0.8935  loss_dice: 1.059  loss_ce_0: 0.3725  loss_cate_0: 0  loss_mask_0: 0.9917  loss_dice_0: 1.141  loss_ce_1: 0.2263  loss_cate_1: 0  loss_mask_1: 1.002  loss_dice_1: 1.078  loss_ce_2: 0.2618  loss_cate_2: 0  loss_mask_2: 0.8773  loss_dice_2: 1.056  loss_ce_3: 0.2388  loss_cate_3: 0  loss_mask_3: 0.9115  loss_dice_3: 1.046  loss_ce_4: 0.2251  loss_cate_4: 0  loss_mask_4: 0.9178  loss_dice_4: 1.039  loss_ce_5: 0.2366  loss_cate_5: 0  loss_mask_5: 0.9004  loss_dice_5: 1.034  loss_ce_6: 0.1932  loss_cate_6: 0  loss_mask_6: 0.8968  loss_dice_6: 1.06  loss_ce_7: 0.179  loss_cate_7: 0  loss_mask_7: 0.8938  loss_dice_7: 1.042  loss_ce_8: 0.1723  loss_cate_8: 0  loss_mask_8: 0.89  loss_dice_8: 1.039  time: 1.1032  data_time: 0.0129  lr: 9.7455e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:21:59 d2.utils.events]: \u001b[0m eta: 23:47:00  iter: 2279  total_loss: 28.65  loss_ce: 0.369  loss_cate: 0.2394  loss_mask: 1.072  loss_dice: 1.183  loss_ce_0: 0.5958  loss_cate_0: 0  loss_mask_0: 1.133  loss_dice_0: 1.221  loss_ce_1: 0.4509  loss_cate_1: 0  loss_mask_1: 1.173  loss_dice_1: 1.244  loss_ce_2: 0.388  loss_cate_2: 0  loss_mask_2: 1.173  loss_dice_2: 1.24  loss_ce_3: 0.3672  loss_cate_3: 0  loss_mask_3: 1.021  loss_dice_3: 1.199  loss_ce_4: 0.3988  loss_cate_4: 0  loss_mask_4: 1.132  loss_dice_4: 1.204  loss_ce_5: 0.3589  loss_cate_5: 0  loss_mask_5: 1.135  loss_dice_5: 1.219  loss_ce_6: 0.4368  loss_cate_6: 0  loss_mask_6: 1.088  loss_dice_6: 1.205  loss_ce_7: 0.4149  loss_cate_7: 0  loss_mask_7: 1.088  loss_dice_7: 1.18  loss_ce_8: 0.3938  loss_cate_8: 0  loss_mask_8: 1.071  loss_dice_8: 1.182  time: 1.1032  data_time: 0.0124  lr: 9.7432e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:22:21 d2.utils.events]: \u001b[0m eta: 23:46:52  iter: 2299  total_loss: 23.62  loss_ce: 0.3295  loss_cate: 0.2165  loss_mask: 0.8446  loss_dice: 1.062  loss_ce_0: 0.5182  loss_cate_0: 0  loss_mask_0: 0.8996  loss_dice_0: 1.221  loss_ce_1: 0.3343  loss_cate_1: 0  loss_mask_1: 0.8534  loss_dice_1: 1.16  loss_ce_2: 0.3036  loss_cate_2: 0  loss_mask_2: 0.8572  loss_dice_2: 1.104  loss_ce_3: 0.2902  loss_cate_3: 0  loss_mask_3: 0.8164  loss_dice_3: 1.11  loss_ce_4: 0.3194  loss_cate_4: 0  loss_mask_4: 0.8265  loss_dice_4: 1.114  loss_ce_5: 0.2849  loss_cate_5: 0  loss_mask_5: 0.8395  loss_dice_5: 1.171  loss_ce_6: 0.3232  loss_cate_6: 0  loss_mask_6: 0.845  loss_dice_6: 1.063  loss_ce_7: 0.2881  loss_cate_7: 0  loss_mask_7: 0.8121  loss_dice_7: 1.065  loss_ce_8: 0.2939  loss_cate_8: 0  loss_mask_8: 0.8609  loss_dice_8: 1.102  time: 1.1032  data_time: 0.0136  lr: 9.741e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:22:44 d2.utils.events]: \u001b[0m eta: 23:46:40  iter: 2319  total_loss: 24.12  loss_ce: 0.3103  loss_cate: 0.2093  loss_mask: 0.8759  loss_dice: 1.059  loss_ce_0: 0.4781  loss_cate_0: 0  loss_mask_0: 0.9352  loss_dice_0: 1.169  loss_ce_1: 0.3849  loss_cate_1: 0  loss_mask_1: 0.8691  loss_dice_1: 1.091  loss_ce_2: 0.3937  loss_cate_2: 0  loss_mask_2: 0.8576  loss_dice_2: 1.123  loss_ce_3: 0.3666  loss_cate_3: 0  loss_mask_3: 0.8788  loss_dice_3: 1.036  loss_ce_4: 0.2791  loss_cate_4: 0  loss_mask_4: 0.9037  loss_dice_4: 1.074  loss_ce_5: 0.327  loss_cate_5: 0  loss_mask_5: 0.9019  loss_dice_5: 1.041  loss_ce_6: 0.2934  loss_cate_6: 0  loss_mask_6: 0.9036  loss_dice_6: 1.027  loss_ce_7: 0.2881  loss_cate_7: 0  loss_mask_7: 0.9277  loss_dice_7: 1.075  loss_ce_8: 0.2749  loss_cate_8: 0  loss_mask_8: 0.8712  loss_dice_8: 1.053  time: 1.1032  data_time: 0.0135  lr: 9.7387e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:23:06 d2.utils.events]: \u001b[0m eta: 23:46:23  iter: 2339  total_loss: 24.85  loss_ce: 0.2996  loss_cate: 0.2082  loss_mask: 0.952  loss_dice: 1.133  loss_ce_0: 0.4059  loss_cate_0: 0  loss_mask_0: 0.9533  loss_dice_0: 1.213  loss_ce_1: 0.3037  loss_cate_1: 0  loss_mask_1: 0.9307  loss_dice_1: 1.138  loss_ce_2: 0.3024  loss_cate_2: 0  loss_mask_2: 0.9251  loss_dice_2: 1.138  loss_ce_3: 0.2889  loss_cate_3: 0  loss_mask_3: 0.9465  loss_dice_3: 1.114  loss_ce_4: 0.254  loss_cate_4: 0  loss_mask_4: 0.9438  loss_dice_4: 1.15  loss_ce_5: 0.2771  loss_cate_5: 0  loss_mask_5: 0.8969  loss_dice_5: 1.132  loss_ce_6: 0.2989  loss_cate_6: 0  loss_mask_6: 0.9066  loss_dice_6: 1.136  loss_ce_7: 0.3012  loss_cate_7: 0  loss_mask_7: 0.8958  loss_dice_7: 1.132  loss_ce_8: 0.2899  loss_cate_8: 0  loss_mask_8: 0.9252  loss_dice_8: 1.132  time: 1.1032  data_time: 0.0127  lr: 9.7365e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:23:28 d2.utils.events]: \u001b[0m eta: 23:46:03  iter: 2359  total_loss: 24.15  loss_ce: 0.2782  loss_cate: 0.212  loss_mask: 0.9184  loss_dice: 1.067  loss_ce_0: 0.4671  loss_cate_0: 0  loss_mask_0: 0.9707  loss_dice_0: 1.145  loss_ce_1: 0.3849  loss_cate_1: 0  loss_mask_1: 0.9256  loss_dice_1: 1.079  loss_ce_2: 0.3392  loss_cate_2: 0  loss_mask_2: 0.9294  loss_dice_2: 1.177  loss_ce_3: 0.3099  loss_cate_3: 0  loss_mask_3: 0.9231  loss_dice_3: 1.109  loss_ce_4: 0.2598  loss_cate_4: 0  loss_mask_4: 0.9721  loss_dice_4: 1.163  loss_ce_5: 0.2661  loss_cate_5: 0  loss_mask_5: 0.9327  loss_dice_5: 1.121  loss_ce_6: 0.3292  loss_cate_6: 0  loss_mask_6: 0.9113  loss_dice_6: 1.058  loss_ce_7: 0.3083  loss_cate_7: 0  loss_mask_7: 0.93  loss_dice_7: 1.054  loss_ce_8: 0.3203  loss_cate_8: 0  loss_mask_8: 0.9012  loss_dice_8: 1.054  time: 1.1033  data_time: 0.0121  lr: 9.7342e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:23:51 d2.utils.events]: \u001b[0m eta: 23:45:41  iter: 2379  total_loss: 26.41  loss_ce: 0.3139  loss_cate: 0.2005  loss_mask: 1.061  loss_dice: 1.071  loss_ce_0: 0.4768  loss_cate_0: 0  loss_mask_0: 1.114  loss_dice_0: 1.161  loss_ce_1: 0.4091  loss_cate_1: 0  loss_mask_1: 1.086  loss_dice_1: 1.173  loss_ce_2: 0.4398  loss_cate_2: 0  loss_mask_2: 1.016  loss_dice_2: 1.087  loss_ce_3: 0.3269  loss_cate_3: 0  loss_mask_3: 1.062  loss_dice_3: 1.103  loss_ce_4: 0.3533  loss_cate_4: 0  loss_mask_4: 1.066  loss_dice_4: 1.13  loss_ce_5: 0.3405  loss_cate_5: 0  loss_mask_5: 1.04  loss_dice_5: 1.118  loss_ce_6: 0.3443  loss_cate_6: 0  loss_mask_6: 1.092  loss_dice_6: 1.135  loss_ce_7: 0.322  loss_cate_7: 0  loss_mask_7: 1.061  loss_dice_7: 1.078  loss_ce_8: 0.3208  loss_cate_8: 0  loss_mask_8: 1.084  loss_dice_8: 1.117  time: 1.1032  data_time: 0.0133  lr: 9.732e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:24:14 d2.utils.events]: \u001b[0m eta: 23:45:28  iter: 2399  total_loss: 24.76  loss_ce: 0.3247  loss_cate: 0.2131  loss_mask: 0.8901  loss_dice: 1.159  loss_ce_0: 0.395  loss_cate_0: 0  loss_mask_0: 0.9813  loss_dice_0: 1.203  loss_ce_1: 0.3152  loss_cate_1: 0  loss_mask_1: 0.9206  loss_dice_1: 1.178  loss_ce_2: 0.3215  loss_cate_2: 0  loss_mask_2: 0.9205  loss_dice_2: 1.203  loss_ce_3: 0.227  loss_cate_3: 0  loss_mask_3: 0.9173  loss_dice_3: 1.183  loss_ce_4: 0.2614  loss_cate_4: 0  loss_mask_4: 0.877  loss_dice_4: 1.158  loss_ce_5: 0.2203  loss_cate_5: 0  loss_mask_5: 0.9111  loss_dice_5: 1.148  loss_ce_6: 0.2399  loss_cate_6: 0  loss_mask_6: 0.8902  loss_dice_6: 1.17  loss_ce_7: 0.2396  loss_cate_7: 0  loss_mask_7: 0.908  loss_dice_7: 1.18  loss_ce_8: 0.2325  loss_cate_8: 0  loss_mask_8: 0.9209  loss_dice_8: 1.212  time: 1.1033  data_time: 0.0139  lr: 9.7297e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:24:36 d2.utils.events]: \u001b[0m eta: 23:45:21  iter: 2419  total_loss: 26.26  loss_ce: 0.3082  loss_cate: 0.2213  loss_mask: 1.038  loss_dice: 1.15  loss_ce_0: 0.5624  loss_cate_0: 0  loss_mask_0: 1.028  loss_dice_0: 1.211  loss_ce_1: 0.4225  loss_cate_1: 0  loss_mask_1: 1.016  loss_dice_1: 1.124  loss_ce_2: 0.4048  loss_cate_2: 0  loss_mask_2: 1.035  loss_dice_2: 1.166  loss_ce_3: 0.3807  loss_cate_3: 0  loss_mask_3: 1.043  loss_dice_3: 1.149  loss_ce_4: 0.277  loss_cate_4: 0  loss_mask_4: 1.079  loss_dice_4: 1.147  loss_ce_5: 0.3432  loss_cate_5: 0  loss_mask_5: 0.9815  loss_dice_5: 1.142  loss_ce_6: 0.3553  loss_cate_6: 0  loss_mask_6: 0.9956  loss_dice_6: 1.134  loss_ce_7: 0.3317  loss_cate_7: 0  loss_mask_7: 1.007  loss_dice_7: 1.098  loss_ce_8: 0.3989  loss_cate_8: 0  loss_mask_8: 1.056  loss_dice_8: 1.119  time: 1.1033  data_time: 0.0141  lr: 9.7274e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:24:59 d2.utils.events]: \u001b[0m eta: 23:44:59  iter: 2439  total_loss: 25.47  loss_ce: 0.3563  loss_cate: 0.2877  loss_mask: 0.9177  loss_dice: 1.213  loss_ce_0: 0.5174  loss_cate_0: 0  loss_mask_0: 0.9004  loss_dice_0: 1.241  loss_ce_1: 0.3672  loss_cate_1: 0  loss_mask_1: 0.9336  loss_dice_1: 1.181  loss_ce_2: 0.3945  loss_cate_2: 0  loss_mask_2: 0.9291  loss_dice_2: 1.192  loss_ce_3: 0.3927  loss_cate_3: 0  loss_mask_3: 0.9333  loss_dice_3: 1.176  loss_ce_4: 0.3804  loss_cate_4: 0  loss_mask_4: 0.9398  loss_dice_4: 1.229  loss_ce_5: 0.4011  loss_cate_5: 0  loss_mask_5: 0.913  loss_dice_5: 1.217  loss_ce_6: 0.372  loss_cate_6: 0  loss_mask_6: 0.9073  loss_dice_6: 1.128  loss_ce_7: 0.3477  loss_cate_7: 0  loss_mask_7: 0.9211  loss_dice_7: 1.205  loss_ce_8: 0.3317  loss_cate_8: 0  loss_mask_8: 0.9317  loss_dice_8: 1.216  time: 1.1033  data_time: 0.0142  lr: 9.7252e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:25:21 d2.utils.events]: \u001b[0m eta: 23:44:42  iter: 2459  total_loss: 26.85  loss_ce: 0.3639  loss_cate: 0.1908  loss_mask: 0.9375  loss_dice: 1.222  loss_ce_0: 0.504  loss_cate_0: 0  loss_mask_0: 0.9276  loss_dice_0: 1.311  loss_ce_1: 0.3087  loss_cate_1: 0  loss_mask_1: 0.9498  loss_dice_1: 1.278  loss_ce_2: 0.3461  loss_cate_2: 0  loss_mask_2: 0.9278  loss_dice_2: 1.245  loss_ce_3: 0.3476  loss_cate_3: 0  loss_mask_3: 0.9693  loss_dice_3: 1.23  loss_ce_4: 0.3858  loss_cate_4: 0  loss_mask_4: 0.9425  loss_dice_4: 1.22  loss_ce_5: 0.3455  loss_cate_5: 0  loss_mask_5: 0.9051  loss_dice_5: 1.227  loss_ce_6: 0.3707  loss_cate_6: 0  loss_mask_6: 0.9319  loss_dice_6: 1.217  loss_ce_7: 0.3353  loss_cate_7: 0  loss_mask_7: 0.9336  loss_dice_7: 1.196  loss_ce_8: 0.3252  loss_cate_8: 0  loss_mask_8: 0.9753  loss_dice_8: 1.221  time: 1.1033  data_time: 0.0138  lr: 9.7229e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:25:43 d2.utils.events]: \u001b[0m eta: 23:44:26  iter: 2479  total_loss: 25.69  loss_ce: 0.2663  loss_cate: 0.2061  loss_mask: 0.8893  loss_dice: 1.132  loss_ce_0: 0.4533  loss_cate_0: 0  loss_mask_0: 0.9772  loss_dice_0: 1.237  loss_ce_1: 0.2833  loss_cate_1: 0  loss_mask_1: 0.9926  loss_dice_1: 1.247  loss_ce_2: 0.3147  loss_cate_2: 0  loss_mask_2: 0.9737  loss_dice_2: 1.164  loss_ce_3: 0.3353  loss_cate_3: 0  loss_mask_3: 0.9734  loss_dice_3: 1.15  loss_ce_4: 0.3084  loss_cate_4: 0  loss_mask_4: 0.9987  loss_dice_4: 1.156  loss_ce_5: 0.2708  loss_cate_5: 0  loss_mask_5: 1.01  loss_dice_5: 1.19  loss_ce_6: 0.3  loss_cate_6: 0  loss_mask_6: 0.9943  loss_dice_6: 1.167  loss_ce_7: 0.2845  loss_cate_7: 0  loss_mask_7: 0.8869  loss_dice_7: 1.177  loss_ce_8: 0.258  loss_cate_8: 0  loss_mask_8: 0.9072  loss_dice_8: 1.174  time: 1.1033  data_time: 0.0125  lr: 9.7207e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:26:06 d2.utils.events]: \u001b[0m eta: 23:44:21  iter: 2499  total_loss: 27.12  loss_ce: 0.35  loss_cate: 0.2473  loss_mask: 1.03  loss_dice: 1.179  loss_ce_0: 0.4018  loss_cate_0: 0  loss_mask_0: 1.03  loss_dice_0: 1.369  loss_ce_1: 0.3054  loss_cate_1: 0  loss_mask_1: 1.05  loss_dice_1: 1.438  loss_ce_2: 0.3538  loss_cate_2: 0  loss_mask_2: 1.069  loss_dice_2: 1.28  loss_ce_3: 0.3878  loss_cate_3: 0  loss_mask_3: 1.088  loss_dice_3: 1.263  loss_ce_4: 0.2822  loss_cate_4: 0  loss_mask_4: 1.078  loss_dice_4: 1.268  loss_ce_5: 0.3482  loss_cate_5: 0  loss_mask_5: 1.081  loss_dice_5: 1.229  loss_ce_6: 0.3428  loss_cate_6: 0  loss_mask_6: 1.063  loss_dice_6: 1.268  loss_ce_7: 0.3142  loss_cate_7: 0  loss_mask_7: 1.031  loss_dice_7: 1.221  loss_ce_8: 0.3156  loss_cate_8: 0  loss_mask_8: 1.042  loss_dice_8: 1.197  time: 1.1033  data_time: 0.0126  lr: 9.7184e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:26:28 d2.utils.events]: \u001b[0m eta: 23:43:53  iter: 2519  total_loss: 21.41  loss_ce: 0.3175  loss_cate: 0.217  loss_mask: 0.7699  loss_dice: 0.9681  loss_ce_0: 0.5142  loss_cate_0: 0  loss_mask_0: 0.8302  loss_dice_0: 1.099  loss_ce_1: 0.3677  loss_cate_1: 0  loss_mask_1: 0.7937  loss_dice_1: 1.028  loss_ce_2: 0.3612  loss_cate_2: 0  loss_mask_2: 0.789  loss_dice_2: 0.9855  loss_ce_3: 0.3408  loss_cate_3: 0  loss_mask_3: 0.769  loss_dice_3: 0.9941  loss_ce_4: 0.3131  loss_cate_4: 0  loss_mask_4: 0.7935  loss_dice_4: 0.9863  loss_ce_5: 0.3192  loss_cate_5: 0  loss_mask_5: 0.7972  loss_dice_5: 1.003  loss_ce_6: 0.2924  loss_cate_6: 0  loss_mask_6: 0.7831  loss_dice_6: 0.9859  loss_ce_7: 0.2871  loss_cate_7: 0  loss_mask_7: 0.7766  loss_dice_7: 0.9698  loss_ce_8: 0.3002  loss_cate_8: 0  loss_mask_8: 0.7669  loss_dice_8: 0.9729  time: 1.1033  data_time: 0.0135  lr: 9.7162e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:26:51 d2.utils.events]: \u001b[0m eta: 23:43:42  iter: 2539  total_loss: 24.54  loss_ce: 0.2797  loss_cate: 0.2101  loss_mask: 0.9245  loss_dice: 1.261  loss_ce_0: 0.4191  loss_cate_0: 0  loss_mask_0: 0.935  loss_dice_0: 1.29  loss_ce_1: 0.3449  loss_cate_1: 0  loss_mask_1: 0.9103  loss_dice_1: 1.24  loss_ce_2: 0.3478  loss_cate_2: 0  loss_mask_2: 0.9028  loss_dice_2: 1.21  loss_ce_3: 0.3966  loss_cate_3: 0  loss_mask_3: 0.912  loss_dice_3: 1.235  loss_ce_4: 0.356  loss_cate_4: 0  loss_mask_4: 0.8852  loss_dice_4: 1.215  loss_ce_5: 0.3626  loss_cate_5: 0  loss_mask_5: 0.9014  loss_dice_5: 1.227  loss_ce_6: 0.3292  loss_cate_6: 0  loss_mask_6: 0.9027  loss_dice_6: 1.217  loss_ce_7: 0.3209  loss_cate_7: 0  loss_mask_7: 0.9029  loss_dice_7: 1.23  loss_ce_8: 0.3124  loss_cate_8: 0  loss_mask_8: 0.9161  loss_dice_8: 1.222  time: 1.1033  data_time: 0.0138  lr: 9.7139e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:27:14 d2.utils.events]: \u001b[0m eta: 23:43:08  iter: 2559  total_loss: 25.5  loss_ce: 0.2422  loss_cate: 0.2434  loss_mask: 1.003  loss_dice: 1.124  loss_ce_0: 0.492  loss_cate_0: 0  loss_mask_0: 1.067  loss_dice_0: 1.247  loss_ce_1: 0.4252  loss_cate_1: 0  loss_mask_1: 1.09  loss_dice_1: 1.16  loss_ce_2: 0.3389  loss_cate_2: 0  loss_mask_2: 0.9495  loss_dice_2: 1.123  loss_ce_3: 0.2877  loss_cate_3: 0  loss_mask_3: 0.9434  loss_dice_3: 1.107  loss_ce_4: 0.3153  loss_cate_4: 0  loss_mask_4: 0.9443  loss_dice_4: 1.142  loss_ce_5: 0.3457  loss_cate_5: 0  loss_mask_5: 0.9777  loss_dice_5: 1.126  loss_ce_6: 0.3247  loss_cate_6: 0  loss_mask_6: 0.9958  loss_dice_6: 1.082  loss_ce_7: 0.2505  loss_cate_7: 0  loss_mask_7: 1.045  loss_dice_7: 1.104  loss_ce_8: 0.2657  loss_cate_8: 0  loss_mask_8: 1.013  loss_dice_8: 1.128  time: 1.1032  data_time: 0.0146  lr: 9.7116e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:27:36 d2.utils.events]: \u001b[0m eta: 23:42:44  iter: 2579  total_loss: 26.67  loss_ce: 0.2974  loss_cate: 0.1952  loss_mask: 1.014  loss_dice: 1.148  loss_ce_0: 0.3945  loss_cate_0: 0  loss_mask_0: 1.022  loss_dice_0: 1.222  loss_ce_1: 0.2573  loss_cate_1: 0  loss_mask_1: 1  loss_dice_1: 1.195  loss_ce_2: 0.2556  loss_cate_2: 0  loss_mask_2: 0.9796  loss_dice_2: 1.199  loss_ce_3: 0.3047  loss_cate_3: 0  loss_mask_3: 0.9694  loss_dice_3: 1.1  loss_ce_4: 0.2014  loss_cate_4: 0  loss_mask_4: 0.9519  loss_dice_4: 1.118  loss_ce_5: 0.215  loss_cate_5: 0  loss_mask_5: 1.002  loss_dice_5: 1.133  loss_ce_6: 0.2578  loss_cate_6: 0  loss_mask_6: 0.947  loss_dice_6: 1.155  loss_ce_7: 0.2468  loss_cate_7: 0  loss_mask_7: 0.9735  loss_dice_7: 1.132  loss_ce_8: 0.2551  loss_cate_8: 0  loss_mask_8: 0.9885  loss_dice_8: 1.183  time: 1.1032  data_time: 0.0127  lr: 9.7094e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:27:59 d2.utils.events]: \u001b[0m eta: 23:42:20  iter: 2599  total_loss: 26.41  loss_ce: 0.3843  loss_cate: 0.2096  loss_mask: 0.9704  loss_dice: 1.216  loss_ce_0: 0.5004  loss_cate_0: 0  loss_mask_0: 0.9833  loss_dice_0: 1.321  loss_ce_1: 0.3479  loss_cate_1: 0  loss_mask_1: 0.947  loss_dice_1: 1.24  loss_ce_2: 0.3573  loss_cate_2: 0  loss_mask_2: 0.9429  loss_dice_2: 1.262  loss_ce_3: 0.3459  loss_cate_3: 0  loss_mask_3: 0.9506  loss_dice_3: 1.234  loss_ce_4: 0.3045  loss_cate_4: 0  loss_mask_4: 0.9543  loss_dice_4: 1.205  loss_ce_5: 0.3946  loss_cate_5: 0  loss_mask_5: 0.9644  loss_dice_5: 1.237  loss_ce_6: 0.3663  loss_cate_6: 0  loss_mask_6: 0.9318  loss_dice_6: 1.238  loss_ce_7: 0.3877  loss_cate_7: 0  loss_mask_7: 0.916  loss_dice_7: 1.313  loss_ce_8: 0.3745  loss_cate_8: 0  loss_mask_8: 0.9144  loss_dice_8: 1.234  time: 1.1032  data_time: 0.0129  lr: 9.7071e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:28:21 d2.utils.events]: \u001b[0m eta: 23:41:46  iter: 2619  total_loss: 23.2  loss_ce: 0.2332  loss_cate: 0.2136  loss_mask: 0.9073  loss_dice: 1.086  loss_ce_0: 0.4082  loss_cate_0: 0  loss_mask_0: 0.8767  loss_dice_0: 1.218  loss_ce_1: 0.3489  loss_cate_1: 0  loss_mask_1: 0.8427  loss_dice_1: 1.188  loss_ce_2: 0.2466  loss_cate_2: 0  loss_mask_2: 0.9026  loss_dice_2: 1.094  loss_ce_3: 0.2303  loss_cate_3: 0  loss_mask_3: 0.8705  loss_dice_3: 1.033  loss_ce_4: 0.276  loss_cate_4: 0  loss_mask_4: 0.8613  loss_dice_4: 1.039  loss_ce_5: 0.252  loss_cate_5: 0  loss_mask_5: 0.862  loss_dice_5: 1.041  loss_ce_6: 0.2179  loss_cate_6: 0  loss_mask_6: 0.909  loss_dice_6: 1.057  loss_ce_7: 0.2292  loss_cate_7: 0  loss_mask_7: 0.8831  loss_dice_7: 1.032  loss_ce_8: 0.216  loss_cate_8: 0  loss_mask_8: 0.898  loss_dice_8: 1.065  time: 1.1031  data_time: 0.0127  lr: 9.7049e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:28:43 d2.utils.events]: \u001b[0m eta: 23:40:57  iter: 2639  total_loss: 24.12  loss_ce: 0.3479  loss_cate: 0.2103  loss_mask: 0.9838  loss_dice: 1.025  loss_ce_0: 0.4044  loss_cate_0: 0  loss_mask_0: 1.028  loss_dice_0: 1.117  loss_ce_1: 0.33  loss_cate_1: 0  loss_mask_1: 0.9282  loss_dice_1: 1.153  loss_ce_2: 0.3128  loss_cate_2: 0  loss_mask_2: 0.9608  loss_dice_2: 1.086  loss_ce_3: 0.2819  loss_cate_3: 0  loss_mask_3: 1.011  loss_dice_3: 1.035  loss_ce_4: 0.3096  loss_cate_4: 0  loss_mask_4: 0.9861  loss_dice_4: 1.059  loss_ce_5: 0.3035  loss_cate_5: 0  loss_mask_5: 0.987  loss_dice_5: 1.045  loss_ce_6: 0.3465  loss_cate_6: 0  loss_mask_6: 0.9908  loss_dice_6: 1.074  loss_ce_7: 0.3398  loss_cate_7: 0  loss_mask_7: 0.9585  loss_dice_7: 1.041  loss_ce_8: 0.3283  loss_cate_8: 0  loss_mask_8: 1.01  loss_dice_8: 1.043  time: 1.1031  data_time: 0.0122  lr: 9.7026e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:29:06 d2.utils.events]: \u001b[0m eta: 23:40:28  iter: 2659  total_loss: 23.54  loss_ce: 0.3212  loss_cate: 0.2107  loss_mask: 0.7969  loss_dice: 1.159  loss_ce_0: 0.4914  loss_cate_0: 0  loss_mask_0: 0.8515  loss_dice_0: 1.152  loss_ce_1: 0.3597  loss_cate_1: 0  loss_mask_1: 0.8258  loss_dice_1: 1.137  loss_ce_2: 0.2695  loss_cate_2: 0  loss_mask_2: 0.8373  loss_dice_2: 1.133  loss_ce_3: 0.2985  loss_cate_3: 0  loss_mask_3: 0.795  loss_dice_3: 1.082  loss_ce_4: 0.3347  loss_cate_4: 0  loss_mask_4: 0.8114  loss_dice_4: 1.144  loss_ce_5: 0.2865  loss_cate_5: 0  loss_mask_5: 0.7851  loss_dice_5: 1.107  loss_ce_6: 0.3265  loss_cate_6: 0  loss_mask_6: 0.7958  loss_dice_6: 1.145  loss_ce_7: 0.3021  loss_cate_7: 0  loss_mask_7: 0.7978  loss_dice_7: 1.166  loss_ce_8: 0.3309  loss_cate_8: 0  loss_mask_8: 0.7941  loss_dice_8: 1.099  time: 1.1030  data_time: 0.0150  lr: 9.7004e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:29:28 d2.utils.events]: \u001b[0m eta: 23:39:29  iter: 2679  total_loss: 24.8  loss_ce: 0.2869  loss_cate: 0.2141  loss_mask: 0.9533  loss_dice: 1.269  loss_ce_0: 0.3932  loss_cate_0: 0  loss_mask_0: 0.9233  loss_dice_0: 1.235  loss_ce_1: 0.3598  loss_cate_1: 0  loss_mask_1: 0.8965  loss_dice_1: 1.251  loss_ce_2: 0.3119  loss_cate_2: 0  loss_mask_2: 0.9071  loss_dice_2: 1.268  loss_ce_3: 0.3015  loss_cate_3: 0  loss_mask_3: 0.9148  loss_dice_3: 1.281  loss_ce_4: 0.2839  loss_cate_4: 0  loss_mask_4: 0.9043  loss_dice_4: 1.24  loss_ce_5: 0.2825  loss_cate_5: 0  loss_mask_5: 0.9061  loss_dice_5: 1.204  loss_ce_6: 0.2907  loss_cate_6: 0  loss_mask_6: 0.9359  loss_dice_6: 1.26  loss_ce_7: 0.3076  loss_cate_7: 0  loss_mask_7: 0.923  loss_dice_7: 1.273  loss_ce_8: 0.2859  loss_cate_8: 0  loss_mask_8: 0.9425  loss_dice_8: 1.243  time: 1.1030  data_time: 0.0131  lr: 9.6981e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:29:50 d2.utils.events]: \u001b[0m eta: 23:38:54  iter: 2699  total_loss: 22.64  loss_ce: 0.3676  loss_cate: 0.2197  loss_mask: 0.7283  loss_dice: 1.063  loss_ce_0: 0.4238  loss_cate_0: 0  loss_mask_0: 0.7799  loss_dice_0: 1.194  loss_ce_1: 0.34  loss_cate_1: 0  loss_mask_1: 0.7185  loss_dice_1: 1.084  loss_ce_2: 0.2974  loss_cate_2: 0  loss_mask_2: 0.7669  loss_dice_2: 1.102  loss_ce_3: 0.325  loss_cate_3: 0  loss_mask_3: 0.7889  loss_dice_3: 1.084  loss_ce_4: 0.2824  loss_cate_4: 0  loss_mask_4: 0.7717  loss_dice_4: 1.049  loss_ce_5: 0.3793  loss_cate_5: 0  loss_mask_5: 0.7513  loss_dice_5: 1.071  loss_ce_6: 0.357  loss_cate_6: 0  loss_mask_6: 0.7539  loss_dice_6: 1.053  loss_ce_7: 0.4087  loss_cate_7: 0  loss_mask_7: 0.7393  loss_dice_7: 1.032  loss_ce_8: 0.4199  loss_cate_8: 0  loss_mask_8: 0.7446  loss_dice_8: 1.034  time: 1.1030  data_time: 0.0136  lr: 9.6958e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:30:13 d2.utils.events]: \u001b[0m eta: 23:38:01  iter: 2719  total_loss: 24.08  loss_ce: 0.3298  loss_cate: 0.2044  loss_mask: 0.8704  loss_dice: 1.09  loss_ce_0: 0.4077  loss_cate_0: 0  loss_mask_0: 0.9321  loss_dice_0: 1.204  loss_ce_1: 0.2556  loss_cate_1: 0  loss_mask_1: 0.905  loss_dice_1: 1.155  loss_ce_2: 0.3247  loss_cate_2: 0  loss_mask_2: 0.8713  loss_dice_2: 1.085  loss_ce_3: 0.2987  loss_cate_3: 0  loss_mask_3: 0.8976  loss_dice_3: 1.106  loss_ce_4: 0.3371  loss_cate_4: 0  loss_mask_4: 0.878  loss_dice_4: 1.088  loss_ce_5: 0.3051  loss_cate_5: 0  loss_mask_5: 0.8978  loss_dice_5: 1.081  loss_ce_6: 0.3506  loss_cate_6: 0  loss_mask_6: 0.849  loss_dice_6: 1.075  loss_ce_7: 0.3302  loss_cate_7: 0  loss_mask_7: 0.8484  loss_dice_7: 1.147  loss_ce_8: 0.3079  loss_cate_8: 0  loss_mask_8: 0.8841  loss_dice_8: 1.082  time: 1.1029  data_time: 0.0123  lr: 9.6936e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:30:35 d2.utils.events]: \u001b[0m eta: 23:37:15  iter: 2739  total_loss: 24.34  loss_ce: 0.285  loss_cate: 0.2008  loss_mask: 0.9088  loss_dice: 1.079  loss_ce_0: 0.4258  loss_cate_0: 0  loss_mask_0: 0.9481  loss_dice_0: 1.083  loss_ce_1: 0.3063  loss_cate_1: 0  loss_mask_1: 0.9498  loss_dice_1: 1.117  loss_ce_2: 0.2508  loss_cate_2: 0  loss_mask_2: 0.9195  loss_dice_2: 1.128  loss_ce_3: 0.2796  loss_cate_3: 0  loss_mask_3: 0.9247  loss_dice_3: 1.155  loss_ce_4: 0.2585  loss_cate_4: 0  loss_mask_4: 0.9103  loss_dice_4: 1.096  loss_ce_5: 0.243  loss_cate_5: 0  loss_mask_5: 0.9109  loss_dice_5: 1.104  loss_ce_6: 0.2457  loss_cate_6: 0  loss_mask_6: 0.8955  loss_dice_6: 1.106  loss_ce_7: 0.2533  loss_cate_7: 0  loss_mask_7: 0.9068  loss_dice_7: 1.072  loss_ce_8: 0.2605  loss_cate_8: 0  loss_mask_8: 0.9117  loss_dice_8: 1.075  time: 1.1029  data_time: 0.0127  lr: 9.6913e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:30:58 d2.utils.events]: \u001b[0m eta: 23:36:41  iter: 2759  total_loss: 21.72  loss_ce: 0.2076  loss_cate: 0.199  loss_mask: 0.8335  loss_dice: 0.9943  loss_ce_0: 0.3998  loss_cate_0: 0  loss_mask_0: 0.8526  loss_dice_0: 1.059  loss_ce_1: 0.2831  loss_cate_1: 0  loss_mask_1: 0.8776  loss_dice_1: 1.038  loss_ce_2: 0.1695  loss_cate_2: 0  loss_mask_2: 0.8261  loss_dice_2: 0.9933  loss_ce_3: 0.2336  loss_cate_3: 0  loss_mask_3: 0.8056  loss_dice_3: 1.002  loss_ce_4: 0.234  loss_cate_4: 0  loss_mask_4: 0.8224  loss_dice_4: 0.9336  loss_ce_5: 0.2271  loss_cate_5: 0  loss_mask_5: 0.8251  loss_dice_5: 0.9772  loss_ce_6: 0.2361  loss_cate_6: 0  loss_mask_6: 0.8247  loss_dice_6: 0.9768  loss_ce_7: 0.2363  loss_cate_7: 0  loss_mask_7: 0.8242  loss_dice_7: 0.9849  loss_ce_8: 0.2365  loss_cate_8: 0  loss_mask_8: 0.8378  loss_dice_8: 1.005  time: 1.1028  data_time: 0.0129  lr: 9.6891e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:31:20 d2.utils.events]: \u001b[0m eta: 23:36:14  iter: 2779  total_loss: 23.55  loss_ce: 0.1576  loss_cate: 0.2038  loss_mask: 0.9399  loss_dice: 1.107  loss_ce_0: 0.3181  loss_cate_0: 0  loss_mask_0: 1.025  loss_dice_0: 1.119  loss_ce_1: 0.2177  loss_cate_1: 0  loss_mask_1: 1.005  loss_dice_1: 1.054  loss_ce_2: 0.235  loss_cate_2: 0  loss_mask_2: 0.9289  loss_dice_2: 1.056  loss_ce_3: 0.1743  loss_cate_3: 0  loss_mask_3: 0.9179  loss_dice_3: 1.11  loss_ce_4: 0.1171  loss_cate_4: 0  loss_mask_4: 0.9412  loss_dice_4: 1.128  loss_ce_5: 0.1856  loss_cate_5: 0  loss_mask_5: 0.921  loss_dice_5: 1.058  loss_ce_6: 0.0972  loss_cate_6: 0  loss_mask_6: 0.991  loss_dice_6: 1.126  loss_ce_7: 0.1031  loss_cate_7: 0  loss_mask_7: 0.9486  loss_dice_7: 1.085  loss_ce_8: 0.1683  loss_cate_8: 0  loss_mask_8: 0.9871  loss_dice_8: 1.065  time: 1.1028  data_time: 0.0126  lr: 9.6868e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:31:42 d2.utils.events]: \u001b[0m eta: 23:35:52  iter: 2799  total_loss: 22.43  loss_ce: 0.2359  loss_cate: 0.2024  loss_mask: 0.8802  loss_dice: 1.132  loss_ce_0: 0.3867  loss_cate_0: 0  loss_mask_0: 0.9987  loss_dice_0: 1.236  loss_ce_1: 0.2486  loss_cate_1: 0  loss_mask_1: 1.005  loss_dice_1: 1.201  loss_ce_2: 0.2321  loss_cate_2: 0  loss_mask_2: 0.8549  loss_dice_2: 1.188  loss_ce_3: 0.2197  loss_cate_3: 0  loss_mask_3: 0.886  loss_dice_3: 1.154  loss_ce_4: 0.2247  loss_cate_4: 0  loss_mask_4: 0.8958  loss_dice_4: 1.168  loss_ce_5: 0.2445  loss_cate_5: 0  loss_mask_5: 0.9206  loss_dice_5: 1.164  loss_ce_6: 0.1898  loss_cate_6: 0  loss_mask_6: 0.8793  loss_dice_6: 1.123  loss_ce_7: 0.2165  loss_cate_7: 0  loss_mask_7: 0.9009  loss_dice_7: 1.149  loss_ce_8: 0.2092  loss_cate_8: 0  loss_mask_8: 0.8872  loss_dice_8: 1.159  time: 1.1028  data_time: 0.0121  lr: 9.6846e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:32:05 d2.utils.events]: \u001b[0m eta: 23:35:17  iter: 2819  total_loss: 25.03  loss_ce: 0.346  loss_cate: 0.2452  loss_mask: 0.9758  loss_dice: 1.142  loss_ce_0: 0.4112  loss_cate_0: 0  loss_mask_0: 0.9844  loss_dice_0: 1.145  loss_ce_1: 0.3762  loss_cate_1: 0  loss_mask_1: 1.035  loss_dice_1: 1.197  loss_ce_2: 0.3266  loss_cate_2: 0  loss_mask_2: 0.9483  loss_dice_2: 1.116  loss_ce_3: 0.2956  loss_cate_3: 0  loss_mask_3: 0.9675  loss_dice_3: 1.171  loss_ce_4: 0.2905  loss_cate_4: 0  loss_mask_4: 0.9753  loss_dice_4: 1.129  loss_ce_5: 0.3131  loss_cate_5: 0  loss_mask_5: 0.9865  loss_dice_5: 1.158  loss_ce_6: 0.3384  loss_cate_6: 0  loss_mask_6: 0.9076  loss_dice_6: 1.102  loss_ce_7: 0.376  loss_cate_7: 0  loss_mask_7: 0.9019  loss_dice_7: 1.108  loss_ce_8: 0.3594  loss_cate_8: 0  loss_mask_8: 0.9373  loss_dice_8: 1.148  time: 1.1028  data_time: 0.0136  lr: 9.6823e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:32:27 d2.utils.events]: \u001b[0m eta: 23:34:42  iter: 2839  total_loss: 24.67  loss_ce: 0.264  loss_cate: 0.2172  loss_mask: 0.89  loss_dice: 1.068  loss_ce_0: 0.3879  loss_cate_0: 0  loss_mask_0: 0.9208  loss_dice_0: 1.158  loss_ce_1: 0.3657  loss_cate_1: 0  loss_mask_1: 0.8596  loss_dice_1: 1.114  loss_ce_2: 0.3472  loss_cate_2: 0  loss_mask_2: 0.8885  loss_dice_2: 1.075  loss_ce_3: 0.3223  loss_cate_3: 0  loss_mask_3: 0.9418  loss_dice_3: 1.168  loss_ce_4: 0.2775  loss_cate_4: 0  loss_mask_4: 0.8913  loss_dice_4: 1.13  loss_ce_5: 0.2624  loss_cate_5: 0  loss_mask_5: 0.9015  loss_dice_5: 1.076  loss_ce_6: 0.2962  loss_cate_6: 0  loss_mask_6: 0.9044  loss_dice_6: 1.089  loss_ce_7: 0.2663  loss_cate_7: 0  loss_mask_7: 0.8897  loss_dice_7: 1.109  loss_ce_8: 0.2744  loss_cate_8: 0  loss_mask_8: 0.901  loss_dice_8: 1.093  time: 1.1028  data_time: 0.0141  lr: 9.68e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:32:49 d2.utils.events]: \u001b[0m eta: 23:34:32  iter: 2859  total_loss: 24.9  loss_ce: 0.3548  loss_cate: 0.1928  loss_mask: 1.081  loss_dice: 1.101  loss_ce_0: 0.4918  loss_cate_0: 0  loss_mask_0: 1.074  loss_dice_0: 1.165  loss_ce_1: 0.3357  loss_cate_1: 0  loss_mask_1: 1.102  loss_dice_1: 1.146  loss_ce_2: 0.2921  loss_cate_2: 0  loss_mask_2: 1.098  loss_dice_2: 1.129  loss_ce_3: 0.3071  loss_cate_3: 0  loss_mask_3: 1.073  loss_dice_3: 1.123  loss_ce_4: 0.3089  loss_cate_4: 0  loss_mask_4: 1.063  loss_dice_4: 1.108  loss_ce_5: 0.2549  loss_cate_5: 0  loss_mask_5: 1.068  loss_dice_5: 1.129  loss_ce_6: 0.34  loss_cate_6: 0  loss_mask_6: 1.074  loss_dice_6: 1.114  loss_ce_7: 0.3373  loss_cate_7: 0  loss_mask_7: 1.082  loss_dice_7: 1.094  loss_ce_8: 0.3211  loss_cate_8: 0  loss_mask_8: 1.065  loss_dice_8: 1.142  time: 1.1028  data_time: 0.0145  lr: 9.6778e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:33:12 d2.utils.events]: \u001b[0m eta: 23:34:12  iter: 2879  total_loss: 24.36  loss_ce: 0.2171  loss_cate: 0.1976  loss_mask: 0.988  loss_dice: 1.26  loss_ce_0: 0.3608  loss_cate_0: 0  loss_mask_0: 1.01  loss_dice_0: 1.216  loss_ce_1: 0.2851  loss_cate_1: 0  loss_mask_1: 0.9448  loss_dice_1: 1.247  loss_ce_2: 0.2554  loss_cate_2: 0  loss_mask_2: 0.9652  loss_dice_2: 1.203  loss_ce_3: 0.2267  loss_cate_3: 0  loss_mask_3: 0.9649  loss_dice_3: 1.195  loss_ce_4: 0.2016  loss_cate_4: 0  loss_mask_4: 0.9811  loss_dice_4: 1.191  loss_ce_5: 0.237  loss_cate_5: 0  loss_mask_5: 0.9813  loss_dice_5: 1.203  loss_ce_6: 0.2387  loss_cate_6: 0  loss_mask_6: 0.896  loss_dice_6: 1.22  loss_ce_7: 0.1985  loss_cate_7: 0  loss_mask_7: 0.9185  loss_dice_7: 1.234  loss_ce_8: 0.2076  loss_cate_8: 0  loss_mask_8: 0.93  loss_dice_8: 1.244  time: 1.1027  data_time: 0.0126  lr: 9.6755e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:33:34 d2.utils.events]: \u001b[0m eta: 23:33:46  iter: 2899  total_loss: 25.64  loss_ce: 0.3259  loss_cate: 0.2105  loss_mask: 1  loss_dice: 1.141  loss_ce_0: 0.4274  loss_cate_0: 0  loss_mask_0: 0.9681  loss_dice_0: 1.231  loss_ce_1: 0.3914  loss_cate_1: 0  loss_mask_1: 0.9448  loss_dice_1: 1.212  loss_ce_2: 0.3742  loss_cate_2: 0  loss_mask_2: 0.9428  loss_dice_2: 1.102  loss_ce_3: 0.4039  loss_cate_3: 0  loss_mask_3: 0.9751  loss_dice_3: 1.081  loss_ce_4: 0.4628  loss_cate_4: 0  loss_mask_4: 0.9421  loss_dice_4: 1.082  loss_ce_5: 0.3883  loss_cate_5: 0  loss_mask_5: 0.9273  loss_dice_5: 1.112  loss_ce_6: 0.341  loss_cate_6: 0  loss_mask_6: 0.9247  loss_dice_6: 1.093  loss_ce_7: 0.3691  loss_cate_7: 0  loss_mask_7: 0.9243  loss_dice_7: 1.104  loss_ce_8: 0.335  loss_cate_8: 0  loss_mask_8: 0.9588  loss_dice_8: 1.134  time: 1.1027  data_time: 0.0132  lr: 9.6733e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:33:56 d2.utils.events]: \u001b[0m eta: 23:33:24  iter: 2919  total_loss: 25.19  loss_ce: 0.2962  loss_cate: 0.2282  loss_mask: 1.043  loss_dice: 1.107  loss_ce_0: 0.4734  loss_cate_0: 0  loss_mask_0: 0.9785  loss_dice_0: 1.118  loss_ce_1: 0.3105  loss_cate_1: 0  loss_mask_1: 1.129  loss_dice_1: 1.087  loss_ce_2: 0.266  loss_cate_2: 0  loss_mask_2: 1.012  loss_dice_2: 1.123  loss_ce_3: 0.2805  loss_cate_3: 0  loss_mask_3: 1.025  loss_dice_3: 1.069  loss_ce_4: 0.27  loss_cate_4: 0  loss_mask_4: 1.048  loss_dice_4: 1.098  loss_ce_5: 0.2897  loss_cate_5: 0  loss_mask_5: 1.034  loss_dice_5: 1.087  loss_ce_6: 0.2292  loss_cate_6: 0  loss_mask_6: 1.065  loss_dice_6: 1.099  loss_ce_7: 0.2448  loss_cate_7: 0  loss_mask_7: 1.085  loss_dice_7: 1.089  loss_ce_8: 0.2723  loss_cate_8: 0  loss_mask_8: 1.056  loss_dice_8: 1.125  time: 1.1027  data_time: 0.0141  lr: 9.671e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:34:19 d2.utils.events]: \u001b[0m eta: 23:33:07  iter: 2939  total_loss: 23.03  loss_ce: 0.2893  loss_cate: 0.2137  loss_mask: 0.8788  loss_dice: 1.068  loss_ce_0: 0.3884  loss_cate_0: 0  loss_mask_0: 0.9074  loss_dice_0: 1.115  loss_ce_1: 0.3098  loss_cate_1: 0  loss_mask_1: 0.8999  loss_dice_1: 1.147  loss_ce_2: 0.2903  loss_cate_2: 0  loss_mask_2: 0.8927  loss_dice_2: 1.071  loss_ce_3: 0.2986  loss_cate_3: 0  loss_mask_3: 0.8571  loss_dice_3: 1.054  loss_ce_4: 0.2885  loss_cate_4: 0  loss_mask_4: 0.887  loss_dice_4: 1.113  loss_ce_5: 0.2601  loss_cate_5: 0  loss_mask_5: 0.8642  loss_dice_5: 1.134  loss_ce_6: 0.308  loss_cate_6: 0  loss_mask_6: 0.8743  loss_dice_6: 1.151  loss_ce_7: 0.2854  loss_cate_7: 0  loss_mask_7: 0.8645  loss_dice_7: 1.085  loss_ce_8: 0.2696  loss_cate_8: 0  loss_mask_8: 0.8974  loss_dice_8: 1.078  time: 1.1027  data_time: 0.0140  lr: 9.6687e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:34:41 d2.utils.events]: \u001b[0m eta: 23:32:45  iter: 2959  total_loss: 21.21  loss_ce: 0.2333  loss_cate: 0.1716  loss_mask: 0.8215  loss_dice: 1.023  loss_ce_0: 0.3947  loss_cate_0: 0  loss_mask_0: 0.8316  loss_dice_0: 1.013  loss_ce_1: 0.2998  loss_cate_1: 0  loss_mask_1: 0.8371  loss_dice_1: 0.9674  loss_ce_2: 0.2628  loss_cate_2: 0  loss_mask_2: 0.8541  loss_dice_2: 0.9671  loss_ce_3: 0.298  loss_cate_3: 0  loss_mask_3: 0.8449  loss_dice_3: 0.9899  loss_ce_4: 0.2651  loss_cate_4: 0  loss_mask_4: 0.8447  loss_dice_4: 0.9621  loss_ce_5: 0.2649  loss_cate_5: 0  loss_mask_5: 0.8284  loss_dice_5: 0.9391  loss_ce_6: 0.2433  loss_cate_6: 0  loss_mask_6: 0.8218  loss_dice_6: 0.9541  loss_ce_7: 0.2311  loss_cate_7: 0  loss_mask_7: 0.8335  loss_dice_7: 1.008  loss_ce_8: 0.2328  loss_cate_8: 0  loss_mask_8: 0.828  loss_dice_8: 0.9887  time: 1.1027  data_time: 0.0120  lr: 9.6665e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:35:04 d2.utils.events]: \u001b[0m eta: 23:32:24  iter: 2979  total_loss: 27.46  loss_ce: 0.3344  loss_cate: 0.1837  loss_mask: 1.047  loss_dice: 1.2  loss_ce_0: 0.5474  loss_cate_0: 0  loss_mask_0: 0.9466  loss_dice_0: 1.291  loss_ce_1: 0.3327  loss_cate_1: 0  loss_mask_1: 1.038  loss_dice_1: 1.294  loss_ce_2: 0.376  loss_cate_2: 0  loss_mask_2: 1.008  loss_dice_2: 1.16  loss_ce_3: 0.354  loss_cate_3: 0  loss_mask_3: 1.056  loss_dice_3: 1.154  loss_ce_4: 0.3438  loss_cate_4: 0  loss_mask_4: 1.026  loss_dice_4: 1.143  loss_ce_5: 0.3536  loss_cate_5: 0  loss_mask_5: 1  loss_dice_5: 1.155  loss_ce_6: 0.3156  loss_cate_6: 0  loss_mask_6: 1.003  loss_dice_6: 1.289  loss_ce_7: 0.3606  loss_cate_7: 0  loss_mask_7: 0.9857  loss_dice_7: 1.209  loss_ce_8: 0.3537  loss_cate_8: 0  loss_mask_8: 1.026  loss_dice_8: 1.203  time: 1.1027  data_time: 0.0130  lr: 9.6642e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:36:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 12:36:23 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 12:36:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 12:37:24 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 12:37:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0013 s/iter. Inference: 0.1622 s/iter. Eval: 0.1044 s/iter. Total: 0.2679 s/iter. ETA=0:08:57\n",
      "\u001b[32m[10/12 12:37:32 d2.evaluation.evaluator]: \u001b[0mInference done 38/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0384 s/iter. Total: 0.2046 s/iter. ETA=0:06:44\n",
      "\u001b[32m[10/12 12:37:37 d2.evaluation.evaluator]: \u001b[0mInference done 64/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0348 s/iter. Total: 0.2006 s/iter. ETA=0:06:31\n",
      "\u001b[32m[10/12 12:37:43 d2.evaluation.evaluator]: \u001b[0mInference done 91/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0313 s/iter. Total: 0.1971 s/iter. ETA=0:06:19\n",
      "\u001b[32m[10/12 12:37:48 d2.evaluation.evaluator]: \u001b[0mInference done 116/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0324 s/iter. Total: 0.1983 s/iter. ETA=0:06:16\n",
      "\u001b[32m[10/12 12:37:53 d2.evaluation.evaluator]: \u001b[0mInference done 144/2016. Dataloading: 0.0024 s/iter. Inference: 0.1630 s/iter. Eval: 0.0298 s/iter. Total: 0.1954 s/iter. ETA=0:06:05\n",
      "\u001b[32m[10/12 12:37:58 d2.evaluation.evaluator]: \u001b[0mInference done 171/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0294 s/iter. Total: 0.1947 s/iter. ETA=0:05:59\n",
      "\u001b[32m[10/12 12:38:03 d2.evaluation.evaluator]: \u001b[0mInference done 198/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0288 s/iter. Total: 0.1942 s/iter. ETA=0:05:53\n",
      "\u001b[32m[10/12 12:38:08 d2.evaluation.evaluator]: \u001b[0mInference done 225/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0284 s/iter. Total: 0.1938 s/iter. ETA=0:05:47\n",
      "\u001b[32m[10/12 12:38:13 d2.evaluation.evaluator]: \u001b[0mInference done 252/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0276 s/iter. Total: 0.1929 s/iter. ETA=0:05:40\n",
      "\u001b[32m[10/12 12:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 279/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0272 s/iter. Total: 0.1925 s/iter. ETA=0:05:34\n",
      "\u001b[32m[10/12 12:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 305/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0273 s/iter. Total: 0.1926 s/iter. ETA=0:05:29\n",
      "\u001b[32m[10/12 12:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 332/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0273 s/iter. Total: 0.1925 s/iter. ETA=0:05:24\n",
      "\u001b[32m[10/12 12:38:34 d2.evaluation.evaluator]: \u001b[0mInference done 358/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0275 s/iter. Total: 0.1927 s/iter. ETA=0:05:19\n",
      "\u001b[32m[10/12 12:38:39 d2.evaluation.evaluator]: \u001b[0mInference done 385/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0274 s/iter. Total: 0.1926 s/iter. ETA=0:05:14\n",
      "\u001b[32m[10/12 12:38:44 d2.evaluation.evaluator]: \u001b[0mInference done 413/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0268 s/iter. Total: 0.1920 s/iter. ETA=0:05:07\n",
      "\u001b[32m[10/12 12:38:49 d2.evaluation.evaluator]: \u001b[0mInference done 439/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0270 s/iter. Total: 0.1922 s/iter. ETA=0:05:03\n",
      "\u001b[32m[10/12 12:38:54 d2.evaluation.evaluator]: \u001b[0mInference done 465/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0271 s/iter. Total: 0.1924 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 12:38:59 d2.evaluation.evaluator]: \u001b[0mInference done 491/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0273 s/iter. Total: 0.1925 s/iter. ETA=0:04:53\n",
      "\u001b[32m[10/12 12:39:04 d2.evaluation.evaluator]: \u001b[0mInference done 519/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0268 s/iter. Total: 0.1920 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 12:39:09 d2.evaluation.evaluator]: \u001b[0mInference done 545/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0270 s/iter. Total: 0.1923 s/iter. ETA=0:04:42\n",
      "\u001b[32m[10/12 12:39:14 d2.evaluation.evaluator]: \u001b[0mInference done 572/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0268 s/iter. Total: 0.1920 s/iter. ETA=0:04:37\n",
      "\u001b[32m[10/12 12:39:20 d2.evaluation.evaluator]: \u001b[0mInference done 599/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0266 s/iter. Total: 0.1919 s/iter. ETA=0:04:31\n",
      "\u001b[32m[10/12 12:39:30 d2.evaluation.evaluator]: \u001b[0mInference done 652/2016. Dataloading: 0.0023 s/iter. Inference: 0.1630 s/iter. Eval: 0.0267 s/iter. Total: 0.1921 s/iter. ETA=0:04:22\n",
      "\u001b[32m[10/12 12:39:35 d2.evaluation.evaluator]: \u001b[0mInference done 680/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0263 s/iter. Total: 0.1917 s/iter. ETA=0:04:16\n",
      "\u001b[32m[10/12 12:39:40 d2.evaluation.evaluator]: \u001b[0mInference done 705/2016. Dataloading: 0.0023 s/iter. Inference: 0.1630 s/iter. Eval: 0.0266 s/iter. Total: 0.1920 s/iter. ETA=0:04:11\n",
      "\u001b[32m[10/12 12:39:45 d2.evaluation.evaluator]: \u001b[0mInference done 732/2016. Dataloading: 0.0023 s/iter. Inference: 0.1630 s/iter. Eval: 0.0265 s/iter. Total: 0.1920 s/iter. ETA=0:04:06\n",
      "\u001b[32m[10/12 12:39:50 d2.evaluation.evaluator]: \u001b[0mInference done 758/2016. Dataloading: 0.0023 s/iter. Inference: 0.1631 s/iter. Eval: 0.0266 s/iter. Total: 0.1921 s/iter. ETA=0:04:01\n",
      "\u001b[32m[10/12 12:39:55 d2.evaluation.evaluator]: \u001b[0mInference done 785/2016. Dataloading: 0.0023 s/iter. Inference: 0.1631 s/iter. Eval: 0.0264 s/iter. Total: 0.1919 s/iter. ETA=0:03:56\n",
      "\u001b[32m[10/12 12:40:00 d2.evaluation.evaluator]: \u001b[0mInference done 812/2016. Dataloading: 0.0023 s/iter. Inference: 0.1631 s/iter. Eval: 0.0263 s/iter. Total: 0.1918 s/iter. ETA=0:03:50\n",
      "\u001b[32m[10/12 12:40:05 d2.evaluation.evaluator]: \u001b[0mInference done 839/2016. Dataloading: 0.0023 s/iter. Inference: 0.1631 s/iter. Eval: 0.0261 s/iter. Total: 0.1916 s/iter. ETA=0:03:45\n",
      "\u001b[32m[10/12 12:40:10 d2.evaluation.evaluator]: \u001b[0mInference done 865/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0262 s/iter. Total: 0.1917 s/iter. ETA=0:03:40\n",
      "\u001b[32m[10/12 12:40:15 d2.evaluation.evaluator]: \u001b[0mInference done 892/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0260 s/iter. Total: 0.1915 s/iter. ETA=0:03:35\n",
      "\u001b[32m[10/12 12:40:20 d2.evaluation.evaluator]: \u001b[0mInference done 918/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0261 s/iter. Total: 0.1916 s/iter. ETA=0:03:30\n",
      "\u001b[32m[10/12 12:40:26 d2.evaluation.evaluator]: \u001b[0mInference done 945/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0260 s/iter. Total: 0.1915 s/iter. ETA=0:03:25\n",
      "\u001b[32m[10/12 12:40:31 d2.evaluation.evaluator]: \u001b[0mInference done 971/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0261 s/iter. Total: 0.1916 s/iter. ETA=0:03:20\n",
      "\u001b[32m[10/12 12:40:36 d2.evaluation.evaluator]: \u001b[0mInference done 998/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0260 s/iter. Total: 0.1914 s/iter. ETA=0:03:14\n",
      "\u001b[32m[10/12 12:40:41 d2.evaluation.evaluator]: \u001b[0mInference done 1023/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0261 s/iter. Total: 0.1917 s/iter. ETA=0:03:10\n",
      "\u001b[32m[10/12 12:40:46 d2.evaluation.evaluator]: \u001b[0mInference done 1049/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0263 s/iter. Total: 0.1918 s/iter. ETA=0:03:05\n",
      "\u001b[32m[10/12 12:40:51 d2.evaluation.evaluator]: \u001b[0mInference done 1075/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0263 s/iter. Total: 0.1919 s/iter. ETA=0:03:00\n",
      "\u001b[32m[10/12 12:40:56 d2.evaluation.evaluator]: \u001b[0mInference done 1101/2016. Dataloading: 0.0023 s/iter. Inference: 0.1632 s/iter. Eval: 0.0263 s/iter. Total: 0.1919 s/iter. ETA=0:02:55\n",
      "\u001b[32m[10/12 12:41:01 d2.evaluation.evaluator]: \u001b[0mInference done 1126/2016. Dataloading: 0.0023 s/iter. Inference: 0.1632 s/iter. Eval: 0.0265 s/iter. Total: 0.1921 s/iter. ETA=0:02:50\n",
      "\u001b[32m[10/12 12:41:06 d2.evaluation.evaluator]: \u001b[0mInference done 1153/2016. Dataloading: 0.0023 s/iter. Inference: 0.1632 s/iter. Eval: 0.0265 s/iter. Total: 0.1921 s/iter. ETA=0:02:45\n",
      "\u001b[32m[10/12 12:41:11 d2.evaluation.evaluator]: \u001b[0mInference done 1179/2016. Dataloading: 0.0023 s/iter. Inference: 0.1633 s/iter. Eval: 0.0266 s/iter. Total: 0.1922 s/iter. ETA=0:02:40\n",
      "\u001b[32m[10/12 12:41:17 d2.evaluation.evaluator]: \u001b[0mInference done 1206/2016. Dataloading: 0.0023 s/iter. Inference: 0.1632 s/iter. Eval: 0.0269 s/iter. Total: 0.1925 s/iter. ETA=0:02:35\n",
      "\u001b[32m[10/12 12:41:22 d2.evaluation.evaluator]: \u001b[0mInference done 1231/2016. Dataloading: 0.0023 s/iter. Inference: 0.1632 s/iter. Eval: 0.0271 s/iter. Total: 0.1927 s/iter. ETA=0:02:31\n",
      "\u001b[32m[10/12 12:41:27 d2.evaluation.evaluator]: \u001b[0mInference done 1258/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0270 s/iter. Total: 0.1926 s/iter. ETA=0:02:26\n",
      "\u001b[32m[10/12 12:41:32 d2.evaluation.evaluator]: \u001b[0mInference done 1284/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0270 s/iter. Total: 0.1926 s/iter. ETA=0:02:21\n",
      "\u001b[32m[10/12 12:41:37 d2.evaluation.evaluator]: \u001b[0mInference done 1312/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0268 s/iter. Total: 0.1925 s/iter. ETA=0:02:15\n",
      "\u001b[32m[10/12 12:41:42 d2.evaluation.evaluator]: \u001b[0mInference done 1338/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0269 s/iter. Total: 0.1925 s/iter. ETA=0:02:10\n",
      "\u001b[32m[10/12 12:41:47 d2.evaluation.evaluator]: \u001b[0mInference done 1365/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0268 s/iter. Total: 0.1925 s/iter. ETA=0:02:05\n",
      "\u001b[32m[10/12 12:41:52 d2.evaluation.evaluator]: \u001b[0mInference done 1391/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0269 s/iter. Total: 0.1925 s/iter. ETA=0:02:00\n",
      "\u001b[32m[10/12 12:41:57 d2.evaluation.evaluator]: \u001b[0mInference done 1418/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0267 s/iter. Total: 0.1923 s/iter. ETA=0:01:55\n",
      "\u001b[32m[10/12 12:42:02 d2.evaluation.evaluator]: \u001b[0mInference done 1443/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0269 s/iter. Total: 0.1925 s/iter. ETA=0:01:50\n",
      "\u001b[32m[10/12 12:42:08 d2.evaluation.evaluator]: \u001b[0mInference done 1471/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0267 s/iter. Total: 0.1924 s/iter. ETA=0:01:44\n",
      "\u001b[32m[10/12 12:42:13 d2.evaluation.evaluator]: \u001b[0mInference done 1496/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0269 s/iter. Total: 0.1925 s/iter. ETA=0:01:40\n",
      "\u001b[32m[10/12 12:42:18 d2.evaluation.evaluator]: \u001b[0mInference done 1522/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0269 s/iter. Total: 0.1925 s/iter. ETA=0:01:35\n",
      "\u001b[32m[10/12 12:42:23 d2.evaluation.evaluator]: \u001b[0mInference done 1549/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0268 s/iter. Total: 0.1925 s/iter. ETA=0:01:29\n",
      "\u001b[32m[10/12 12:42:28 d2.evaluation.evaluator]: \u001b[0mInference done 1574/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0270 s/iter. Total: 0.1926 s/iter. ETA=0:01:25\n",
      "\u001b[32m[10/12 12:42:33 d2.evaluation.evaluator]: \u001b[0mInference done 1601/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0269 s/iter. Total: 0.1926 s/iter. ETA=0:01:19\n",
      "\u001b[32m[10/12 12:42:38 d2.evaluation.evaluator]: \u001b[0mInference done 1626/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0271 s/iter. Total: 0.1927 s/iter. ETA=0:01:15\n",
      "\u001b[32m[10/12 12:42:43 d2.evaluation.evaluator]: \u001b[0mInference done 1651/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0272 s/iter. Total: 0.1928 s/iter. ETA=0:01:10\n",
      "\u001b[32m[10/12 12:42:48 d2.evaluation.evaluator]: \u001b[0mInference done 1674/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0276 s/iter. Total: 0.1932 s/iter. ETA=0:01:06\n",
      "\u001b[32m[10/12 12:42:53 d2.evaluation.evaluator]: \u001b[0mInference done 1699/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0277 s/iter. Total: 0.1933 s/iter. ETA=0:01:01\n",
      "\u001b[32m[10/12 12:42:58 d2.evaluation.evaluator]: \u001b[0mInference done 1725/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0277 s/iter. Total: 0.1934 s/iter. ETA=0:00:56\n",
      "\u001b[32m[10/12 12:43:03 d2.evaluation.evaluator]: \u001b[0mInference done 1751/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0278 s/iter. Total: 0.1935 s/iter. ETA=0:00:51\n",
      "\u001b[32m[10/12 12:43:08 d2.evaluation.evaluator]: \u001b[0mInference done 1776/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1936 s/iter. ETA=0:00:46\n",
      "\u001b[32m[10/12 12:43:14 d2.evaluation.evaluator]: \u001b[0mInference done 1802/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1936 s/iter. ETA=0:00:41\n",
      "\u001b[32m[10/12 12:43:19 d2.evaluation.evaluator]: \u001b[0mInference done 1829/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1936 s/iter. ETA=0:00:36\n",
      "\u001b[32m[10/12 12:43:24 d2.evaluation.evaluator]: \u001b[0mInference done 1854/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0280 s/iter. Total: 0.1937 s/iter. ETA=0:00:31\n",
      "\u001b[32m[10/12 12:43:29 d2.evaluation.evaluator]: \u001b[0mInference done 1881/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1936 s/iter. ETA=0:00:26\n",
      "\u001b[32m[10/12 12:43:34 d2.evaluation.evaluator]: \u001b[0mInference done 1907/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1937 s/iter. ETA=0:00:21\n",
      "\u001b[32m[10/12 12:43:39 d2.evaluation.evaluator]: \u001b[0mInference done 1933/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0280 s/iter. Total: 0.1937 s/iter. ETA=0:00:16\n",
      "\u001b[32m[10/12 12:43:44 d2.evaluation.evaluator]: \u001b[0mInference done 1960/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1937 s/iter. ETA=0:00:10\n",
      "\u001b[32m[10/12 12:43:49 d2.evaluation.evaluator]: \u001b[0mInference done 1985/2016. Dataloading: 0.0022 s/iter. Inference: 0.1634 s/iter. Eval: 0.0280 s/iter. Total: 0.1938 s/iter. ETA=0:00:06\n",
      "\u001b[32m[10/12 12:43:54 d2.evaluation.evaluator]: \u001b[0mInference done 2011/2016. Dataloading: 0.0022 s/iter. Inference: 0.1634 s/iter. Eval: 0.0280 s/iter. Total: 0.1938 s/iter. ETA=0:00:00\n",
      "\u001b[32m[10/12 12:43:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:29.744197 (0.193806 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 12:43:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:28 (0.163361 s / iter per device, on 1 devices)\n",
      "miou = 73.96910083453007\n",
      "OA = 88.21129382602753\n",
      "Kappa = 84.60440692970838\n",
      "F1_score = 70.07815503831901\n",
      "\u001b[32m[10/12 12:43:56 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 73.96910083453007, 'fwIoU': 79.55166326186415, 'IoU-Background': 37.645210963775085, 'IoU-Surfaces': 83.2861363893502, 'IoU-Building': 90.77658332401111, 'IoU-Low vegetation': 74.07734350004813, 'IoU-tree': 74.66135692917226, 'IoU-Car': 83.3679739008236, 'mACC': 82.6166075646742, 'pACC': 88.21129382602753, 'ACC-Background': 49.49859972203502, 'ACC-Surfaces': 92.20791480513468, 'ACC-Building': 93.64568743033024, 'ACC-Low vegetation': 89.0975042510188, 'ACC-tree': 82.64383144281835, 'ACC-Car': 88.60610773670801})])\n",
      "\u001b[32m[10/12 12:43:56 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 12:43:56 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 12:43:56 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 12:43:56 d2.evaluation.testing]: \u001b[0mcopypaste: 73.9691,79.5517,82.6166,88.2113\n",
      "\u001b[32m[10/12 12:43:56 d2.utils.events]: \u001b[0m eta: 23:32:05  iter: 2999  total_loss: 25.09  loss_ce: 0.292  loss_cate: 0.2266  loss_mask: 1.044  loss_dice: 1.194  loss_ce_0: 0.4077  loss_cate_0: 0  loss_mask_0: 1.019  loss_dice_0: 1.24  loss_ce_1: 0.3644  loss_cate_1: 0  loss_mask_1: 1.018  loss_dice_1: 1.163  loss_ce_2: 0.344  loss_cate_2: 0  loss_mask_2: 0.9487  loss_dice_2: 1.103  loss_ce_3: 0.3333  loss_cate_3: 0  loss_mask_3: 1.022  loss_dice_3: 1.098  loss_ce_4: 0.3408  loss_cate_4: 0  loss_mask_4: 1.004  loss_dice_4: 1.126  loss_ce_5: 0.3185  loss_cate_5: 0  loss_mask_5: 1.032  loss_dice_5: 1.12  loss_ce_6: 0.2876  loss_cate_6: 0  loss_mask_6: 1.056  loss_dice_6: 1.181  loss_ce_7: 0.3133  loss_cate_7: 0  loss_mask_7: 0.9982  loss_dice_7: 1.132  loss_ce_8: 0.2978  loss_cate_8: 0  loss_mask_8: 0.9651  loss_dice_8: 1.167  time: 1.1027  data_time: 0.0140  lr: 9.662e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:44:18 d2.utils.events]: \u001b[0m eta: 23:31:44  iter: 3019  total_loss: 26.18  loss_ce: 0.2559  loss_cate: 0.2129  loss_mask: 1.073  loss_dice: 1.2  loss_ce_0: 0.3942  loss_cate_0: 0  loss_mask_0: 0.9747  loss_dice_0: 1.286  loss_ce_1: 0.3093  loss_cate_1: 0  loss_mask_1: 1.013  loss_dice_1: 1.267  loss_ce_2: 0.2802  loss_cate_2: 0  loss_mask_2: 1.091  loss_dice_2: 1.207  loss_ce_3: 0.2522  loss_cate_3: 0  loss_mask_3: 1.011  loss_dice_3: 1.189  loss_ce_4: 0.2738  loss_cate_4: 0  loss_mask_4: 1.081  loss_dice_4: 1.241  loss_ce_5: 0.2542  loss_cate_5: 0  loss_mask_5: 1.065  loss_dice_5: 1.195  loss_ce_6: 0.2515  loss_cate_6: 0  loss_mask_6: 1.057  loss_dice_6: 1.176  loss_ce_7: 0.2503  loss_cate_7: 0  loss_mask_7: 1.065  loss_dice_7: 1.17  loss_ce_8: 0.2696  loss_cate_8: 0  loss_mask_8: 1.032  loss_dice_8: 1.207  time: 1.1027  data_time: 0.0141  lr: 9.6597e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:44:40 d2.utils.events]: \u001b[0m eta: 23:31:05  iter: 3039  total_loss: 23.88  loss_ce: 0.3097  loss_cate: 0.1912  loss_mask: 0.9383  loss_dice: 1.129  loss_ce_0: 0.4189  loss_cate_0: 0  loss_mask_0: 0.9874  loss_dice_0: 1.181  loss_ce_1: 0.2769  loss_cate_1: 0  loss_mask_1: 0.9452  loss_dice_1: 1.157  loss_ce_2: 0.2867  loss_cate_2: 0  loss_mask_2: 0.9394  loss_dice_2: 1.127  loss_ce_3: 0.2527  loss_cate_3: 0  loss_mask_3: 0.9546  loss_dice_3: 1.145  loss_ce_4: 0.3014  loss_cate_4: 0  loss_mask_4: 0.9457  loss_dice_4: 1.162  loss_ce_5: 0.2541  loss_cate_5: 0  loss_mask_5: 0.9322  loss_dice_5: 1.143  loss_ce_6: 0.2865  loss_cate_6: 0  loss_mask_6: 0.9424  loss_dice_6: 1.111  loss_ce_7: 0.3125  loss_cate_7: 0  loss_mask_7: 0.9499  loss_dice_7: 1.096  loss_ce_8: 0.3061  loss_cate_8: 0  loss_mask_8: 0.9407  loss_dice_8: 1.106  time: 1.1026  data_time: 0.0122  lr: 9.6575e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:45:03 d2.utils.events]: \u001b[0m eta: 23:30:30  iter: 3059  total_loss: 25.85  loss_ce: 0.258  loss_cate: 0.2327  loss_mask: 1.062  loss_dice: 1.147  loss_ce_0: 0.4161  loss_cate_0: 0  loss_mask_0: 1.131  loss_dice_0: 1.205  loss_ce_1: 0.2653  loss_cate_1: 0  loss_mask_1: 1.139  loss_dice_1: 1.206  loss_ce_2: 0.2528  loss_cate_2: 0  loss_mask_2: 1.139  loss_dice_2: 1.186  loss_ce_3: 0.2363  loss_cate_3: 0  loss_mask_3: 1.084  loss_dice_3: 1.158  loss_ce_4: 0.2279  loss_cate_4: 0  loss_mask_4: 1.1  loss_dice_4: 1.127  loss_ce_5: 0.2198  loss_cate_5: 0  loss_mask_5: 1.147  loss_dice_5: 1.169  loss_ce_6: 0.2192  loss_cate_6: 0  loss_mask_6: 1.036  loss_dice_6: 1.166  loss_ce_7: 0.2581  loss_cate_7: 0  loss_mask_7: 1.067  loss_dice_7: 1.159  loss_ce_8: 0.2474  loss_cate_8: 0  loss_mask_8: 1.072  loss_dice_8: 1.146  time: 1.1026  data_time: 0.0132  lr: 9.6552e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:45:26 d2.utils.events]: \u001b[0m eta: 23:30:08  iter: 3079  total_loss: 23.75  loss_ce: 0.3658  loss_cate: 0.1751  loss_mask: 0.7558  loss_dice: 1.15  loss_ce_0: 0.429  loss_cate_0: 0  loss_mask_0: 0.7832  loss_dice_0: 1.16  loss_ce_1: 0.3679  loss_cate_1: 0  loss_mask_1: 0.7689  loss_dice_1: 1.155  loss_ce_2: 0.3681  loss_cate_2: 0  loss_mask_2: 0.7906  loss_dice_2: 1.121  loss_ce_3: 0.3467  loss_cate_3: 0  loss_mask_3: 0.7977  loss_dice_3: 1.133  loss_ce_4: 0.337  loss_cate_4: 0  loss_mask_4: 0.7936  loss_dice_4: 1.14  loss_ce_5: 0.3535  loss_cate_5: 0  loss_mask_5: 0.7976  loss_dice_5: 1.163  loss_ce_6: 0.3479  loss_cate_6: 0  loss_mask_6: 0.794  loss_dice_6: 1.123  loss_ce_7: 0.3553  loss_cate_7: 0  loss_mask_7: 0.7837  loss_dice_7: 1.172  loss_ce_8: 0.3635  loss_cate_8: 0  loss_mask_8: 0.7925  loss_dice_8: 1.156  time: 1.1025  data_time: 0.0143  lr: 9.6529e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:45:48 d2.utils.events]: \u001b[0m eta: 23:29:38  iter: 3099  total_loss: 25.74  loss_ce: 0.3496  loss_cate: 0.2533  loss_mask: 1.075  loss_dice: 1.095  loss_ce_0: 0.429  loss_cate_0: 0  loss_mask_0: 1.044  loss_dice_0: 1.082  loss_ce_1: 0.3461  loss_cate_1: 0  loss_mask_1: 1.029  loss_dice_1: 1.086  loss_ce_2: 0.3212  loss_cate_2: 0  loss_mask_2: 1.046  loss_dice_2: 1.085  loss_ce_3: 0.3637  loss_cate_3: 0  loss_mask_3: 1.079  loss_dice_3: 1.092  loss_ce_4: 0.3594  loss_cate_4: 0  loss_mask_4: 1.048  loss_dice_4: 1.094  loss_ce_5: 0.3455  loss_cate_5: 0  loss_mask_5: 1.051  loss_dice_5: 1.101  loss_ce_6: 0.3501  loss_cate_6: 0  loss_mask_6: 1.098  loss_dice_6: 1.093  loss_ce_7: 0.3534  loss_cate_7: 0  loss_mask_7: 1.077  loss_dice_7: 1.091  loss_ce_8: 0.3563  loss_cate_8: 0  loss_mask_8: 1.088  loss_dice_8: 1.085  time: 1.1025  data_time: 0.0133  lr: 9.6507e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:46:10 d2.utils.events]: \u001b[0m eta: 23:29:13  iter: 3119  total_loss: 23.61  loss_ce: 0.2366  loss_cate: 0.1826  loss_mask: 0.9841  loss_dice: 1.11  loss_ce_0: 0.4098  loss_cate_0: 0  loss_mask_0: 0.9356  loss_dice_0: 1.151  loss_ce_1: 0.3778  loss_cate_1: 0  loss_mask_1: 0.9377  loss_dice_1: 1.156  loss_ce_2: 0.344  loss_cate_2: 0  loss_mask_2: 1.02  loss_dice_2: 1.095  loss_ce_3: 0.3011  loss_cate_3: 0  loss_mask_3: 0.995  loss_dice_3: 1.108  loss_ce_4: 0.3155  loss_cate_4: 0  loss_mask_4: 1.008  loss_dice_4: 1.064  loss_ce_5: 0.3032  loss_cate_5: 0  loss_mask_5: 0.9729  loss_dice_5: 1.121  loss_ce_6: 0.2779  loss_cate_6: 0  loss_mask_6: 0.9805  loss_dice_6: 1.109  loss_ce_7: 0.2873  loss_cate_7: 0  loss_mask_7: 1.04  loss_dice_7: 1.149  loss_ce_8: 0.3026  loss_cate_8: 0  loss_mask_8: 0.9859  loss_dice_8: 1.161  time: 1.1026  data_time: 0.0171  lr: 9.6484e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:46:33 d2.utils.events]: \u001b[0m eta: 23:28:49  iter: 3139  total_loss: 25.4  loss_ce: 0.3195  loss_cate: 0.2459  loss_mask: 0.9318  loss_dice: 1.201  loss_ce_0: 0.4882  loss_cate_0: 0  loss_mask_0: 0.9361  loss_dice_0: 1.215  loss_ce_1: 0.3493  loss_cate_1: 0  loss_mask_1: 0.9011  loss_dice_1: 1.217  loss_ce_2: 0.3288  loss_cate_2: 0  loss_mask_2: 0.9058  loss_dice_2: 1.284  loss_ce_3: 0.2848  loss_cate_3: 0  loss_mask_3: 0.9132  loss_dice_3: 1.236  loss_ce_4: 0.331  loss_cate_4: 0  loss_mask_4: 0.921  loss_dice_4: 1.197  loss_ce_5: 0.3358  loss_cate_5: 0  loss_mask_5: 0.9192  loss_dice_5: 1.171  loss_ce_6: 0.3145  loss_cate_6: 0  loss_mask_6: 0.9209  loss_dice_6: 1.209  loss_ce_7: 0.3636  loss_cate_7: 0  loss_mask_7: 0.9008  loss_dice_7: 1.232  loss_ce_8: 0.3734  loss_cate_8: 0  loss_mask_8: 0.9228  loss_dice_8: 1.164  time: 1.1025  data_time: 0.0141  lr: 9.6462e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:46:55 d2.utils.events]: \u001b[0m eta: 23:28:23  iter: 3159  total_loss: 24.51  loss_ce: 0.2612  loss_cate: 0.2096  loss_mask: 1.03  loss_dice: 1.259  loss_ce_0: 0.3911  loss_cate_0: 0  loss_mask_0: 1.049  loss_dice_0: 1.226  loss_ce_1: 0.229  loss_cate_1: 0  loss_mask_1: 1.057  loss_dice_1: 1.256  loss_ce_2: 0.27  loss_cate_2: 0  loss_mask_2: 1.049  loss_dice_2: 1.204  loss_ce_3: 0.2464  loss_cate_3: 0  loss_mask_3: 0.9967  loss_dice_3: 1.232  loss_ce_4: 0.2399  loss_cate_4: 0  loss_mask_4: 0.998  loss_dice_4: 1.239  loss_ce_5: 0.2157  loss_cate_5: 0  loss_mask_5: 1.024  loss_dice_5: 1.238  loss_ce_6: 0.2426  loss_cate_6: 0  loss_mask_6: 1.018  loss_dice_6: 1.262  loss_ce_7: 0.279  loss_cate_7: 0  loss_mask_7: 1.02  loss_dice_7: 1.256  loss_ce_8: 0.2505  loss_cate_8: 0  loss_mask_8: 1.02  loss_dice_8: 1.219  time: 1.1025  data_time: 0.0127  lr: 9.6439e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:47:17 d2.utils.events]: \u001b[0m eta: 23:27:58  iter: 3179  total_loss: 25.6  loss_ce: 0.3588  loss_cate: 0.1891  loss_mask: 0.9258  loss_dice: 1.157  loss_ce_0: 0.5378  loss_cate_0: 0  loss_mask_0: 0.8945  loss_dice_0: 1.226  loss_ce_1: 0.3942  loss_cate_1: 0  loss_mask_1: 0.8836  loss_dice_1: 1.289  loss_ce_2: 0.4176  loss_cate_2: 0  loss_mask_2: 0.9016  loss_dice_2: 1.219  loss_ce_3: 0.3774  loss_cate_3: 0  loss_mask_3: 0.9444  loss_dice_3: 1.19  loss_ce_4: 0.4141  loss_cate_4: 0  loss_mask_4: 0.9014  loss_dice_4: 1.234  loss_ce_5: 0.3695  loss_cate_5: 0  loss_mask_5: 0.9032  loss_dice_5: 1.25  loss_ce_6: 0.4026  loss_cate_6: 0  loss_mask_6: 0.8716  loss_dice_6: 1.193  loss_ce_7: 0.3829  loss_cate_7: 0  loss_mask_7: 0.9107  loss_dice_7: 1.182  loss_ce_8: 0.3911  loss_cate_8: 0  loss_mask_8: 0.9107  loss_dice_8: 1.098  time: 1.1025  data_time: 0.0126  lr: 9.6416e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:47:40 d2.utils.events]: \u001b[0m eta: 23:27:36  iter: 3199  total_loss: 23.27  loss_ce: 0.2849  loss_cate: 0.1992  loss_mask: 0.9799  loss_dice: 1.117  loss_ce_0: 0.4773  loss_cate_0: 0  loss_mask_0: 0.9682  loss_dice_0: 1.206  loss_ce_1: 0.364  loss_cate_1: 0  loss_mask_1: 0.9493  loss_dice_1: 1.148  loss_ce_2: 0.2686  loss_cate_2: 0  loss_mask_2: 0.9498  loss_dice_2: 1.151  loss_ce_3: 0.3228  loss_cate_3: 0  loss_mask_3: 0.9579  loss_dice_3: 1.114  loss_ce_4: 0.3242  loss_cate_4: 0  loss_mask_4: 0.9544  loss_dice_4: 1.113  loss_ce_5: 0.3033  loss_cate_5: 0  loss_mask_5: 0.9583  loss_dice_5: 1.121  loss_ce_6: 0.2824  loss_cate_6: 0  loss_mask_6: 0.9469  loss_dice_6: 1.11  loss_ce_7: 0.2942  loss_cate_7: 0  loss_mask_7: 0.9588  loss_dice_7: 1.091  loss_ce_8: 0.2848  loss_cate_8: 0  loss_mask_8: 0.9424  loss_dice_8: 1.116  time: 1.1026  data_time: 0.0133  lr: 9.6394e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:48:02 d2.utils.events]: \u001b[0m eta: 23:27:21  iter: 3219  total_loss: 24.11  loss_ce: 0.2046  loss_cate: 0.1967  loss_mask: 0.8761  loss_dice: 1.123  loss_ce_0: 0.3476  loss_cate_0: 0  loss_mask_0: 0.961  loss_dice_0: 1.122  loss_ce_1: 0.2575  loss_cate_1: 0  loss_mask_1: 0.9415  loss_dice_1: 1.094  loss_ce_2: 0.2579  loss_cate_2: 0  loss_mask_2: 0.9236  loss_dice_2: 1.082  loss_ce_3: 0.2711  loss_cate_3: 0  loss_mask_3: 0.9608  loss_dice_3: 1.082  loss_ce_4: 0.245  loss_cate_4: 0  loss_mask_4: 0.9358  loss_dice_4: 1.041  loss_ce_5: 0.2365  loss_cate_5: 0  loss_mask_5: 0.9125  loss_dice_5: 1.11  loss_ce_6: 0.239  loss_cate_6: 0  loss_mask_6: 0.9146  loss_dice_6: 1.106  loss_ce_7: 0.2089  loss_cate_7: 0  loss_mask_7: 0.913  loss_dice_7: 1.113  loss_ce_8: 0.2132  loss_cate_8: 0  loss_mask_8: 0.8931  loss_dice_8: 1.12  time: 1.1026  data_time: 0.0124  lr: 9.6371e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:48:25 d2.utils.events]: \u001b[0m eta: 23:26:55  iter: 3239  total_loss: 27.35  loss_ce: 0.3546  loss_cate: 0.2093  loss_mask: 0.9327  loss_dice: 1.296  loss_ce_0: 0.3794  loss_cate_0: 0  loss_mask_0: 1.011  loss_dice_0: 1.424  loss_ce_1: 0.4587  loss_cate_1: 0  loss_mask_1: 0.9512  loss_dice_1: 1.3  loss_ce_2: 0.4949  loss_cate_2: 0  loss_mask_2: 0.9478  loss_dice_2: 1.278  loss_ce_3: 0.3375  loss_cate_3: 0  loss_mask_3: 0.9449  loss_dice_3: 1.314  loss_ce_4: 0.3951  loss_cate_4: 0  loss_mask_4: 0.948  loss_dice_4: 1.243  loss_ce_5: 0.4453  loss_cate_5: 0  loss_mask_5: 0.9239  loss_dice_5: 1.262  loss_ce_6: 0.3162  loss_cate_6: 0  loss_mask_6: 0.963  loss_dice_6: 1.258  loss_ce_7: 0.3288  loss_cate_7: 0  loss_mask_7: 0.9551  loss_dice_7: 1.25  loss_ce_8: 0.3221  loss_cate_8: 0  loss_mask_8: 0.9342  loss_dice_8: 1.289  time: 1.1026  data_time: 0.0134  lr: 9.6349e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:48:47 d2.utils.events]: \u001b[0m eta: 23:26:30  iter: 3259  total_loss: 25.05  loss_ce: 0.3176  loss_cate: 0.2304  loss_mask: 0.9661  loss_dice: 1.091  loss_ce_0: 0.449  loss_cate_0: 0  loss_mask_0: 1.005  loss_dice_0: 1.118  loss_ce_1: 0.3506  loss_cate_1: 0  loss_mask_1: 0.9621  loss_dice_1: 1.066  loss_ce_2: 0.3515  loss_cate_2: 0  loss_mask_2: 0.9932  loss_dice_2: 1.049  loss_ce_3: 0.3077  loss_cate_3: 0  loss_mask_3: 0.9847  loss_dice_3: 1.078  loss_ce_4: 0.2983  loss_cate_4: 0  loss_mask_4: 1.013  loss_dice_4: 1.076  loss_ce_5: 0.3297  loss_cate_5: 0  loss_mask_5: 0.9799  loss_dice_5: 1.147  loss_ce_6: 0.3138  loss_cate_6: 0  loss_mask_6: 0.9584  loss_dice_6: 1.07  loss_ce_7: 0.3235  loss_cate_7: 0  loss_mask_7: 0.962  loss_dice_7: 1.101  loss_ce_8: 0.3076  loss_cate_8: 0  loss_mask_8: 0.9623  loss_dice_8: 1.076  time: 1.1026  data_time: 0.0128  lr: 9.6326e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:49:11 d2.utils.events]: \u001b[0m eta: 23:26:15  iter: 3279  total_loss: 25.96  loss_ce: 0.3013  loss_cate: 0.1982  loss_mask: 1.034  loss_dice: 1.248  loss_ce_0: 0.3439  loss_cate_0: 0  loss_mask_0: 1.102  loss_dice_0: 1.34  loss_ce_1: 0.3353  loss_cate_1: 0  loss_mask_1: 1.109  loss_dice_1: 1.288  loss_ce_2: 0.3099  loss_cate_2: 0  loss_mask_2: 1.101  loss_dice_2: 1.302  loss_ce_3: 0.2973  loss_cate_3: 0  loss_mask_3: 1.05  loss_dice_3: 1.342  loss_ce_4: 0.2973  loss_cate_4: 0  loss_mask_4: 1.047  loss_dice_4: 1.358  loss_ce_5: 0.3438  loss_cate_5: 0  loss_mask_5: 1.009  loss_dice_5: 1.369  loss_ce_6: 0.31  loss_cate_6: 0  loss_mask_6: 0.9956  loss_dice_6: 1.348  loss_ce_7: 0.2992  loss_cate_7: 0  loss_mask_7: 1.035  loss_dice_7: 1.321  loss_ce_8: 0.3126  loss_cate_8: 0  loss_mask_8: 1.032  loss_dice_8: 1.32  time: 1.1026  data_time: 0.0145  lr: 9.6303e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:49:33 d2.utils.events]: \u001b[0m eta: 23:25:46  iter: 3299  total_loss: 25.15  loss_ce: 0.2466  loss_cate: 0.2039  loss_mask: 0.9224  loss_dice: 1.098  loss_ce_0: 0.3844  loss_cate_0: 0  loss_mask_0: 0.9389  loss_dice_0: 1.198  loss_ce_1: 0.247  loss_cate_1: 0  loss_mask_1: 0.9349  loss_dice_1: 1.141  loss_ce_2: 0.2353  loss_cate_2: 0  loss_mask_2: 0.9066  loss_dice_2: 1.092  loss_ce_3: 0.2379  loss_cate_3: 0  loss_mask_3: 0.8954  loss_dice_3: 1.151  loss_ce_4: 0.2568  loss_cate_4: 0  loss_mask_4: 0.9106  loss_dice_4: 1.151  loss_ce_5: 0.2301  loss_cate_5: 0  loss_mask_5: 0.9075  loss_dice_5: 1.178  loss_ce_6: 0.2349  loss_cate_6: 0  loss_mask_6: 0.9243  loss_dice_6: 1.161  loss_ce_7: 0.236  loss_cate_7: 0  loss_mask_7: 0.8994  loss_dice_7: 1.162  loss_ce_8: 0.217  loss_cate_8: 0  loss_mask_8: 0.8979  loss_dice_8: 1.117  time: 1.1026  data_time: 0.0122  lr: 9.6281e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:49:55 d2.utils.events]: \u001b[0m eta: 23:25:18  iter: 3319  total_loss: 24.13  loss_ce: 0.3597  loss_cate: 0.2072  loss_mask: 0.8597  loss_dice: 1.065  loss_ce_0: 0.5716  loss_cate_0: 0  loss_mask_0: 0.8243  loss_dice_0: 1.109  loss_ce_1: 0.4283  loss_cate_1: 0  loss_mask_1: 0.8741  loss_dice_1: 1.07  loss_ce_2: 0.3691  loss_cate_2: 0  loss_mask_2: 0.8824  loss_dice_2: 1.049  loss_ce_3: 0.3896  loss_cate_3: 0  loss_mask_3: 0.8668  loss_dice_3: 1.04  loss_ce_4: 0.4814  loss_cate_4: 0  loss_mask_4: 0.8485  loss_dice_4: 0.9864  loss_ce_5: 0.3495  loss_cate_5: 0  loss_mask_5: 0.8751  loss_dice_5: 1.046  loss_ce_6: 0.4492  loss_cate_6: 0  loss_mask_6: 0.8642  loss_dice_6: 1.012  loss_ce_7: 0.3847  loss_cate_7: 0  loss_mask_7: 0.8679  loss_dice_7: 1.052  loss_ce_8: 0.3853  loss_cate_8: 0  loss_mask_8: 0.8644  loss_dice_8: 1.05  time: 1.1025  data_time: 0.0142  lr: 9.6258e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:50:18 d2.utils.events]: \u001b[0m eta: 23:25:00  iter: 3339  total_loss: 26.02  loss_ce: 0.2631  loss_cate: 0.2191  loss_mask: 1.037  loss_dice: 1.22  loss_ce_0: 0.3803  loss_cate_0: 0  loss_mask_0: 1.039  loss_dice_0: 1.22  loss_ce_1: 0.3286  loss_cate_1: 0  loss_mask_1: 1.046  loss_dice_1: 1.191  loss_ce_2: 0.3303  loss_cate_2: 0  loss_mask_2: 1.047  loss_dice_2: 1.185  loss_ce_3: 0.2759  loss_cate_3: 0  loss_mask_3: 1.019  loss_dice_3: 1.233  loss_ce_4: 0.2686  loss_cate_4: 0  loss_mask_4: 0.991  loss_dice_4: 1.233  loss_ce_5: 0.2559  loss_cate_5: 0  loss_mask_5: 0.9943  loss_dice_5: 1.228  loss_ce_6: 0.2421  loss_cate_6: 0  loss_mask_6: 1.004  loss_dice_6: 1.221  loss_ce_7: 0.2506  loss_cate_7: 0  loss_mask_7: 0.9997  loss_dice_7: 1.224  loss_ce_8: 0.2994  loss_cate_8: 0  loss_mask_8: 1.019  loss_dice_8: 1.212  time: 1.1026  data_time: 0.0146  lr: 9.6236e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:50:40 d2.utils.events]: \u001b[0m eta: 23:24:21  iter: 3359  total_loss: 21.57  loss_ce: 0.2605  loss_cate: 0.196  loss_mask: 0.8515  loss_dice: 1.037  loss_ce_0: 0.37  loss_cate_0: 0  loss_mask_0: 0.846  loss_dice_0: 1.05  loss_ce_1: 0.2317  loss_cate_1: 0  loss_mask_1: 0.8574  loss_dice_1: 1.067  loss_ce_2: 0.2518  loss_cate_2: 0  loss_mask_2: 0.8159  loss_dice_2: 1.046  loss_ce_3: 0.2435  loss_cate_3: 0  loss_mask_3: 0.8672  loss_dice_3: 1.031  loss_ce_4: 0.2472  loss_cate_4: 0  loss_mask_4: 0.8506  loss_dice_4: 1.046  loss_ce_5: 0.255  loss_cate_5: 0  loss_mask_5: 0.8445  loss_dice_5: 1.013  loss_ce_6: 0.2494  loss_cate_6: 0  loss_mask_6: 0.8309  loss_dice_6: 1.011  loss_ce_7: 0.248  loss_cate_7: 0  loss_mask_7: 0.8453  loss_dice_7: 1.019  loss_ce_8: 0.2526  loss_cate_8: 0  loss_mask_8: 0.8518  loss_dice_8: 0.9985  time: 1.1025  data_time: 0.0140  lr: 9.6213e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:51:02 d2.utils.events]: \u001b[0m eta: 23:24:01  iter: 3379  total_loss: 23.14  loss_ce: 0.2485  loss_cate: 0.1667  loss_mask: 0.8388  loss_dice: 1.006  loss_ce_0: 0.3543  loss_cate_0: 0  loss_mask_0: 0.8909  loss_dice_0: 1.052  loss_ce_1: 0.2908  loss_cate_1: 0  loss_mask_1: 0.8658  loss_dice_1: 1.058  loss_ce_2: 0.2391  loss_cate_2: 0  loss_mask_2: 0.8845  loss_dice_2: 1.057  loss_ce_3: 0.2591  loss_cate_3: 0  loss_mask_3: 0.8345  loss_dice_3: 0.9784  loss_ce_4: 0.2332  loss_cate_4: 0  loss_mask_4: 0.834  loss_dice_4: 1.029  loss_ce_5: 0.1949  loss_cate_5: 0  loss_mask_5: 0.8976  loss_dice_5: 1.029  loss_ce_6: 0.2513  loss_cate_6: 0  loss_mask_6: 0.8862  loss_dice_6: 0.9872  loss_ce_7: 0.2405  loss_cate_7: 0  loss_mask_7: 0.8718  loss_dice_7: 0.9883  loss_ce_8: 0.2519  loss_cate_8: 0  loss_mask_8: 0.8522  loss_dice_8: 0.9906  time: 1.1025  data_time: 0.0122  lr: 9.619e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:51:25 d2.utils.events]: \u001b[0m eta: 23:23:16  iter: 3399  total_loss: 23.32  loss_ce: 0.2792  loss_cate: 0.183  loss_mask: 0.949  loss_dice: 1.155  loss_ce_0: 0.3883  loss_cate_0: 0  loss_mask_0: 0.9622  loss_dice_0: 1.196  loss_ce_1: 0.2628  loss_cate_1: 0  loss_mask_1: 0.9242  loss_dice_1: 1.143  loss_ce_2: 0.2793  loss_cate_2: 0  loss_mask_2: 0.9116  loss_dice_2: 1.149  loss_ce_3: 0.3089  loss_cate_3: 0  loss_mask_3: 0.9101  loss_dice_3: 1.06  loss_ce_4: 0.2991  loss_cate_4: 0  loss_mask_4: 0.9247  loss_dice_4: 1.118  loss_ce_5: 0.3141  loss_cate_5: 0  loss_mask_5: 0.9161  loss_dice_5: 1.095  loss_ce_6: 0.2521  loss_cate_6: 0  loss_mask_6: 0.9264  loss_dice_6: 1.177  loss_ce_7: 0.2573  loss_cate_7: 0  loss_mask_7: 0.9066  loss_dice_7: 1.151  loss_ce_8: 0.264  loss_cate_8: 0  loss_mask_8: 0.8939  loss_dice_8: 1.13  time: 1.1025  data_time: 0.0120  lr: 9.6168e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:51:47 d2.utils.events]: \u001b[0m eta: 23:22:49  iter: 3419  total_loss: 23.78  loss_ce: 0.2818  loss_cate: 0.2004  loss_mask: 0.8764  loss_dice: 1.008  loss_ce_0: 0.4064  loss_cate_0: 0  loss_mask_0: 0.9349  loss_dice_0: 1.182  loss_ce_1: 0.3856  loss_cate_1: 0  loss_mask_1: 0.8981  loss_dice_1: 1.083  loss_ce_2: 0.343  loss_cate_2: 0  loss_mask_2: 0.8897  loss_dice_2: 1.035  loss_ce_3: 0.3501  loss_cate_3: 0  loss_mask_3: 0.8319  loss_dice_3: 0.9783  loss_ce_4: 0.3239  loss_cate_4: 0  loss_mask_4: 0.8652  loss_dice_4: 0.9964  loss_ce_5: 0.313  loss_cate_5: 0  loss_mask_5: 0.8865  loss_dice_5: 1.019  loss_ce_6: 0.3353  loss_cate_6: 0  loss_mask_6: 0.8749  loss_dice_6: 0.9903  loss_ce_7: 0.3265  loss_cate_7: 0  loss_mask_7: 0.8625  loss_dice_7: 1.001  loss_ce_8: 0.3078  loss_cate_8: 0  loss_mask_8: 0.8887  loss_dice_8: 0.9923  time: 1.1024  data_time: 0.0124  lr: 9.6145e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:52:10 d2.utils.events]: \u001b[0m eta: 23:22:23  iter: 3439  total_loss: 25.19  loss_ce: 0.2694  loss_cate: 0.1887  loss_mask: 0.9588  loss_dice: 1.114  loss_ce_0: 0.3784  loss_cate_0: 0  loss_mask_0: 0.9493  loss_dice_0: 1.21  loss_ce_1: 0.2756  loss_cate_1: 0  loss_mask_1: 0.9207  loss_dice_1: 1.218  loss_ce_2: 0.2911  loss_cate_2: 0  loss_mask_2: 0.9396  loss_dice_2: 1.156  loss_ce_3: 0.3613  loss_cate_3: 0  loss_mask_3: 0.9327  loss_dice_3: 1.152  loss_ce_4: 0.3001  loss_cate_4: 0  loss_mask_4: 0.9524  loss_dice_4: 1.149  loss_ce_5: 0.315  loss_cate_5: 0  loss_mask_5: 0.953  loss_dice_5: 1.174  loss_ce_6: 0.3101  loss_cate_6: 0  loss_mask_6: 0.9024  loss_dice_6: 1.136  loss_ce_7: 0.2289  loss_cate_7: 0  loss_mask_7: 0.9184  loss_dice_7: 1.193  loss_ce_8: 0.2934  loss_cate_8: 0  loss_mask_8: 0.9104  loss_dice_8: 1.146  time: 1.1024  data_time: 0.0129  lr: 9.6123e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:52:32 d2.utils.events]: \u001b[0m eta: 23:21:56  iter: 3459  total_loss: 24.2  loss_ce: 0.2451  loss_cate: 0.2032  loss_mask: 0.9839  loss_dice: 1.124  loss_ce_0: 0.4089  loss_cate_0: 0  loss_mask_0: 0.9394  loss_dice_0: 1.147  loss_ce_1: 0.2453  loss_cate_1: 0  loss_mask_1: 0.9483  loss_dice_1: 1.117  loss_ce_2: 0.2505  loss_cate_2: 0  loss_mask_2: 0.9148  loss_dice_2: 1.068  loss_ce_3: 0.2635  loss_cate_3: 0  loss_mask_3: 0.9506  loss_dice_3: 1.097  loss_ce_4: 0.2531  loss_cate_4: 0  loss_mask_4: 0.9292  loss_dice_4: 1.114  loss_ce_5: 0.2811  loss_cate_5: 0  loss_mask_5: 0.9668  loss_dice_5: 1.091  loss_ce_6: 0.2706  loss_cate_6: 0  loss_mask_6: 0.9286  loss_dice_6: 1.093  loss_ce_7: 0.2935  loss_cate_7: 0  loss_mask_7: 0.9207  loss_dice_7: 1.092  loss_ce_8: 0.2487  loss_cate_8: 0  loss_mask_8: 0.9247  loss_dice_8: 1.07  time: 1.1024  data_time: 0.0130  lr: 9.61e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:52:56 d2.utils.events]: \u001b[0m eta: 23:21:33  iter: 3479  total_loss: 22.85  loss_ce: 0.2867  loss_cate: 0.1897  loss_mask: 0.8971  loss_dice: 1.048  loss_ce_0: 0.4794  loss_cate_0: 0  loss_mask_0: 0.9601  loss_dice_0: 1.115  loss_ce_1: 0.3988  loss_cate_1: 0  loss_mask_1: 0.874  loss_dice_1: 1.072  loss_ce_2: 0.3674  loss_cate_2: 0  loss_mask_2: 0.9024  loss_dice_2: 1.048  loss_ce_3: 0.3707  loss_cate_3: 0  loss_mask_3: 0.9014  loss_dice_3: 1.044  loss_ce_4: 0.3498  loss_cate_4: 0  loss_mask_4: 0.9179  loss_dice_4: 1.04  loss_ce_5: 0.4024  loss_cate_5: 0  loss_mask_5: 0.9138  loss_dice_5: 1.058  loss_ce_6: 0.3462  loss_cate_6: 0  loss_mask_6: 0.9103  loss_dice_6: 1.064  loss_ce_7: 0.3186  loss_cate_7: 0  loss_mask_7: 0.9172  loss_dice_7: 1.072  loss_ce_8: 0.3048  loss_cate_8: 0  loss_mask_8: 0.8933  loss_dice_8: 1.112  time: 1.1024  data_time: 0.0125  lr: 9.6077e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:53:18 d2.utils.events]: \u001b[0m eta: 23:20:51  iter: 3499  total_loss: 22.43  loss_ce: 0.2308  loss_cate: 0.1855  loss_mask: 0.918  loss_dice: 1.067  loss_ce_0: 0.2766  loss_cate_0: 0  loss_mask_0: 0.9765  loss_dice_0: 1.164  loss_ce_1: 0.2413  loss_cate_1: 0  loss_mask_1: 0.9535  loss_dice_1: 1.115  loss_ce_2: 0.2919  loss_cate_2: 0  loss_mask_2: 0.9426  loss_dice_2: 1.108  loss_ce_3: 0.2108  loss_cate_3: 0  loss_mask_3: 0.921  loss_dice_3: 1.106  loss_ce_4: 0.1964  loss_cate_4: 0  loss_mask_4: 0.9408  loss_dice_4: 1.113  loss_ce_5: 0.2435  loss_cate_5: 0  loss_mask_5: 0.8809  loss_dice_5: 1.099  loss_ce_6: 0.2236  loss_cate_6: 0  loss_mask_6: 0.8767  loss_dice_6: 1.114  loss_ce_7: 0.2537  loss_cate_7: 0  loss_mask_7: 0.886  loss_dice_7: 1.077  loss_ce_8: 0.2194  loss_cate_8: 0  loss_mask_8: 0.8999  loss_dice_8: 1.112  time: 1.1024  data_time: 0.0124  lr: 9.6055e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:53:40 d2.utils.events]: \u001b[0m eta: 23:20:19  iter: 3519  total_loss: 20.55  loss_ce: 0.3312  loss_cate: 0.1729  loss_mask: 0.8461  loss_dice: 0.9711  loss_ce_0: 0.4172  loss_cate_0: 0  loss_mask_0: 0.8283  loss_dice_0: 1.101  loss_ce_1: 0.349  loss_cate_1: 0  loss_mask_1: 0.7934  loss_dice_1: 0.9666  loss_ce_2: 0.3362  loss_cate_2: 0  loss_mask_2: 0.7981  loss_dice_2: 0.9451  loss_ce_3: 0.3483  loss_cate_3: 0  loss_mask_3: 0.8055  loss_dice_3: 0.9621  loss_ce_4: 0.2882  loss_cate_4: 0  loss_mask_4: 0.7934  loss_dice_4: 0.9779  loss_ce_5: 0.3003  loss_cate_5: 0  loss_mask_5: 0.7843  loss_dice_5: 1  loss_ce_6: 0.3084  loss_cate_6: 0  loss_mask_6: 0.8507  loss_dice_6: 0.9425  loss_ce_7: 0.3054  loss_cate_7: 0  loss_mask_7: 0.852  loss_dice_7: 0.9796  loss_ce_8: 0.318  loss_cate_8: 0  loss_mask_8: 0.8658  loss_dice_8: 0.9741  time: 1.1024  data_time: 0.0128  lr: 9.6032e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:54:03 d2.utils.events]: \u001b[0m eta: 23:19:49  iter: 3539  total_loss: 22.25  loss_ce: 0.2249  loss_cate: 0.1668  loss_mask: 0.876  loss_dice: 1.104  loss_ce_0: 0.3868  loss_cate_0: 0  loss_mask_0: 0.896  loss_dice_0: 1.163  loss_ce_1: 0.2374  loss_cate_1: 0  loss_mask_1: 0.9124  loss_dice_1: 1.209  loss_ce_2: 0.265  loss_cate_2: 0  loss_mask_2: 0.8773  loss_dice_2: 1.049  loss_ce_3: 0.1988  loss_cate_3: 0  loss_mask_3: 0.8684  loss_dice_3: 1.058  loss_ce_4: 0.2105  loss_cate_4: 0  loss_mask_4: 0.9012  loss_dice_4: 1.106  loss_ce_5: 0.2496  loss_cate_5: 0  loss_mask_5: 0.8807  loss_dice_5: 1.126  loss_ce_6: 0.2299  loss_cate_6: 0  loss_mask_6: 0.8479  loss_dice_6: 1.083  loss_ce_7: 0.197  loss_cate_7: 0  loss_mask_7: 0.8687  loss_dice_7: 1.094  loss_ce_8: 0.2016  loss_cate_8: 0  loss_mask_8: 0.8669  loss_dice_8: 1.097  time: 1.1023  data_time: 0.0125  lr: 9.601e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:54:26 d2.utils.events]: \u001b[0m eta: 23:19:24  iter: 3559  total_loss: 21.73  loss_ce: 0.2495  loss_cate: 0.1978  loss_mask: 0.8714  loss_dice: 0.9693  loss_ce_0: 0.4337  loss_cate_0: 0  loss_mask_0: 0.8496  loss_dice_0: 1.023  loss_ce_1: 0.2733  loss_cate_1: 0  loss_mask_1: 0.8449  loss_dice_1: 0.9835  loss_ce_2: 0.2675  loss_cate_2: 0  loss_mask_2: 0.843  loss_dice_2: 0.9855  loss_ce_3: 0.2657  loss_cate_3: 0  loss_mask_3: 0.8724  loss_dice_3: 1.016  loss_ce_4: 0.2965  loss_cate_4: 0  loss_mask_4: 0.8609  loss_dice_4: 0.9565  loss_ce_5: 0.2637  loss_cate_5: 0  loss_mask_5: 0.8617  loss_dice_5: 0.9846  loss_ce_6: 0.247  loss_cate_6: 0  loss_mask_6: 0.8648  loss_dice_6: 0.9702  loss_ce_7: 0.2598  loss_cate_7: 0  loss_mask_7: 0.857  loss_dice_7: 0.9847  loss_ce_8: 0.2456  loss_cate_8: 0  loss_mask_8: 0.8692  loss_dice_8: 0.9679  time: 1.1023  data_time: 0.0127  lr: 9.5987e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:54:48 d2.utils.events]: \u001b[0m eta: 23:18:55  iter: 3579  total_loss: 21.4  loss_ce: 0.2085  loss_cate: 0.1824  loss_mask: 0.8898  loss_dice: 0.9963  loss_ce_0: 0.3814  loss_cate_0: 0  loss_mask_0: 0.9607  loss_dice_0: 1.025  loss_ce_1: 0.2349  loss_cate_1: 0  loss_mask_1: 0.8875  loss_dice_1: 0.9726  loss_ce_2: 0.2605  loss_cate_2: 0  loss_mask_2: 0.9748  loss_dice_2: 0.9203  loss_ce_3: 0.2118  loss_cate_3: 0  loss_mask_3: 0.9663  loss_dice_3: 0.9958  loss_ce_4: 0.2412  loss_cate_4: 0  loss_mask_4: 0.9237  loss_dice_4: 0.9612  loss_ce_5: 0.2448  loss_cate_5: 0  loss_mask_5: 0.8974  loss_dice_5: 0.976  loss_ce_6: 0.189  loss_cate_6: 0  loss_mask_6: 0.9311  loss_dice_6: 0.9921  loss_ce_7: 0.1948  loss_cate_7: 0  loss_mask_7: 0.9113  loss_dice_7: 0.9935  loss_ce_8: 0.2153  loss_cate_8: 0  loss_mask_8: 0.9151  loss_dice_8: 0.9671  time: 1.1023  data_time: 0.0141  lr: 9.5964e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:55:14 d2.utils.events]: \u001b[0m eta: 23:18:32  iter: 3599  total_loss: 23.8  loss_ce: 0.2972  loss_cate: 0.1924  loss_mask: 0.9424  loss_dice: 1.073  loss_ce_0: 0.3814  loss_cate_0: 0  loss_mask_0: 0.9544  loss_dice_0: 1.099  loss_ce_1: 0.3964  loss_cate_1: 0  loss_mask_1: 0.8959  loss_dice_1: 1.126  loss_ce_2: 0.3571  loss_cate_2: 0  loss_mask_2: 0.9167  loss_dice_2: 1.064  loss_ce_3: 0.3041  loss_cate_3: 0  loss_mask_3: 0.9293  loss_dice_3: 1.097  loss_ce_4: 0.3074  loss_cate_4: 0  loss_mask_4: 0.9464  loss_dice_4: 1.071  loss_ce_5: 0.3097  loss_cate_5: 0  loss_mask_5: 0.9069  loss_dice_5: 1.105  loss_ce_6: 0.2934  loss_cate_6: 0  loss_mask_6: 0.936  loss_dice_6: 1.094  loss_ce_7: 0.2651  loss_cate_7: 0  loss_mask_7: 0.945  loss_dice_7: 1.114  loss_ce_8: 0.2719  loss_cate_8: 0  loss_mask_8: 0.9617  loss_dice_8: 1.116  time: 1.1022  data_time: 0.0128  lr: 9.5942e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:55:36 d2.utils.events]: \u001b[0m eta: 23:18:10  iter: 3619  total_loss: 21.89  loss_ce: 0.2756  loss_cate: 0.1939  loss_mask: 0.7733  loss_dice: 1.07  loss_ce_0: 0.3233  loss_cate_0: 0  loss_mask_0: 0.8092  loss_dice_0: 1.072  loss_ce_1: 0.2877  loss_cate_1: 0  loss_mask_1: 0.7699  loss_dice_1: 1.071  loss_ce_2: 0.3005  loss_cate_2: 0  loss_mask_2: 0.7623  loss_dice_2: 1.027  loss_ce_3: 0.2859  loss_cate_3: 0  loss_mask_3: 0.7433  loss_dice_3: 1.081  loss_ce_4: 0.2958  loss_cate_4: 0  loss_mask_4: 0.7614  loss_dice_4: 1.086  loss_ce_5: 0.2746  loss_cate_5: 0  loss_mask_5: 0.7587  loss_dice_5: 1.082  loss_ce_6: 0.2871  loss_cate_6: 0  loss_mask_6: 0.7685  loss_dice_6: 1.054  loss_ce_7: 0.2878  loss_cate_7: 0  loss_mask_7: 0.7626  loss_dice_7: 1.054  loss_ce_8: 0.329  loss_cate_8: 0  loss_mask_8: 0.7656  loss_dice_8: 1.056  time: 1.1022  data_time: 0.0125  lr: 9.5919e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:55:58 d2.utils.events]: \u001b[0m eta: 23:17:36  iter: 3639  total_loss: 23.05  loss_ce: 0.2412  loss_cate: 0.1835  loss_mask: 0.9688  loss_dice: 1.12  loss_ce_0: 0.361  loss_cate_0: 0  loss_mask_0: 0.9694  loss_dice_0: 1.209  loss_ce_1: 0.2472  loss_cate_1: 0  loss_mask_1: 0.9552  loss_dice_1: 1.162  loss_ce_2: 0.2199  loss_cate_2: 0  loss_mask_2: 1.014  loss_dice_2: 1.137  loss_ce_3: 0.2109  loss_cate_3: 0  loss_mask_3: 0.9815  loss_dice_3: 1.161  loss_ce_4: 0.238  loss_cate_4: 0  loss_mask_4: 0.9685  loss_dice_4: 1.107  loss_ce_5: 0.2013  loss_cate_5: 0  loss_mask_5: 0.9894  loss_dice_5: 1.084  loss_ce_6: 0.2179  loss_cate_6: 0  loss_mask_6: 0.9871  loss_dice_6: 1.087  loss_ce_7: 0.2637  loss_cate_7: 0  loss_mask_7: 0.996  loss_dice_7: 1.068  loss_ce_8: 0.2481  loss_cate_8: 0  loss_mask_8: 0.9753  loss_dice_8: 1.11  time: 1.1022  data_time: 0.0127  lr: 9.5897e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:56:21 d2.utils.events]: \u001b[0m eta: 23:17:10  iter: 3659  total_loss: 21.79  loss_ce: 0.2637  loss_cate: 0.1898  loss_mask: 0.8501  loss_dice: 1.001  loss_ce_0: 0.3788  loss_cate_0: 0  loss_mask_0: 0.9161  loss_dice_0: 1.064  loss_ce_1: 0.3306  loss_cate_1: 0  loss_mask_1: 0.8777  loss_dice_1: 1.037  loss_ce_2: 0.3107  loss_cate_2: 0  loss_mask_2: 0.8622  loss_dice_2: 1.024  loss_ce_3: 0.2948  loss_cate_3: 0  loss_mask_3: 0.8828  loss_dice_3: 1.055  loss_ce_4: 0.2697  loss_cate_4: 0  loss_mask_4: 0.8848  loss_dice_4: 1.039  loss_ce_5: 0.3037  loss_cate_5: 0  loss_mask_5: 0.8727  loss_dice_5: 1.014  loss_ce_6: 0.2974  loss_cate_6: 0  loss_mask_6: 0.8484  loss_dice_6: 1.006  loss_ce_7: 0.2461  loss_cate_7: 0  loss_mask_7: 0.8477  loss_dice_7: 0.9934  loss_ce_8: 0.2763  loss_cate_8: 0  loss_mask_8: 0.8549  loss_dice_8: 0.9823  time: 1.1021  data_time: 0.0133  lr: 9.5874e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:56:43 d2.utils.events]: \u001b[0m eta: 23:16:47  iter: 3679  total_loss: 24.49  loss_ce: 0.1938  loss_cate: 0.1999  loss_mask: 0.9748  loss_dice: 1.143  loss_ce_0: 0.3655  loss_cate_0: 0  loss_mask_0: 0.9912  loss_dice_0: 1.265  loss_ce_1: 0.2358  loss_cate_1: 0  loss_mask_1: 0.9839  loss_dice_1: 1.134  loss_ce_2: 0.2993  loss_cate_2: 0  loss_mask_2: 0.9414  loss_dice_2: 1.137  loss_ce_3: 0.2306  loss_cate_3: 0  loss_mask_3: 0.9508  loss_dice_3: 1.162  loss_ce_4: 0.1847  loss_cate_4: 0  loss_mask_4: 0.9742  loss_dice_4: 1.159  loss_ce_5: 0.2236  loss_cate_5: 0  loss_mask_5: 0.995  loss_dice_5: 1.136  loss_ce_6: 0.2385  loss_cate_6: 0  loss_mask_6: 0.964  loss_dice_6: 1.122  loss_ce_7: 0.1963  loss_cate_7: 0  loss_mask_7: 0.9778  loss_dice_7: 1.196  loss_ce_8: 0.1601  loss_cate_8: 0  loss_mask_8: 1.012  loss_dice_8: 1.161  time: 1.1021  data_time: 0.0135  lr: 9.5851e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:57:05 d2.utils.events]: \u001b[0m eta: 23:16:00  iter: 3699  total_loss: 24.1  loss_ce: 0.2866  loss_cate: 0.2124  loss_mask: 0.832  loss_dice: 1.189  loss_ce_0: 0.4238  loss_cate_0: 0  loss_mask_0: 0.8851  loss_dice_0: 1.256  loss_ce_1: 0.3515  loss_cate_1: 0  loss_mask_1: 0.789  loss_dice_1: 1.206  loss_ce_2: 0.379  loss_cate_2: 0  loss_mask_2: 0.7897  loss_dice_2: 1.231  loss_ce_3: 0.3264  loss_cate_3: 0  loss_mask_3: 0.8162  loss_dice_3: 1.167  loss_ce_4: 0.3001  loss_cate_4: 0  loss_mask_4: 0.8209  loss_dice_4: 1.188  loss_ce_5: 0.2854  loss_cate_5: 0  loss_mask_5: 0.8145  loss_dice_5: 1.175  loss_ce_6: 0.3045  loss_cate_6: 0  loss_mask_6: 0.8079  loss_dice_6: 1.151  loss_ce_7: 0.3487  loss_cate_7: 0  loss_mask_7: 0.8246  loss_dice_7: 1.17  loss_ce_8: 0.3575  loss_cate_8: 0  loss_mask_8: 0.8172  loss_dice_8: 1.169  time: 1.1020  data_time: 0.0124  lr: 9.5829e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:57:28 d2.utils.events]: \u001b[0m eta: 23:15:28  iter: 3719  total_loss: 23.39  loss_ce: 0.2576  loss_cate: 0.1722  loss_mask: 0.8445  loss_dice: 1.072  loss_ce_0: 0.3271  loss_cate_0: 0  loss_mask_0: 0.8787  loss_dice_0: 1.111  loss_ce_1: 0.3231  loss_cate_1: 0  loss_mask_1: 0.8258  loss_dice_1: 1.076  loss_ce_2: 0.318  loss_cate_2: 0  loss_mask_2: 0.8262  loss_dice_2: 1.092  loss_ce_3: 0.2339  loss_cate_3: 0  loss_mask_3: 0.8082  loss_dice_3: 1.065  loss_ce_4: 0.2716  loss_cate_4: 0  loss_mask_4: 0.8289  loss_dice_4: 1.135  loss_ce_5: 0.2736  loss_cate_5: 0  loss_mask_5: 0.8277  loss_dice_5: 1.111  loss_ce_6: 0.2475  loss_cate_6: 0  loss_mask_6: 0.8285  loss_dice_6: 1.082  loss_ce_7: 0.2819  loss_cate_7: 0  loss_mask_7: 0.8286  loss_dice_7: 1.086  loss_ce_8: 0.2636  loss_cate_8: 0  loss_mask_8: 0.8428  loss_dice_8: 1.103  time: 1.1020  data_time: 0.0126  lr: 9.5806e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:57:50 d2.utils.events]: \u001b[0m eta: 23:14:58  iter: 3739  total_loss: 24.59  loss_ce: 0.309  loss_cate: 0.1909  loss_mask: 0.9726  loss_dice: 1.096  loss_ce_0: 0.3862  loss_cate_0: 0  loss_mask_0: 1.035  loss_dice_0: 1.16  loss_ce_1: 0.3697  loss_cate_1: 0  loss_mask_1: 1.009  loss_dice_1: 1.151  loss_ce_2: 0.3686  loss_cate_2: 0  loss_mask_2: 0.978  loss_dice_2: 1.153  loss_ce_3: 0.2945  loss_cate_3: 0  loss_mask_3: 0.9919  loss_dice_3: 1.127  loss_ce_4: 0.3193  loss_cate_4: 0  loss_mask_4: 0.972  loss_dice_4: 1.106  loss_ce_5: 0.3222  loss_cate_5: 0  loss_mask_5: 0.9913  loss_dice_5: 1.157  loss_ce_6: 0.271  loss_cate_6: 0  loss_mask_6: 0.98  loss_dice_6: 1.14  loss_ce_7: 0.3025  loss_cate_7: 0  loss_mask_7: 0.9703  loss_dice_7: 1.125  loss_ce_8: 0.2925  loss_cate_8: 0  loss_mask_8: 0.9819  loss_dice_8: 1.12  time: 1.1020  data_time: 0.0127  lr: 9.5784e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:58:13 d2.utils.events]: \u001b[0m eta: 23:14:36  iter: 3759  total_loss: 25.79  loss_ce: 0.3346  loss_cate: 0.2045  loss_mask: 0.9528  loss_dice: 1.236  loss_ce_0: 0.4801  loss_cate_0: 0  loss_mask_0: 0.9636  loss_dice_0: 1.246  loss_ce_1: 0.4487  loss_cate_1: 0  loss_mask_1: 0.9702  loss_dice_1: 1.185  loss_ce_2: 0.3802  loss_cate_2: 0  loss_mask_2: 0.9689  loss_dice_2: 1.152  loss_ce_3: 0.3857  loss_cate_3: 0  loss_mask_3: 0.9845  loss_dice_3: 1.098  loss_ce_4: 0.313  loss_cate_4: 0  loss_mask_4: 0.9978  loss_dice_4: 1.132  loss_ce_5: 0.303  loss_cate_5: 0  loss_mask_5: 0.986  loss_dice_5: 1.225  loss_ce_6: 0.3275  loss_cate_6: 0  loss_mask_6: 0.9316  loss_dice_6: 1.18  loss_ce_7: 0.3367  loss_cate_7: 0  loss_mask_7: 0.927  loss_dice_7: 1.17  loss_ce_8: 0.3699  loss_cate_8: 0  loss_mask_8: 1  loss_dice_8: 1.216  time: 1.1019  data_time: 0.0131  lr: 9.5761e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:58:36 d2.utils.events]: \u001b[0m eta: 23:14:21  iter: 3779  total_loss: 22.9  loss_ce: 0.2658  loss_cate: 0.1841  loss_mask: 0.8716  loss_dice: 1.054  loss_ce_0: 0.3532  loss_cate_0: 0  loss_mask_0: 0.8898  loss_dice_0: 1.158  loss_ce_1: 0.2778  loss_cate_1: 0  loss_mask_1: 0.8788  loss_dice_1: 1.057  loss_ce_2: 0.2882  loss_cate_2: 0  loss_mask_2: 0.8621  loss_dice_2: 1.071  loss_ce_3: 0.3374  loss_cate_3: 0  loss_mask_3: 0.8963  loss_dice_3: 1.025  loss_ce_4: 0.2839  loss_cate_4: 0  loss_mask_4: 0.8642  loss_dice_4: 1.063  loss_ce_5: 0.252  loss_cate_5: 0  loss_mask_5: 0.8858  loss_dice_5: 1.06  loss_ce_6: 0.2263  loss_cate_6: 0  loss_mask_6: 0.9036  loss_dice_6: 1.056  loss_ce_7: 0.2708  loss_cate_7: 0  loss_mask_7: 0.8767  loss_dice_7: 1.044  loss_ce_8: 0.272  loss_cate_8: 0  loss_mask_8: 0.886  loss_dice_8: 1.057  time: 1.1019  data_time: 0.0142  lr: 9.5738e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:58:58 d2.utils.events]: \u001b[0m eta: 23:13:55  iter: 3799  total_loss: 24.35  loss_ce: 0.263  loss_cate: 0.1688  loss_mask: 0.8807  loss_dice: 1.163  loss_ce_0: 0.4151  loss_cate_0: 0  loss_mask_0: 0.9189  loss_dice_0: 1.183  loss_ce_1: 0.3032  loss_cate_1: 0  loss_mask_1: 0.8955  loss_dice_1: 1.092  loss_ce_2: 0.2889  loss_cate_2: 0  loss_mask_2: 0.8698  loss_dice_2: 1.109  loss_ce_3: 0.2415  loss_cate_3: 0  loss_mask_3: 0.9008  loss_dice_3: 1.143  loss_ce_4: 0.2503  loss_cate_4: 0  loss_mask_4: 0.8892  loss_dice_4: 1.134  loss_ce_5: 0.2913  loss_cate_5: 0  loss_mask_5: 0.9011  loss_dice_5: 1.153  loss_ce_6: 0.2516  loss_cate_6: 0  loss_mask_6: 0.88  loss_dice_6: 1.169  loss_ce_7: 0.2566  loss_cate_7: 0  loss_mask_7: 0.908  loss_dice_7: 1.175  loss_ce_8: 0.2565  loss_cate_8: 0  loss_mask_8: 0.8999  loss_dice_8: 1.138  time: 1.1019  data_time: 0.0128  lr: 9.5716e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:59:20 d2.utils.events]: \u001b[0m eta: 23:13:38  iter: 3819  total_loss: 24.77  loss_ce: 0.378  loss_cate: 0.2242  loss_mask: 0.9382  loss_dice: 1.007  loss_ce_0: 0.4456  loss_cate_0: 0  loss_mask_0: 0.9958  loss_dice_0: 1.085  loss_ce_1: 0.4743  loss_cate_1: 0  loss_mask_1: 0.9273  loss_dice_1: 1.038  loss_ce_2: 0.3698  loss_cate_2: 0  loss_mask_2: 0.9497  loss_dice_2: 1.051  loss_ce_3: 0.2982  loss_cate_3: 0  loss_mask_3: 0.9005  loss_dice_3: 1.047  loss_ce_4: 0.4033  loss_cate_4: 0  loss_mask_4: 0.9071  loss_dice_4: 1.035  loss_ce_5: 0.3905  loss_cate_5: 0  loss_mask_5: 0.9148  loss_dice_5: 1.047  loss_ce_6: 0.3926  loss_cate_6: 0  loss_mask_6: 0.8555  loss_dice_6: 1.035  loss_ce_7: 0.3357  loss_cate_7: 0  loss_mask_7: 0.8801  loss_dice_7: 1.027  loss_ce_8: 0.3637  loss_cate_8: 0  loss_mask_8: 0.908  loss_dice_8: 1.033  time: 1.1019  data_time: 0.0128  lr: 9.5693e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 12:59:43 d2.utils.events]: \u001b[0m eta: 23:13:08  iter: 3839  total_loss: 23.02  loss_ce: 0.216  loss_cate: 0.2182  loss_mask: 0.8998  loss_dice: 1.073  loss_ce_0: 0.3261  loss_cate_0: 0  loss_mask_0: 0.9377  loss_dice_0: 1.191  loss_ce_1: 0.2674  loss_cate_1: 0  loss_mask_1: 0.9107  loss_dice_1: 1.093  loss_ce_2: 0.2562  loss_cate_2: 0  loss_mask_2: 0.8897  loss_dice_2: 1.073  loss_ce_3: 0.1851  loss_cate_3: 0  loss_mask_3: 0.8984  loss_dice_3: 1.043  loss_ce_4: 0.2128  loss_cate_4: 0  loss_mask_4: 0.9008  loss_dice_4: 1.118  loss_ce_5: 0.2525  loss_cate_5: 0  loss_mask_5: 0.899  loss_dice_5: 1.099  loss_ce_6: 0.2509  loss_cate_6: 0  loss_mask_6: 0.8889  loss_dice_6: 1.094  loss_ce_7: 0.2515  loss_cate_7: 0  loss_mask_7: 0.8885  loss_dice_7: 1.097  loss_ce_8: 0.2915  loss_cate_8: 0  loss_mask_8: 0.9136  loss_dice_8: 1.074  time: 1.1018  data_time: 0.0125  lr: 9.5671e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:00:05 d2.utils.events]: \u001b[0m eta: 23:12:40  iter: 3859  total_loss: 23.19  loss_ce: 0.2678  loss_cate: 0.1927  loss_mask: 0.8547  loss_dice: 1.076  loss_ce_0: 0.4811  loss_cate_0: 0  loss_mask_0: 0.9101  loss_dice_0: 1.136  loss_ce_1: 0.3063  loss_cate_1: 0  loss_mask_1: 0.8576  loss_dice_1: 1.132  loss_ce_2: 0.2786  loss_cate_2: 0  loss_mask_2: 0.9338  loss_dice_2: 1.119  loss_ce_3: 0.2879  loss_cate_3: 0  loss_mask_3: 0.9019  loss_dice_3: 1.112  loss_ce_4: 0.3207  loss_cate_4: 0  loss_mask_4: 0.9113  loss_dice_4: 1.094  loss_ce_5: 0.3266  loss_cate_5: 0  loss_mask_5: 0.9263  loss_dice_5: 1.095  loss_ce_6: 0.3084  loss_cate_6: 0  loss_mask_6: 0.8949  loss_dice_6: 1.081  loss_ce_7: 0.2584  loss_cate_7: 0  loss_mask_7: 0.8867  loss_dice_7: 1.088  loss_ce_8: 0.272  loss_cate_8: 0  loss_mask_8: 0.8982  loss_dice_8: 1.073  time: 1.1018  data_time: 0.0133  lr: 9.5648e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:00:27 d2.utils.events]: \u001b[0m eta: 23:12:20  iter: 3879  total_loss: 23.93  loss_ce: 0.1809  loss_cate: 0.1846  loss_mask: 0.9334  loss_dice: 1.08  loss_ce_0: 0.3652  loss_cate_0: 0  loss_mask_0: 0.9772  loss_dice_0: 1.108  loss_ce_1: 0.2276  loss_cate_1: 0  loss_mask_1: 0.9447  loss_dice_1: 1.085  loss_ce_2: 0.2826  loss_cate_2: 0  loss_mask_2: 0.957  loss_dice_2: 1.071  loss_ce_3: 0.2151  loss_cate_3: 0  loss_mask_3: 0.9699  loss_dice_3: 1.025  loss_ce_4: 0.2403  loss_cate_4: 0  loss_mask_4: 0.9508  loss_dice_4: 1.068  loss_ce_5: 0.2696  loss_cate_5: 0  loss_mask_5: 0.9385  loss_dice_5: 1.063  loss_ce_6: 0.2094  loss_cate_6: 0  loss_mask_6: 0.9276  loss_dice_6: 1.075  loss_ce_7: 0.2137  loss_cate_7: 0  loss_mask_7: 0.9178  loss_dice_7: 1.056  loss_ce_8: 0.1936  loss_cate_8: 0  loss_mask_8: 0.9406  loss_dice_8: 1.042  time: 1.1018  data_time: 0.0133  lr: 9.5625e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:00:51 d2.utils.events]: \u001b[0m eta: 23:12:02  iter: 3899  total_loss: 25.43  loss_ce: 0.3104  loss_cate: 0.1913  loss_mask: 1.062  loss_dice: 1.12  loss_ce_0: 0.3989  loss_cate_0: 0  loss_mask_0: 1.012  loss_dice_0: 1.162  loss_ce_1: 0.2915  loss_cate_1: 0  loss_mask_1: 1.064  loss_dice_1: 1.155  loss_ce_2: 0.2782  loss_cate_2: 0  loss_mask_2: 1.015  loss_dice_2: 1.152  loss_ce_3: 0.2372  loss_cate_3: 0  loss_mask_3: 1.033  loss_dice_3: 1.159  loss_ce_4: 0.2301  loss_cate_4: 0  loss_mask_4: 1.033  loss_dice_4: 1.138  loss_ce_5: 0.3323  loss_cate_5: 0  loss_mask_5: 1.01  loss_dice_5: 1.116  loss_ce_6: 0.3079  loss_cate_6: 0  loss_mask_6: 0.9935  loss_dice_6: 1.108  loss_ce_7: 0.2984  loss_cate_7: 0  loss_mask_7: 1.026  loss_dice_7: 1.077  loss_ce_8: 0.3153  loss_cate_8: 0  loss_mask_8: 1.054  loss_dice_8: 1.116  time: 1.1018  data_time: 0.0128  lr: 9.5603e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:01:13 d2.utils.events]: \u001b[0m eta: 23:11:27  iter: 3919  total_loss: 23.97  loss_ce: 0.2606  loss_cate: 0.1784  loss_mask: 0.9342  loss_dice: 1.002  loss_ce_0: 0.3292  loss_cate_0: 0  loss_mask_0: 0.9632  loss_dice_0: 1.106  loss_ce_1: 0.2243  loss_cate_1: 0  loss_mask_1: 0.9396  loss_dice_1: 1.133  loss_ce_2: 0.251  loss_cate_2: 0  loss_mask_2: 0.9303  loss_dice_2: 1.141  loss_ce_3: 0.2458  loss_cate_3: 0  loss_mask_3: 0.9192  loss_dice_3: 1.094  loss_ce_4: 0.2708  loss_cate_4: 0  loss_mask_4: 0.9162  loss_dice_4: 1.101  loss_ce_5: 0.2594  loss_cate_5: 0  loss_mask_5: 0.9053  loss_dice_5: 1.067  loss_ce_6: 0.3143  loss_cate_6: 0  loss_mask_6: 0.9107  loss_dice_6: 1.025  loss_ce_7: 0.2789  loss_cate_7: 0  loss_mask_7: 0.909  loss_dice_7: 0.9939  loss_ce_8: 0.2809  loss_cate_8: 0  loss_mask_8: 0.923  loss_dice_8: 1.026  time: 1.1018  data_time: 0.0120  lr: 9.558e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:01:35 d2.utils.events]: \u001b[0m eta: 23:10:59  iter: 3939  total_loss: 22.03  loss_ce: 0.2456  loss_cate: 0.1775  loss_mask: 0.8245  loss_dice: 1.08  loss_ce_0: 0.457  loss_cate_0: 0  loss_mask_0: 0.8187  loss_dice_0: 1.126  loss_ce_1: 0.3175  loss_cate_1: 0  loss_mask_1: 0.8256  loss_dice_1: 1.033  loss_ce_2: 0.2385  loss_cate_2: 0  loss_mask_2: 0.84  loss_dice_2: 1.01  loss_ce_3: 0.3111  loss_cate_3: 0  loss_mask_3: 0.845  loss_dice_3: 0.9798  loss_ce_4: 0.3004  loss_cate_4: 0  loss_mask_4: 0.8297  loss_dice_4: 1.003  loss_ce_5: 0.3074  loss_cate_5: 0  loss_mask_5: 0.8288  loss_dice_5: 1.005  loss_ce_6: 0.2399  loss_cate_6: 0  loss_mask_6: 0.8199  loss_dice_6: 1.078  loss_ce_7: 0.2349  loss_cate_7: 0  loss_mask_7: 0.8161  loss_dice_7: 1.055  loss_ce_8: 0.2083  loss_cate_8: 0  loss_mask_8: 0.8303  loss_dice_8: 1.052  time: 1.1018  data_time: 0.0129  lr: 9.5558e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:01:57 d2.utils.events]: \u001b[0m eta: 23:10:33  iter: 3959  total_loss: 20.94  loss_ce: 0.3423  loss_cate: 0.1831  loss_mask: 0.7261  loss_dice: 0.9348  loss_ce_0: 0.3992  loss_cate_0: 0  loss_mask_0: 0.7972  loss_dice_0: 1.061  loss_ce_1: 0.3495  loss_cate_1: 0  loss_mask_1: 0.7407  loss_dice_1: 1.036  loss_ce_2: 0.3678  loss_cate_2: 0  loss_mask_2: 0.7329  loss_dice_2: 0.9644  loss_ce_3: 0.2676  loss_cate_3: 0  loss_mask_3: 0.7165  loss_dice_3: 0.9645  loss_ce_4: 0.2403  loss_cate_4: 0  loss_mask_4: 0.7029  loss_dice_4: 0.9166  loss_ce_5: 0.2796  loss_cate_5: 0  loss_mask_5: 0.6961  loss_dice_5: 0.9188  loss_ce_6: 0.3516  loss_cate_6: 0  loss_mask_6: 0.6995  loss_dice_6: 0.9518  loss_ce_7: 0.3188  loss_cate_7: 0  loss_mask_7: 0.71  loss_dice_7: 0.936  loss_ce_8: 0.2871  loss_cate_8: 0  loss_mask_8: 0.7081  loss_dice_8: 0.9897  time: 1.1018  data_time: 0.0129  lr: 9.5535e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:02:20 d2.utils.events]: \u001b[0m eta: 23:10:10  iter: 3979  total_loss: 24.83  loss_ce: 0.3232  loss_cate: 0.1831  loss_mask: 1.039  loss_dice: 1.054  loss_ce_0: 0.3895  loss_cate_0: 0  loss_mask_0: 1.015  loss_dice_0: 1.162  loss_ce_1: 0.2875  loss_cate_1: 0  loss_mask_1: 1.022  loss_dice_1: 1.113  loss_ce_2: 0.3261  loss_cate_2: 0  loss_mask_2: 0.9696  loss_dice_2: 1.085  loss_ce_3: 0.2725  loss_cate_3: 0  loss_mask_3: 1.01  loss_dice_3: 1.07  loss_ce_4: 0.2997  loss_cate_4: 0  loss_mask_4: 0.9768  loss_dice_4: 1.004  loss_ce_5: 0.3217  loss_cate_5: 0  loss_mask_5: 1.002  loss_dice_5: 1.088  loss_ce_6: 0.3355  loss_cate_6: 0  loss_mask_6: 1.001  loss_dice_6: 0.9949  loss_ce_7: 0.3059  loss_cate_7: 0  loss_mask_7: 1.016  loss_dice_7: 1.057  loss_ce_8: 0.2952  loss_cate_8: 0  loss_mask_8: 1.007  loss_dice_8: 1.056  time: 1.1018  data_time: 0.0133  lr: 9.5512e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:03:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 13:03:41 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 13:03:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 13:04:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 13:04:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0019 s/iter. Inference: 0.1739 s/iter. Eval: 0.0142 s/iter. Total: 0.1901 s/iter. ETA=0:06:21\n",
      "\u001b[32m[10/12 13:04:40 d2.evaluation.evaluator]: \u001b[0mInference done 36/2016. Dataloading: 0.0026 s/iter. Inference: 0.1687 s/iter. Eval: 0.0269 s/iter. Total: 0.1984 s/iter. ETA=0:06:32\n",
      "\u001b[32m[10/12 13:04:45 d2.evaluation.evaluator]: \u001b[0mInference done 61/2016. Dataloading: 0.0026 s/iter. Inference: 0.1679 s/iter. Eval: 0.0288 s/iter. Total: 0.1996 s/iter. ETA=0:06:30\n",
      "\u001b[32m[10/12 13:04:50 d2.evaluation.evaluator]: \u001b[0mInference done 87/2016. Dataloading: 0.0026 s/iter. Inference: 0.1667 s/iter. Eval: 0.0292 s/iter. Total: 0.1986 s/iter. ETA=0:06:23\n",
      "\u001b[32m[10/12 13:04:55 d2.evaluation.evaluator]: \u001b[0mInference done 113/2016. Dataloading: 0.0026 s/iter. Inference: 0.1664 s/iter. Eval: 0.0293 s/iter. Total: 0.1985 s/iter. ETA=0:06:17\n",
      "\u001b[32m[10/12 13:05:00 d2.evaluation.evaluator]: \u001b[0mInference done 140/2016. Dataloading: 0.0026 s/iter. Inference: 0.1660 s/iter. Eval: 0.0283 s/iter. Total: 0.1970 s/iter. ETA=0:06:09\n",
      "\u001b[32m[10/12 13:05:05 d2.evaluation.evaluator]: \u001b[0mInference done 165/2016. Dataloading: 0.0026 s/iter. Inference: 0.1659 s/iter. Eval: 0.0290 s/iter. Total: 0.1976 s/iter. ETA=0:06:05\n",
      "\u001b[32m[10/12 13:05:11 d2.evaluation.evaluator]: \u001b[0mInference done 190/2016. Dataloading: 0.0026 s/iter. Inference: 0.1657 s/iter. Eval: 0.0296 s/iter. Total: 0.1981 s/iter. ETA=0:06:01\n",
      "\u001b[32m[10/12 13:05:16 d2.evaluation.evaluator]: \u001b[0mInference done 216/2016. Dataloading: 0.0025 s/iter. Inference: 0.1653 s/iter. Eval: 0.0299 s/iter. Total: 0.1980 s/iter. ETA=0:05:56\n",
      "\u001b[32m[10/12 13:05:21 d2.evaluation.evaluator]: \u001b[0mInference done 240/2016. Dataloading: 0.0025 s/iter. Inference: 0.1652 s/iter. Eval: 0.0315 s/iter. Total: 0.1993 s/iter. ETA=0:05:54\n",
      "\u001b[32m[10/12 13:05:26 d2.evaluation.evaluator]: \u001b[0mInference done 267/2016. Dataloading: 0.0025 s/iter. Inference: 0.1651 s/iter. Eval: 0.0306 s/iter. Total: 0.1984 s/iter. ETA=0:05:46\n",
      "\u001b[32m[10/12 13:05:31 d2.evaluation.evaluator]: \u001b[0mInference done 293/2016. Dataloading: 0.0025 s/iter. Inference: 0.1649 s/iter. Eval: 0.0307 s/iter. Total: 0.1983 s/iter. ETA=0:05:41\n",
      "\u001b[32m[10/12 13:05:36 d2.evaluation.evaluator]: \u001b[0mInference done 318/2016. Dataloading: 0.0024 s/iter. Inference: 0.1648 s/iter. Eval: 0.0310 s/iter. Total: 0.1984 s/iter. ETA=0:05:36\n",
      "\u001b[32m[10/12 13:05:41 d2.evaluation.evaluator]: \u001b[0mInference done 344/2016. Dataloading: 0.0024 s/iter. Inference: 0.1646 s/iter. Eval: 0.0313 s/iter. Total: 0.1985 s/iter. ETA=0:05:31\n",
      "\u001b[32m[10/12 13:05:46 d2.evaluation.evaluator]: \u001b[0mInference done 369/2016. Dataloading: 0.0024 s/iter. Inference: 0.1645 s/iter. Eval: 0.0317 s/iter. Total: 0.1988 s/iter. ETA=0:05:27\n",
      "\u001b[32m[10/12 13:05:51 d2.evaluation.evaluator]: \u001b[0mInference done 396/2016. Dataloading: 0.0024 s/iter. Inference: 0.1643 s/iter. Eval: 0.0311 s/iter. Total: 0.1980 s/iter. ETA=0:05:20\n",
      "\u001b[32m[10/12 13:05:56 d2.evaluation.evaluator]: \u001b[0mInference done 422/2016. Dataloading: 0.0024 s/iter. Inference: 0.1642 s/iter. Eval: 0.0312 s/iter. Total: 0.1980 s/iter. ETA=0:05:15\n",
      "\u001b[32m[10/12 13:06:02 d2.evaluation.evaluator]: \u001b[0mInference done 448/2016. Dataloading: 0.0024 s/iter. Inference: 0.1642 s/iter. Eval: 0.0312 s/iter. Total: 0.1979 s/iter. ETA=0:05:10\n",
      "\u001b[32m[10/12 13:06:07 d2.evaluation.evaluator]: \u001b[0mInference done 475/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0308 s/iter. Total: 0.1973 s/iter. ETA=0:05:04\n",
      "\u001b[32m[10/12 13:06:12 d2.evaluation.evaluator]: \u001b[0mInference done 501/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0307 s/iter. Total: 0.1972 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 13:06:17 d2.evaluation.evaluator]: \u001b[0mInference done 528/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0304 s/iter. Total: 0.1969 s/iter. ETA=0:04:52\n",
      "\u001b[32m[10/12 13:06:22 d2.evaluation.evaluator]: \u001b[0mInference done 554/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0302 s/iter. Total: 0.1968 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 13:06:27 d2.evaluation.evaluator]: \u001b[0mInference done 580/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0301 s/iter. Total: 0.1966 s/iter. ETA=0:04:42\n",
      "\u001b[32m[10/12 13:06:32 d2.evaluation.evaluator]: \u001b[0mInference done 605/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0304 s/iter. Total: 0.1969 s/iter. ETA=0:04:37\n",
      "\u001b[32m[10/12 13:06:37 d2.evaluation.evaluator]: \u001b[0mInference done 632/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0300 s/iter. Total: 0.1965 s/iter. ETA=0:04:31\n",
      "\u001b[32m[10/12 13:06:42 d2.evaluation.evaluator]: \u001b[0mInference done 657/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0302 s/iter. Total: 0.1967 s/iter. ETA=0:04:27\n",
      "\u001b[32m[10/12 13:06:47 d2.evaluation.evaluator]: \u001b[0mInference done 683/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0301 s/iter. Total: 0.1966 s/iter. ETA=0:04:22\n",
      "\u001b[32m[10/12 13:06:52 d2.evaluation.evaluator]: \u001b[0mInference done 709/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0301 s/iter. Total: 0.1965 s/iter. ETA=0:04:16\n",
      "\u001b[32m[10/12 13:06:57 d2.evaluation.evaluator]: \u001b[0mInference done 735/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0300 s/iter. Total: 0.1965 s/iter. ETA=0:04:11\n",
      "\u001b[32m[10/12 13:07:02 d2.evaluation.evaluator]: \u001b[0mInference done 759/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0305 s/iter. Total: 0.1969 s/iter. ETA=0:04:07\n",
      "\u001b[32m[10/12 13:07:07 d2.evaluation.evaluator]: \u001b[0mInference done 785/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0305 s/iter. Total: 0.1969 s/iter. ETA=0:04:02\n",
      "\u001b[32m[10/12 13:07:13 d2.evaluation.evaluator]: \u001b[0mInference done 812/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0303 s/iter. Total: 0.1967 s/iter. ETA=0:03:56\n",
      "\u001b[32m[10/12 13:07:18 d2.evaluation.evaluator]: \u001b[0mInference done 838/2016. Dataloading: 0.0024 s/iter. Inference: 0.1637 s/iter. Eval: 0.0303 s/iter. Total: 0.1966 s/iter. ETA=0:03:51\n",
      "\u001b[32m[10/12 13:07:23 d2.evaluation.evaluator]: \u001b[0mInference done 864/2016. Dataloading: 0.0024 s/iter. Inference: 0.1637 s/iter. Eval: 0.0302 s/iter. Total: 0.1970 s/iter. ETA=0:03:46\n",
      "\u001b[32m[10/12 13:07:28 d2.evaluation.evaluator]: \u001b[0mInference done 890/2016. Dataloading: 0.0024 s/iter. Inference: 0.1637 s/iter. Eval: 0.0303 s/iter. Total: 0.1971 s/iter. ETA=0:03:41\n",
      "\u001b[32m[10/12 13:07:33 d2.evaluation.evaluator]: \u001b[0mInference done 916/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0302 s/iter. Total: 0.1970 s/iter. ETA=0:03:36\n",
      "\u001b[32m[10/12 13:07:38 d2.evaluation.evaluator]: \u001b[0mInference done 941/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0305 s/iter. Total: 0.1972 s/iter. ETA=0:03:31\n",
      "\u001b[32m[10/12 13:07:44 d2.evaluation.evaluator]: \u001b[0mInference done 968/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0303 s/iter. Total: 0.1969 s/iter. ETA=0:03:26\n",
      "\u001b[32m[10/12 13:07:49 d2.evaluation.evaluator]: \u001b[0mInference done 993/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0305 s/iter. Total: 0.1971 s/iter. ETA=0:03:21\n",
      "\u001b[32m[10/12 13:07:54 d2.evaluation.evaluator]: \u001b[0mInference done 1020/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0304 s/iter. Total: 0.1970 s/iter. ETA=0:03:16\n",
      "\u001b[32m[10/12 13:07:59 d2.evaluation.evaluator]: \u001b[0mInference done 1045/2016. Dataloading: 0.0024 s/iter. Inference: 0.1635 s/iter. Eval: 0.0306 s/iter. Total: 0.1971 s/iter. ETA=0:03:11\n",
      "\u001b[32m[10/12 13:08:04 d2.evaluation.evaluator]: \u001b[0mInference done 1070/2016. Dataloading: 0.0024 s/iter. Inference: 0.1635 s/iter. Eval: 0.0308 s/iter. Total: 0.1973 s/iter. ETA=0:03:06\n",
      "\u001b[32m[10/12 13:08:09 d2.evaluation.evaluator]: \u001b[0mInference done 1097/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0306 s/iter. Total: 0.1970 s/iter. ETA=0:03:01\n",
      "\u001b[32m[10/12 13:08:14 d2.evaluation.evaluator]: \u001b[0mInference done 1124/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0305 s/iter. Total: 0.1969 s/iter. ETA=0:02:55\n",
      "\u001b[32m[10/12 13:08:19 d2.evaluation.evaluator]: \u001b[0mInference done 1151/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0303 s/iter. Total: 0.1967 s/iter. ETA=0:02:50\n",
      "\u001b[32m[10/12 13:08:24 d2.evaluation.evaluator]: \u001b[0mInference done 1178/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0302 s/iter. Total: 0.1965 s/iter. ETA=0:02:44\n",
      "\u001b[32m[10/12 13:08:29 d2.evaluation.evaluator]: \u001b[0mInference done 1205/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0300 s/iter. Total: 0.1963 s/iter. ETA=0:02:39\n",
      "\u001b[32m[10/12 13:08:35 d2.evaluation.evaluator]: \u001b[0mInference done 1232/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0299 s/iter. Total: 0.1962 s/iter. ETA=0:02:33\n",
      "\u001b[32m[10/12 13:08:40 d2.evaluation.evaluator]: \u001b[0mInference done 1259/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1960 s/iter. ETA=0:02:28\n",
      "\u001b[32m[10/12 13:08:45 d2.evaluation.evaluator]: \u001b[0mInference done 1286/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1959 s/iter. ETA=0:02:23\n",
      "\u001b[32m[10/12 13:08:50 d2.evaluation.evaluator]: \u001b[0mInference done 1312/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0296 s/iter. Total: 0.1959 s/iter. ETA=0:02:17\n",
      "\u001b[32m[10/12 13:08:55 d2.evaluation.evaluator]: \u001b[0mInference done 1337/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0298 s/iter. Total: 0.1960 s/iter. ETA=0:02:13\n",
      "\u001b[32m[10/12 13:09:00 d2.evaluation.evaluator]: \u001b[0mInference done 1364/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1959 s/iter. ETA=0:02:07\n",
      "\u001b[32m[10/12 13:09:05 d2.evaluation.evaluator]: \u001b[0mInference done 1390/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1959 s/iter. ETA=0:02:02\n",
      "\u001b[32m[10/12 13:09:10 d2.evaluation.evaluator]: \u001b[0mInference done 1417/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:01:57\n",
      "\u001b[32m[10/12 13:09:16 d2.evaluation.evaluator]: \u001b[0mInference done 1443/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1959 s/iter. ETA=0:01:52\n",
      "\u001b[32m[10/12 13:09:21 d2.evaluation.evaluator]: \u001b[0mInference done 1470/2016. Dataloading: 0.0023 s/iter. Inference: 0.1633 s/iter. Eval: 0.0296 s/iter. Total: 0.1957 s/iter. ETA=0:01:46\n",
      "\u001b[32m[10/12 13:09:26 d2.evaluation.evaluator]: \u001b[0mInference done 1496/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:01:41\n",
      "\u001b[32m[10/12 13:09:31 d2.evaluation.evaluator]: \u001b[0mInference done 1521/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1959 s/iter. ETA=0:01:36\n",
      "\u001b[32m[10/12 13:09:36 d2.evaluation.evaluator]: \u001b[0mInference done 1548/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:01:31\n",
      "\u001b[32m[10/12 13:09:41 d2.evaluation.evaluator]: \u001b[0mInference done 1574/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:01:26\n",
      "\u001b[32m[10/12 13:09:46 d2.evaluation.evaluator]: \u001b[0mInference done 1600/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:01:21\n",
      "\u001b[32m[10/12 13:09:51 d2.evaluation.evaluator]: \u001b[0mInference done 1626/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:01:16\n",
      "\u001b[32m[10/12 13:09:56 d2.evaluation.evaluator]: \u001b[0mInference done 1652/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:01:11\n",
      "\u001b[32m[10/12 13:10:01 d2.evaluation.evaluator]: \u001b[0mInference done 1678/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:01:06\n",
      "\u001b[32m[10/12 13:10:06 d2.evaluation.evaluator]: \u001b[0mInference done 1704/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0295 s/iter. Total: 0.1958 s/iter. ETA=0:01:01\n",
      "\u001b[32m[10/12 13:10:12 d2.evaluation.evaluator]: \u001b[0mInference done 1729/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1959 s/iter. ETA=0:00:56\n",
      "\u001b[32m[10/12 13:10:17 d2.evaluation.evaluator]: \u001b[0mInference done 1756/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0295 s/iter. Total: 0.1958 s/iter. ETA=0:00:50\n",
      "\u001b[32m[10/12 13:10:22 d2.evaluation.evaluator]: \u001b[0mInference done 1781/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0297 s/iter. Total: 0.1959 s/iter. ETA=0:00:46\n",
      "\u001b[32m[10/12 13:10:27 d2.evaluation.evaluator]: \u001b[0mInference done 1808/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:00:40\n",
      "\u001b[32m[10/12 13:10:32 d2.evaluation.evaluator]: \u001b[0mInference done 1834/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:00:35\n",
      "\u001b[32m[10/12 13:10:37 d2.evaluation.evaluator]: \u001b[0mInference done 1860/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:00:30\n",
      "\u001b[32m[10/12 13:10:42 d2.evaluation.evaluator]: \u001b[0mInference done 1887/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0294 s/iter. Total: 0.1956 s/iter. ETA=0:00:25\n",
      "\u001b[32m[10/12 13:10:47 d2.evaluation.evaluator]: \u001b[0mInference done 1911/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:00:20\n",
      "\u001b[32m[10/12 13:10:52 d2.evaluation.evaluator]: \u001b[0mInference done 1937/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1958 s/iter. ETA=0:00:15\n",
      "\u001b[32m[10/12 13:10:57 d2.evaluation.evaluator]: \u001b[0mInference done 1963/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:00:10\n",
      "\u001b[32m[10/12 13:11:02 d2.evaluation.evaluator]: \u001b[0mInference done 1989/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0295 s/iter. Total: 0.1957 s/iter. ETA=0:00:05\n",
      "\u001b[32m[10/12 13:11:07 d2.evaluation.evaluator]: \u001b[0mInference done 2015/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0296 s/iter. Total: 0.1957 s/iter. ETA=0:00:00\n",
      "\u001b[32m[10/12 13:11:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:33.710830 (0.195779 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 13:11:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:28 (0.163445 s / iter per device, on 1 devices)\n",
      "miou = 72.71806230432222\n",
      "OA = 88.13281513395764\n",
      "Kappa = 84.49925508000213\n",
      "F1_score = 67.71959958059303\n",
      "\u001b[32m[10/12 13:11:09 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 72.71806230432222, 'fwIoU': 79.31667474204448, 'IoU-Background': 29.87427416918279, 'IoU-Surfaces': 83.53961966937591, 'IoU-Building': 90.80095660070036, 'IoU-Low vegetation': 73.03145696137648, 'IoU-tree': 76.23333616501715, 'IoU-Car': 82.82873026028057, 'mACC': 81.12754940567207, 'pACC': 88.13281513395764, 'ACC-Background': 36.974015130194374, 'ACC-Surfaces': 90.71992602482942, 'ACC-Building': 96.09102047581098, 'ACC-Low vegetation': 87.09295831130652, 'ACC-tree': 87.33839988193859, 'ACC-Car': 88.54897660995255})])\n",
      "\u001b[32m[10/12 13:11:09 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 13:11:09 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 13:11:09 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 13:11:09 d2.evaluation.testing]: \u001b[0mcopypaste: 72.7181,79.3167,81.1275,88.1328\n",
      "\u001b[32m[10/12 13:11:09 d2.utils.events]: \u001b[0m eta: 23:09:34  iter: 3999  total_loss: 24.81  loss_ce: 0.3083  loss_cate: 0.2023  loss_mask: 0.903  loss_dice: 1.098  loss_ce_0: 0.4916  loss_cate_0: 0  loss_mask_0: 0.9668  loss_dice_0: 1.196  loss_ce_1: 0.4071  loss_cate_1: 0  loss_mask_1: 0.9634  loss_dice_1: 1.178  loss_ce_2: 0.3851  loss_cate_2: 0  loss_mask_2: 0.9156  loss_dice_2: 1.104  loss_ce_3: 0.4165  loss_cate_3: 0  loss_mask_3: 0.9219  loss_dice_3: 1.082  loss_ce_4: 0.4516  loss_cate_4: 0  loss_mask_4: 0.9287  loss_dice_4: 1.133  loss_ce_5: 0.304  loss_cate_5: 0  loss_mask_5: 0.9218  loss_dice_5: 1.152  loss_ce_6: 0.3513  loss_cate_6: 0  loss_mask_6: 0.9112  loss_dice_6: 1.106  loss_ce_7: 0.3044  loss_cate_7: 0  loss_mask_7: 0.9273  loss_dice_7: 1.113  loss_ce_8: 0.3153  loss_cate_8: 0  loss_mask_8: 0.95  loss_dice_8: 1.143  time: 1.1017  data_time: 0.0124  lr: 9.549e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:11:33 d2.utils.events]: \u001b[0m eta: 23:09:10  iter: 4019  total_loss: 21.03  loss_ce: 0.2388  loss_cate: 0.1584  loss_mask: 0.8811  loss_dice: 1.003  loss_ce_0: 0.4271  loss_cate_0: 0  loss_mask_0: 0.8393  loss_dice_0: 1.124  loss_ce_1: 0.2414  loss_cate_1: 0  loss_mask_1: 0.8892  loss_dice_1: 1.091  loss_ce_2: 0.2858  loss_cate_2: 0  loss_mask_2: 0.8616  loss_dice_2: 0.9866  loss_ce_3: 0.3022  loss_cate_3: 0  loss_mask_3: 0.8594  loss_dice_3: 0.9708  loss_ce_4: 0.2836  loss_cate_4: 0  loss_mask_4: 0.8325  loss_dice_4: 1.015  loss_ce_5: 0.2298  loss_cate_5: 0  loss_mask_5: 0.8606  loss_dice_5: 0.9916  loss_ce_6: 0.2679  loss_cate_6: 0  loss_mask_6: 0.853  loss_dice_6: 0.9829  loss_ce_7: 0.303  loss_cate_7: 0  loss_mask_7: 0.8638  loss_dice_7: 0.9857  loss_ce_8: 0.2879  loss_cate_8: 0  loss_mask_8: 0.8573  loss_dice_8: 0.9714  time: 1.1017  data_time: 0.0134  lr: 9.5467e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:11:55 d2.utils.events]: \u001b[0m eta: 23:08:50  iter: 4039  total_loss: 25.06  loss_ce: 0.3131  loss_cate: 0.1904  loss_mask: 0.9582  loss_dice: 1.099  loss_ce_0: 0.4923  loss_cate_0: 0  loss_mask_0: 0.9533  loss_dice_0: 1.149  loss_ce_1: 0.2775  loss_cate_1: 0  loss_mask_1: 0.9873  loss_dice_1: 1.137  loss_ce_2: 0.3217  loss_cate_2: 0  loss_mask_2: 0.9572  loss_dice_2: 1.127  loss_ce_3: 0.3398  loss_cate_3: 0  loss_mask_3: 0.952  loss_dice_3: 1.127  loss_ce_4: 0.3057  loss_cate_4: 0  loss_mask_4: 0.9362  loss_dice_4: 1.152  loss_ce_5: 0.3169  loss_cate_5: 0  loss_mask_5: 0.9218  loss_dice_5: 1.111  loss_ce_6: 0.3086  loss_cate_6: 0  loss_mask_6: 0.9737  loss_dice_6: 1.144  loss_ce_7: 0.2819  loss_cate_7: 0  loss_mask_7: 0.9588  loss_dice_7: 1.105  loss_ce_8: 0.245  loss_cate_8: 0  loss_mask_8: 0.9843  loss_dice_8: 1.086  time: 1.1017  data_time: 0.0129  lr: 9.5444e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:12:17 d2.utils.events]: \u001b[0m eta: 23:08:34  iter: 4059  total_loss: 23.42  loss_ce: 0.2947  loss_cate: 0.1932  loss_mask: 0.7999  loss_dice: 1.137  loss_ce_0: 0.3716  loss_cate_0: 0  loss_mask_0: 0.7875  loss_dice_0: 1.187  loss_ce_1: 0.3183  loss_cate_1: 0  loss_mask_1: 0.8461  loss_dice_1: 1.159  loss_ce_2: 0.2907  loss_cate_2: 0  loss_mask_2: 0.8154  loss_dice_2: 1.131  loss_ce_3: 0.2776  loss_cate_3: 0  loss_mask_3: 0.8329  loss_dice_3: 1.148  loss_ce_4: 0.2882  loss_cate_4: 0  loss_mask_4: 0.8376  loss_dice_4: 1.152  loss_ce_5: 0.3106  loss_cate_5: 0  loss_mask_5: 0.8608  loss_dice_5: 1.144  loss_ce_6: 0.3402  loss_cate_6: 0  loss_mask_6: 0.8487  loss_dice_6: 1.166  loss_ce_7: 0.2731  loss_cate_7: 0  loss_mask_7: 0.82  loss_dice_7: 1.172  loss_ce_8: 0.2666  loss_cate_8: 0  loss_mask_8: 0.8552  loss_dice_8: 1.166  time: 1.1017  data_time: 0.0125  lr: 9.5422e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:12:39 d2.utils.events]: \u001b[0m eta: 23:08:07  iter: 4079  total_loss: 21.76  loss_ce: 0.2821  loss_cate: 0.1814  loss_mask: 0.7703  loss_dice: 1.091  loss_ce_0: 0.333  loss_cate_0: 0  loss_mask_0: 0.7828  loss_dice_0: 1.165  loss_ce_1: 0.2634  loss_cate_1: 0  loss_mask_1: 0.7845  loss_dice_1: 1.088  loss_ce_2: 0.253  loss_cate_2: 0  loss_mask_2: 0.7319  loss_dice_2: 1.054  loss_ce_3: 0.2611  loss_cate_3: 0  loss_mask_3: 0.738  loss_dice_3: 1.078  loss_ce_4: 0.2286  loss_cate_4: 0  loss_mask_4: 0.7634  loss_dice_4: 1.09  loss_ce_5: 0.2064  loss_cate_5: 0  loss_mask_5: 0.7652  loss_dice_5: 1.075  loss_ce_6: 0.2244  loss_cate_6: 0  loss_mask_6: 0.7372  loss_dice_6: 1.055  loss_ce_7: 0.2729  loss_cate_7: 0  loss_mask_7: 0.7546  loss_dice_7: 1.043  loss_ce_8: 0.2625  loss_cate_8: 0  loss_mask_8: 0.767  loss_dice_8: 1.082  time: 1.1017  data_time: 0.0127  lr: 9.5399e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:13:02 d2.utils.events]: \u001b[0m eta: 23:07:42  iter: 4099  total_loss: 22.61  loss_ce: 0.2855  loss_cate: 0.1862  loss_mask: 0.8628  loss_dice: 1.106  loss_ce_0: 0.354  loss_cate_0: 0  loss_mask_0: 0.8584  loss_dice_0: 1.156  loss_ce_1: 0.3363  loss_cate_1: 0  loss_mask_1: 0.8812  loss_dice_1: 1.116  loss_ce_2: 0.3451  loss_cate_2: 0  loss_mask_2: 0.8425  loss_dice_2: 1.142  loss_ce_3: 0.3491  loss_cate_3: 0  loss_mask_3: 0.8726  loss_dice_3: 1.072  loss_ce_4: 0.3009  loss_cate_4: 0  loss_mask_4: 0.8695  loss_dice_4: 1.076  loss_ce_5: 0.32  loss_cate_5: 0  loss_mask_5: 0.8626  loss_dice_5: 1.075  loss_ce_6: 0.2368  loss_cate_6: 0  loss_mask_6: 0.8548  loss_dice_6: 1.098  loss_ce_7: 0.2617  loss_cate_7: 0  loss_mask_7: 0.8517  loss_dice_7: 1.085  loss_ce_8: 0.3111  loss_cate_8: 0  loss_mask_8: 0.8449  loss_dice_8: 1.092  time: 1.1017  data_time: 0.0132  lr: 9.5377e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:13:24 d2.utils.events]: \u001b[0m eta: 23:07:10  iter: 4119  total_loss: 26.98  loss_ce: 0.2737  loss_cate: 0.2259  loss_mask: 1.017  loss_dice: 1.244  loss_ce_0: 0.3953  loss_cate_0: 0  loss_mask_0: 1.025  loss_dice_0: 1.25  loss_ce_1: 0.367  loss_cate_1: 0  loss_mask_1: 1.013  loss_dice_1: 1.226  loss_ce_2: 0.3082  loss_cate_2: 0  loss_mask_2: 0.9545  loss_dice_2: 1.181  loss_ce_3: 0.3641  loss_cate_3: 0  loss_mask_3: 0.9521  loss_dice_3: 1.248  loss_ce_4: 0.2674  loss_cate_4: 0  loss_mask_4: 0.9688  loss_dice_4: 1.2  loss_ce_5: 0.2786  loss_cate_5: 0  loss_mask_5: 0.9849  loss_dice_5: 1.261  loss_ce_6: 0.2465  loss_cate_6: 0  loss_mask_6: 0.9495  loss_dice_6: 1.198  loss_ce_7: 0.2306  loss_cate_7: 0  loss_mask_7: 1.006  loss_dice_7: 1.277  loss_ce_8: 0.2752  loss_cate_8: 0  loss_mask_8: 0.991  loss_dice_8: 1.26  time: 1.1017  data_time: 0.0137  lr: 9.5354e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:13:46 d2.utils.events]: \u001b[0m eta: 23:06:47  iter: 4139  total_loss: 21.91  loss_ce: 0.2266  loss_cate: 0.1986  loss_mask: 0.9198  loss_dice: 1.019  loss_ce_0: 0.4045  loss_cate_0: 0  loss_mask_0: 0.928  loss_dice_0: 1.034  loss_ce_1: 0.2547  loss_cate_1: 0  loss_mask_1: 0.9393  loss_dice_1: 1.001  loss_ce_2: 0.2138  loss_cate_2: 0  loss_mask_2: 0.9572  loss_dice_2: 0.9927  loss_ce_3: 0.1975  loss_cate_3: 0  loss_mask_3: 0.9159  loss_dice_3: 1.021  loss_ce_4: 0.1855  loss_cate_4: 0  loss_mask_4: 0.9064  loss_dice_4: 1.013  loss_ce_5: 0.1572  loss_cate_5: 0  loss_mask_5: 0.9254  loss_dice_5: 1.029  loss_ce_6: 0.1949  loss_cate_6: 0  loss_mask_6: 0.9088  loss_dice_6: 0.9685  loss_ce_7: 0.1555  loss_cate_7: 0  loss_mask_7: 0.9131  loss_dice_7: 1.034  loss_ce_8: 0.1945  loss_cate_8: 0  loss_mask_8: 0.9268  loss_dice_8: 1.081  time: 1.1016  data_time: 0.0129  lr: 9.5331e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:14:09 d2.utils.events]: \u001b[0m eta: 23:06:23  iter: 4159  total_loss: 22.92  loss_ce: 0.2262  loss_cate: 0.178  loss_mask: 0.794  loss_dice: 1.012  loss_ce_0: 0.3249  loss_cate_0: 0  loss_mask_0: 0.8303  loss_dice_0: 1.094  loss_ce_1: 0.2723  loss_cate_1: 0  loss_mask_1: 0.8296  loss_dice_1: 1.047  loss_ce_2: 0.2376  loss_cate_2: 0  loss_mask_2: 0.8785  loss_dice_2: 1.098  loss_ce_3: 0.2241  loss_cate_3: 0  loss_mask_3: 0.8486  loss_dice_3: 1.084  loss_ce_4: 0.2778  loss_cate_4: 0  loss_mask_4: 0.8407  loss_dice_4: 1.034  loss_ce_5: 0.296  loss_cate_5: 0  loss_mask_5: 0.8153  loss_dice_5: 1.043  loss_ce_6: 0.2244  loss_cate_6: 0  loss_mask_6: 0.8284  loss_dice_6: 1.034  loss_ce_7: 0.1892  loss_cate_7: 0  loss_mask_7: 0.8537  loss_dice_7: 1.029  loss_ce_8: 0.2221  loss_cate_8: 0  loss_mask_8: 0.8383  loss_dice_8: 1.044  time: 1.1016  data_time: 0.0129  lr: 9.5309e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:14:31 d2.utils.events]: \u001b[0m eta: 23:06:00  iter: 4179  total_loss: 25.64  loss_ce: 0.376  loss_cate: 0.2356  loss_mask: 0.8683  loss_dice: 1.179  loss_ce_0: 0.3989  loss_cate_0: 0  loss_mask_0: 0.9454  loss_dice_0: 1.304  loss_ce_1: 0.3328  loss_cate_1: 0  loss_mask_1: 0.8581  loss_dice_1: 1.222  loss_ce_2: 0.3302  loss_cate_2: 0  loss_mask_2: 0.9613  loss_dice_2: 1.209  loss_ce_3: 0.3792  loss_cate_3: 0  loss_mask_3: 0.8876  loss_dice_3: 1.203  loss_ce_4: 0.3434  loss_cate_4: 0  loss_mask_4: 0.871  loss_dice_4: 1.198  loss_ce_5: 0.3663  loss_cate_5: 0  loss_mask_5: 0.8693  loss_dice_5: 1.218  loss_ce_6: 0.3702  loss_cate_6: 0  loss_mask_6: 0.8892  loss_dice_6: 1.214  loss_ce_7: 0.2941  loss_cate_7: 0  loss_mask_7: 0.8858  loss_dice_7: 1.214  loss_ce_8: 0.3254  loss_cate_8: 0  loss_mask_8: 0.9058  loss_dice_8: 1.289  time: 1.1016  data_time: 0.0128  lr: 9.5286e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:14:54 d2.utils.events]: \u001b[0m eta: 23:05:27  iter: 4199  total_loss: 23.69  loss_ce: 0.3354  loss_cate: 0.1932  loss_mask: 0.9145  loss_dice: 1.117  loss_ce_0: 0.4333  loss_cate_0: 0  loss_mask_0: 0.9162  loss_dice_0: 1.161  loss_ce_1: 0.3302  loss_cate_1: 0  loss_mask_1: 0.879  loss_dice_1: 1.14  loss_ce_2: 0.3363  loss_cate_2: 0  loss_mask_2: 0.8918  loss_dice_2: 1.059  loss_ce_3: 0.327  loss_cate_3: 0  loss_mask_3: 0.8734  loss_dice_3: 1.111  loss_ce_4: 0.3507  loss_cate_4: 0  loss_mask_4: 0.9093  loss_dice_4: 1.115  loss_ce_5: 0.3355  loss_cate_5: 0  loss_mask_5: 0.8622  loss_dice_5: 1.103  loss_ce_6: 0.3121  loss_cate_6: 0  loss_mask_6: 1.012  loss_dice_6: 1.066  loss_ce_7: 0.3201  loss_cate_7: 0  loss_mask_7: 0.9972  loss_dice_7: 1.092  loss_ce_8: 0.3467  loss_cate_8: 0  loss_mask_8: 1.021  loss_dice_8: 1.077  time: 1.1016  data_time: 0.0137  lr: 9.5263e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:15:16 d2.utils.events]: \u001b[0m eta: 23:05:03  iter: 4219  total_loss: 23.99  loss_ce: 0.3061  loss_cate: 0.1904  loss_mask: 0.945  loss_dice: 1.097  loss_ce_0: 0.3914  loss_cate_0: 0  loss_mask_0: 1.012  loss_dice_0: 1.109  loss_ce_1: 0.3546  loss_cate_1: 0  loss_mask_1: 0.9515  loss_dice_1: 1.118  loss_ce_2: 0.2839  loss_cate_2: 0  loss_mask_2: 0.9432  loss_dice_2: 1.033  loss_ce_3: 0.3351  loss_cate_3: 0  loss_mask_3: 0.9312  loss_dice_3: 1.026  loss_ce_4: 0.3277  loss_cate_4: 0  loss_mask_4: 0.9392  loss_dice_4: 1.018  loss_ce_5: 0.33  loss_cate_5: 0  loss_mask_5: 0.9311  loss_dice_5: 1.069  loss_ce_6: 0.2972  loss_cate_6: 0  loss_mask_6: 0.9122  loss_dice_6: 1.038  loss_ce_7: 0.271  loss_cate_7: 0  loss_mask_7: 0.9297  loss_dice_7: 1.093  loss_ce_8: 0.3375  loss_cate_8: 0  loss_mask_8: 0.9481  loss_dice_8: 1.124  time: 1.1016  data_time: 0.0139  lr: 9.5241e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:15:38 d2.utils.events]: \u001b[0m eta: 23:04:36  iter: 4239  total_loss: 24.13  loss_ce: 0.4284  loss_cate: 0.1902  loss_mask: 0.8609  loss_dice: 1.119  loss_ce_0: 0.3693  loss_cate_0: 0  loss_mask_0: 0.9099  loss_dice_0: 1.277  loss_ce_1: 0.3443  loss_cate_1: 0  loss_mask_1: 0.9247  loss_dice_1: 1.248  loss_ce_2: 0.3904  loss_cate_2: 0  loss_mask_2: 0.9139  loss_dice_2: 1.185  loss_ce_3: 0.369  loss_cate_3: 0  loss_mask_3: 0.9023  loss_dice_3: 1.126  loss_ce_4: 0.3352  loss_cate_4: 0  loss_mask_4: 0.9005  loss_dice_4: 1.173  loss_ce_5: 0.3271  loss_cate_5: 0  loss_mask_5: 0.8896  loss_dice_5: 1.169  loss_ce_6: 0.3166  loss_cate_6: 0  loss_mask_6: 0.8365  loss_dice_6: 1.118  loss_ce_7: 0.3453  loss_cate_7: 0  loss_mask_7: 0.8346  loss_dice_7: 1.099  loss_ce_8: 0.3687  loss_cate_8: 0  loss_mask_8: 0.9023  loss_dice_8: 1.134  time: 1.1016  data_time: 0.0124  lr: 9.5218e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:16:01 d2.utils.events]: \u001b[0m eta: 23:04:15  iter: 4259  total_loss: 24.93  loss_ce: 0.3124  loss_cate: 0.1925  loss_mask: 0.9756  loss_dice: 1.231  loss_ce_0: 0.364  loss_cate_0: 0  loss_mask_0: 0.9752  loss_dice_0: 1.244  loss_ce_1: 0.3394  loss_cate_1: 0  loss_mask_1: 0.9806  loss_dice_1: 1.149  loss_ce_2: 0.368  loss_cate_2: 0  loss_mask_2: 0.9669  loss_dice_2: 1.167  loss_ce_3: 0.3683  loss_cate_3: 0  loss_mask_3: 0.9536  loss_dice_3: 1.185  loss_ce_4: 0.3653  loss_cate_4: 0  loss_mask_4: 0.9521  loss_dice_4: 1.201  loss_ce_5: 0.3845  loss_cate_5: 0  loss_mask_5: 0.959  loss_dice_5: 1.203  loss_ce_6: 0.3597  loss_cate_6: 0  loss_mask_6: 0.9451  loss_dice_6: 1.183  loss_ce_7: 0.3681  loss_cate_7: 0  loss_mask_7: 0.9642  loss_dice_7: 1.21  loss_ce_8: 0.3597  loss_cate_8: 0  loss_mask_8: 0.9478  loss_dice_8: 1.21  time: 1.1016  data_time: 0.0130  lr: 9.5196e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:16:24 d2.utils.events]: \u001b[0m eta: 23:03:50  iter: 4279  total_loss: 25.25  loss_ce: 0.3514  loss_cate: 0.182  loss_mask: 0.9209  loss_dice: 1.194  loss_ce_0: 0.4632  loss_cate_0: 0  loss_mask_0: 0.918  loss_dice_0: 1.222  loss_ce_1: 0.3455  loss_cate_1: 0  loss_mask_1: 0.9271  loss_dice_1: 1.156  loss_ce_2: 0.3884  loss_cate_2: 0  loss_mask_2: 0.9411  loss_dice_2: 1.159  loss_ce_3: 0.312  loss_cate_3: 0  loss_mask_3: 0.9359  loss_dice_3: 1.203  loss_ce_4: 0.3478  loss_cate_4: 0  loss_mask_4: 0.9414  loss_dice_4: 1.24  loss_ce_5: 0.3066  loss_cate_5: 0  loss_mask_5: 0.9348  loss_dice_5: 1.194  loss_ce_6: 0.3673  loss_cate_6: 0  loss_mask_6: 0.9268  loss_dice_6: 1.195  loss_ce_7: 0.3772  loss_cate_7: 0  loss_mask_7: 0.9438  loss_dice_7: 1.189  loss_ce_8: 0.3677  loss_cate_8: 0  loss_mask_8: 0.9324  loss_dice_8: 1.163  time: 1.1016  data_time: 0.0132  lr: 9.5173e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:16:46 d2.utils.events]: \u001b[0m eta: 23:03:31  iter: 4299  total_loss: 22.27  loss_ce: 0.3024  loss_cate: 0.1751  loss_mask: 0.831  loss_dice: 1.029  loss_ce_0: 0.406  loss_cate_0: 0  loss_mask_0: 0.7927  loss_dice_0: 1.117  loss_ce_1: 0.322  loss_cate_1: 0  loss_mask_1: 0.7931  loss_dice_1: 1.055  loss_ce_2: 0.251  loss_cate_2: 0  loss_mask_2: 0.8162  loss_dice_2: 1.005  loss_ce_3: 0.2805  loss_cate_3: 0  loss_mask_3: 0.8421  loss_dice_3: 0.9981  loss_ce_4: 0.3238  loss_cate_4: 0  loss_mask_4: 0.8238  loss_dice_4: 1.017  loss_ce_5: 0.3019  loss_cate_5: 0  loss_mask_5: 0.8303  loss_dice_5: 1.04  loss_ce_6: 0.3848  loss_cate_6: 0  loss_mask_6: 0.8046  loss_dice_6: 1.009  loss_ce_7: 0.3131  loss_cate_7: 0  loss_mask_7: 0.845  loss_dice_7: 1.02  loss_ce_8: 0.3455  loss_cate_8: 0  loss_mask_8: 0.8194  loss_dice_8: 1.056  time: 1.1016  data_time: 0.0135  lr: 9.515e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:17:09 d2.utils.events]: \u001b[0m eta: 23:03:15  iter: 4319  total_loss: 21.57  loss_ce: 0.1999  loss_cate: 0.1718  loss_mask: 0.8078  loss_dice: 1.018  loss_ce_0: 0.3986  loss_cate_0: 0  loss_mask_0: 0.818  loss_dice_0: 1.104  loss_ce_1: 0.2893  loss_cate_1: 0  loss_mask_1: 0.8088  loss_dice_1: 1.011  loss_ce_2: 0.2838  loss_cate_2: 0  loss_mask_2: 0.7836  loss_dice_2: 0.9803  loss_ce_3: 0.2827  loss_cate_3: 0  loss_mask_3: 0.7795  loss_dice_3: 0.9482  loss_ce_4: 0.2437  loss_cate_4: 0  loss_mask_4: 0.7939  loss_dice_4: 1.014  loss_ce_5: 0.1886  loss_cate_5: 0  loss_mask_5: 0.7696  loss_dice_5: 1.068  loss_ce_6: 0.2002  loss_cate_6: 0  loss_mask_6: 0.7943  loss_dice_6: 1.027  loss_ce_7: 0.2402  loss_cate_7: 0  loss_mask_7: 0.7926  loss_dice_7: 1.021  loss_ce_8: 0.1879  loss_cate_8: 0  loss_mask_8: 0.8171  loss_dice_8: 1.018  time: 1.1016  data_time: 0.0152  lr: 9.5128e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:17:31 d2.utils.events]: \u001b[0m eta: 23:02:51  iter: 4339  total_loss: 22.64  loss_ce: 0.2317  loss_cate: 0.1981  loss_mask: 0.9995  loss_dice: 1.069  loss_ce_0: 0.3671  loss_cate_0: 0  loss_mask_0: 1.014  loss_dice_0: 1.179  loss_ce_1: 0.2531  loss_cate_1: 0  loss_mask_1: 0.9765  loss_dice_1: 1.058  loss_ce_2: 0.2697  loss_cate_2: 0  loss_mask_2: 0.9881  loss_dice_2: 1.048  loss_ce_3: 0.2394  loss_cate_3: 0  loss_mask_3: 0.9817  loss_dice_3: 1.043  loss_ce_4: 0.2545  loss_cate_4: 0  loss_mask_4: 0.9867  loss_dice_4: 1.033  loss_ce_5: 0.1954  loss_cate_5: 0  loss_mask_5: 0.9867  loss_dice_5: 1.079  loss_ce_6: 0.2272  loss_cate_6: 0  loss_mask_6: 0.9831  loss_dice_6: 1.089  loss_ce_7: 0.2683  loss_cate_7: 0  loss_mask_7: 0.954  loss_dice_7: 1.073  loss_ce_8: 0.2069  loss_cate_8: 0  loss_mask_8: 0.9675  loss_dice_8: 1.093  time: 1.1017  data_time: 0.0119  lr: 9.5105e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:17:54 d2.utils.events]: \u001b[0m eta: 23:02:39  iter: 4359  total_loss: 25.52  loss_ce: 0.2546  loss_cate: 0.2012  loss_mask: 0.9279  loss_dice: 1.158  loss_ce_0: 0.4432  loss_cate_0: 0  loss_mask_0: 1.006  loss_dice_0: 1.15  loss_ce_1: 0.3207  loss_cate_1: 0  loss_mask_1: 1.002  loss_dice_1: 1.1  loss_ce_2: 0.2446  loss_cate_2: 0  loss_mask_2: 0.9665  loss_dice_2: 1.049  loss_ce_3: 0.2648  loss_cate_3: 0  loss_mask_3: 0.9853  loss_dice_3: 1.124  loss_ce_4: 0.2791  loss_cate_4: 0  loss_mask_4: 0.9709  loss_dice_4: 1.099  loss_ce_5: 0.2462  loss_cate_5: 0  loss_mask_5: 0.9548  loss_dice_5: 1.128  loss_ce_6: 0.2643  loss_cate_6: 0  loss_mask_6: 0.956  loss_dice_6: 1.115  loss_ce_7: 0.2575  loss_cate_7: 0  loss_mask_7: 0.9409  loss_dice_7: 1.161  loss_ce_8: 0.2781  loss_cate_8: 0  loss_mask_8: 0.9344  loss_dice_8: 1.128  time: 1.1017  data_time: 0.0126  lr: 9.5082e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:18:16 d2.utils.events]: \u001b[0m eta: 23:02:12  iter: 4379  total_loss: 23.03  loss_ce: 0.2604  loss_cate: 0.185  loss_mask: 0.8861  loss_dice: 0.9879  loss_ce_0: 0.4588  loss_cate_0: 0  loss_mask_0: 0.922  loss_dice_0: 1.167  loss_ce_1: 0.3012  loss_cate_1: 0  loss_mask_1: 0.9252  loss_dice_1: 1.126  loss_ce_2: 0.2626  loss_cate_2: 0  loss_mask_2: 0.8635  loss_dice_2: 1.102  loss_ce_3: 0.2463  loss_cate_3: 0  loss_mask_3: 0.9126  loss_dice_3: 1.057  loss_ce_4: 0.2757  loss_cate_4: 0  loss_mask_4: 0.9176  loss_dice_4: 1.051  loss_ce_5: 0.2647  loss_cate_5: 0  loss_mask_5: 0.917  loss_dice_5: 1.014  loss_ce_6: 0.2594  loss_cate_6: 0  loss_mask_6: 0.9077  loss_dice_6: 1.001  loss_ce_7: 0.2589  loss_cate_7: 0  loss_mask_7: 0.8847  loss_dice_7: 1.043  loss_ce_8: 0.2669  loss_cate_8: 0  loss_mask_8: 0.8701  loss_dice_8: 1.017  time: 1.1017  data_time: 0.0130  lr: 9.506e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:18:39 d2.utils.events]: \u001b[0m eta: 23:01:51  iter: 4399  total_loss: 22.21  loss_ce: 0.2704  loss_cate: 0.1849  loss_mask: 0.8172  loss_dice: 1.013  loss_ce_0: 0.3886  loss_cate_0: 0  loss_mask_0: 0.7931  loss_dice_0: 1.125  loss_ce_1: 0.2375  loss_cate_1: 0  loss_mask_1: 0.8331  loss_dice_1: 1.013  loss_ce_2: 0.2622  loss_cate_2: 0  loss_mask_2: 0.781  loss_dice_2: 1.013  loss_ce_3: 0.2023  loss_cate_3: 0  loss_mask_3: 0.8363  loss_dice_3: 1.044  loss_ce_4: 0.2078  loss_cate_4: 0  loss_mask_4: 0.8393  loss_dice_4: 1.039  loss_ce_5: 0.2772  loss_cate_5: 0  loss_mask_5: 0.8079  loss_dice_5: 0.9909  loss_ce_6: 0.2067  loss_cate_6: 0  loss_mask_6: 0.8337  loss_dice_6: 1.023  loss_ce_7: 0.3074  loss_cate_7: 0  loss_mask_7: 0.8176  loss_dice_7: 1.016  loss_ce_8: 0.2346  loss_cate_8: 0  loss_mask_8: 0.8199  loss_dice_8: 0.9844  time: 1.1016  data_time: 0.0130  lr: 9.5037e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:19:02 d2.utils.events]: \u001b[0m eta: 23:01:34  iter: 4419  total_loss: 24.14  loss_ce: 0.3199  loss_cate: 0.178  loss_mask: 0.9651  loss_dice: 1.149  loss_ce_0: 0.3404  loss_cate_0: 0  loss_mask_0: 0.9445  loss_dice_0: 1.204  loss_ce_1: 0.2638  loss_cate_1: 0  loss_mask_1: 0.9627  loss_dice_1: 1.118  loss_ce_2: 0.2979  loss_cate_2: 0  loss_mask_2: 0.9386  loss_dice_2: 1.155  loss_ce_3: 0.2641  loss_cate_3: 0  loss_mask_3: 0.9116  loss_dice_3: 1.103  loss_ce_4: 0.2517  loss_cate_4: 0  loss_mask_4: 0.9495  loss_dice_4: 1.145  loss_ce_5: 0.2559  loss_cate_5: 0  loss_mask_5: 0.9342  loss_dice_5: 1.174  loss_ce_6: 0.2725  loss_cate_6: 0  loss_mask_6: 0.9667  loss_dice_6: 1.147  loss_ce_7: 0.2659  loss_cate_7: 0  loss_mask_7: 0.959  loss_dice_7: 1.134  loss_ce_8: 0.2626  loss_cate_8: 0  loss_mask_8: 0.9653  loss_dice_8: 1.103  time: 1.1016  data_time: 0.0131  lr: 9.5015e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:19:24 d2.utils.events]: \u001b[0m eta: 23:01:08  iter: 4439  total_loss: 24.46  loss_ce: 0.3236  loss_cate: 0.1809  loss_mask: 0.7851  loss_dice: 1.05  loss_ce_0: 0.4578  loss_cate_0: 0  loss_mask_0: 0.8704  loss_dice_0: 1.093  loss_ce_1: 0.3747  loss_cate_1: 0  loss_mask_1: 0.8744  loss_dice_1: 1.124  loss_ce_2: 0.3785  loss_cate_2: 0  loss_mask_2: 0.8697  loss_dice_2: 1.081  loss_ce_3: 0.3424  loss_cate_3: 0  loss_mask_3: 0.8281  loss_dice_3: 1.059  loss_ce_4: 0.2863  loss_cate_4: 0  loss_mask_4: 0.7736  loss_dice_4: 1.04  loss_ce_5: 0.3023  loss_cate_5: 0  loss_mask_5: 0.8221  loss_dice_5: 1.098  loss_ce_6: 0.3424  loss_cate_6: 0  loss_mask_6: 0.8566  loss_dice_6: 1.094  loss_ce_7: 0.3082  loss_cate_7: 0  loss_mask_7: 0.7955  loss_dice_7: 1.078  loss_ce_8: 0.2853  loss_cate_8: 0  loss_mask_8: 0.8108  loss_dice_8: 1.062  time: 1.1016  data_time: 0.0123  lr: 9.4992e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:19:47 d2.utils.events]: \u001b[0m eta: 23:00:42  iter: 4459  total_loss: 20.73  loss_ce: 0.2138  loss_cate: 0.1774  loss_mask: 0.7922  loss_dice: 0.9246  loss_ce_0: 0.2495  loss_cate_0: 0  loss_mask_0: 0.8233  loss_dice_0: 1.039  loss_ce_1: 0.2334  loss_cate_1: 0  loss_mask_1: 0.8564  loss_dice_1: 0.9597  loss_ce_2: 0.2699  loss_cate_2: 0  loss_mask_2: 0.8503  loss_dice_2: 0.9994  loss_ce_3: 0.236  loss_cate_3: 0  loss_mask_3: 0.8487  loss_dice_3: 0.9725  loss_ce_4: 0.2129  loss_cate_4: 0  loss_mask_4: 0.7998  loss_dice_4: 0.9439  loss_ce_5: 0.2339  loss_cate_5: 0  loss_mask_5: 0.8108  loss_dice_5: 0.954  loss_ce_6: 0.1924  loss_cate_6: 0  loss_mask_6: 0.822  loss_dice_6: 0.9492  loss_ce_7: 0.2  loss_cate_7: 0  loss_mask_7: 0.8355  loss_dice_7: 0.9564  loss_ce_8: 0.1514  loss_cate_8: 0  loss_mask_8: 0.8257  loss_dice_8: 0.9149  time: 1.1016  data_time: 0.0133  lr: 9.4969e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:20:09 d2.utils.events]: \u001b[0m eta: 23:00:20  iter: 4479  total_loss: 24.36  loss_ce: 0.3708  loss_cate: 0.1947  loss_mask: 0.8066  loss_dice: 1.156  loss_ce_0: 0.3629  loss_cate_0: 0  loss_mask_0: 0.8384  loss_dice_0: 1.261  loss_ce_1: 0.3987  loss_cate_1: 0  loss_mask_1: 0.937  loss_dice_1: 1.173  loss_ce_2: 0.4527  loss_cate_2: 0  loss_mask_2: 0.8313  loss_dice_2: 1.172  loss_ce_3: 0.3188  loss_cate_3: 0  loss_mask_3: 0.8037  loss_dice_3: 1.144  loss_ce_4: 0.3619  loss_cate_4: 0  loss_mask_4: 0.7796  loss_dice_4: 1.108  loss_ce_5: 0.3522  loss_cate_5: 0  loss_mask_5: 0.8204  loss_dice_5: 1.163  loss_ce_6: 0.3358  loss_cate_6: 0  loss_mask_6: 0.8268  loss_dice_6: 1.15  loss_ce_7: 0.3473  loss_cate_7: 0  loss_mask_7: 0.8391  loss_dice_7: 1.157  loss_ce_8: 0.3731  loss_cate_8: 0  loss_mask_8: 0.817  loss_dice_8: 1.156  time: 1.1016  data_time: 0.0127  lr: 9.4947e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:20:31 d2.utils.events]: \u001b[0m eta: 22:59:58  iter: 4499  total_loss: 20.27  loss_ce: 0.1649  loss_cate: 0.1762  loss_mask: 0.7746  loss_dice: 1  loss_ce_0: 0.3176  loss_cate_0: 0  loss_mask_0: 0.8635  loss_dice_0: 1.035  loss_ce_1: 0.2327  loss_cate_1: 0  loss_mask_1: 0.8328  loss_dice_1: 1.037  loss_ce_2: 0.2167  loss_cate_2: 0  loss_mask_2: 0.8049  loss_dice_2: 1.021  loss_ce_3: 0.218  loss_cate_3: 0  loss_mask_3: 0.8082  loss_dice_3: 1.019  loss_ce_4: 0.2058  loss_cate_4: 0  loss_mask_4: 0.7809  loss_dice_4: 0.9938  loss_ce_5: 0.2197  loss_cate_5: 0  loss_mask_5: 0.7743  loss_dice_5: 0.9784  loss_ce_6: 0.1486  loss_cate_6: 0  loss_mask_6: 0.7986  loss_dice_6: 1.001  loss_ce_7: 0.1721  loss_cate_7: 0  loss_mask_7: 0.7852  loss_dice_7: 0.9479  loss_ce_8: 0.1796  loss_cate_8: 0  loss_mask_8: 0.755  loss_dice_8: 1.001  time: 1.1016  data_time: 0.0135  lr: 9.4924e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:20:53 d2.utils.events]: \u001b[0m eta: 22:59:39  iter: 4519  total_loss: 24.02  loss_ce: 0.2836  loss_cate: 0.2038  loss_mask: 0.9121  loss_dice: 1.095  loss_ce_0: 0.4718  loss_cate_0: 0  loss_mask_0: 0.879  loss_dice_0: 1.134  loss_ce_1: 0.3513  loss_cate_1: 0  loss_mask_1: 0.9118  loss_dice_1: 1.159  loss_ce_2: 0.2985  loss_cate_2: 0  loss_mask_2: 0.8675  loss_dice_2: 1.135  loss_ce_3: 0.2861  loss_cate_3: 0  loss_mask_3: 0.8672  loss_dice_3: 1.167  loss_ce_4: 0.2631  loss_cate_4: 0  loss_mask_4: 0.9003  loss_dice_4: 1.153  loss_ce_5: 0.2474  loss_cate_5: 0  loss_mask_5: 0.8854  loss_dice_5: 1.111  loss_ce_6: 0.2292  loss_cate_6: 0  loss_mask_6: 0.8983  loss_dice_6: 1.083  loss_ce_7: 0.2262  loss_cate_7: 0  loss_mask_7: 0.8877  loss_dice_7: 1.092  loss_ce_8: 0.2951  loss_cate_8: 0  loss_mask_8: 0.896  loss_dice_8: 1.098  time: 1.1015  data_time: 0.0125  lr: 9.4901e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:21:16 d2.utils.events]: \u001b[0m eta: 22:59:05  iter: 4539  total_loss: 25.02  loss_ce: 0.3616  loss_cate: 0.2094  loss_mask: 0.9755  loss_dice: 1.213  loss_ce_0: 0.3879  loss_cate_0: 0  loss_mask_0: 0.9699  loss_dice_0: 1.254  loss_ce_1: 0.378  loss_cate_1: 0  loss_mask_1: 0.9691  loss_dice_1: 1.229  loss_ce_2: 0.37  loss_cate_2: 0  loss_mask_2: 0.9337  loss_dice_2: 1.233  loss_ce_3: 0.3448  loss_cate_3: 0  loss_mask_3: 0.9454  loss_dice_3: 1.255  loss_ce_4: 0.3506  loss_cate_4: 0  loss_mask_4: 0.9549  loss_dice_4: 1.275  loss_ce_5: 0.3328  loss_cate_5: 0  loss_mask_5: 0.9376  loss_dice_5: 1.237  loss_ce_6: 0.3431  loss_cate_6: 0  loss_mask_6: 0.9  loss_dice_6: 1.234  loss_ce_7: 0.3496  loss_cate_7: 0  loss_mask_7: 0.9171  loss_dice_7: 1.233  loss_ce_8: 0.3604  loss_cate_8: 0  loss_mask_8: 0.917  loss_dice_8: 1.206  time: 1.1015  data_time: 0.0126  lr: 9.4879e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:21:38 d2.utils.events]: \u001b[0m eta: 22:58:42  iter: 4559  total_loss: 24.89  loss_ce: 0.2891  loss_cate: 0.1813  loss_mask: 0.8804  loss_dice: 1.186  loss_ce_0: 0.3933  loss_cate_0: 0  loss_mask_0: 0.9265  loss_dice_0: 1.287  loss_ce_1: 0.3215  loss_cate_1: 0  loss_mask_1: 0.8815  loss_dice_1: 1.163  loss_ce_2: 0.2669  loss_cate_2: 0  loss_mask_2: 0.8799  loss_dice_2: 1.223  loss_ce_3: 0.2578  loss_cate_3: 0  loss_mask_3: 0.8608  loss_dice_3: 1.221  loss_ce_4: 0.2437  loss_cate_4: 0  loss_mask_4: 0.8806  loss_dice_4: 1.201  loss_ce_5: 0.2366  loss_cate_5: 0  loss_mask_5: 0.8698  loss_dice_5: 1.189  loss_ce_6: 0.2459  loss_cate_6: 0  loss_mask_6: 0.8637  loss_dice_6: 1.198  loss_ce_7: 0.2215  loss_cate_7: 0  loss_mask_7: 0.8805  loss_dice_7: 1.216  loss_ce_8: 0.2568  loss_cate_8: 0  loss_mask_8: 0.8788  loss_dice_8: 1.171  time: 1.1015  data_time: 0.0135  lr: 9.4856e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:22:00 d2.utils.events]: \u001b[0m eta: 22:58:26  iter: 4579  total_loss: 22.85  loss_ce: 0.2  loss_cate: 0.1743  loss_mask: 0.948  loss_dice: 1.045  loss_ce_0: 0.398  loss_cate_0: 0  loss_mask_0: 0.9716  loss_dice_0: 1.081  loss_ce_1: 0.2201  loss_cate_1: 0  loss_mask_1: 0.9534  loss_dice_1: 1.105  loss_ce_2: 0.3202  loss_cate_2: 0  loss_mask_2: 0.9497  loss_dice_2: 1.044  loss_ce_3: 0.2998  loss_cate_3: 0  loss_mask_3: 0.9261  loss_dice_3: 1.036  loss_ce_4: 0.2502  loss_cate_4: 0  loss_mask_4: 0.94  loss_dice_4: 1.026  loss_ce_5: 0.2448  loss_cate_5: 0  loss_mask_5: 0.9511  loss_dice_5: 1.024  loss_ce_6: 0.2498  loss_cate_6: 0  loss_mask_6: 0.9201  loss_dice_6: 1.046  loss_ce_7: 0.2261  loss_cate_7: 0  loss_mask_7: 0.9285  loss_dice_7: 1.038  loss_ce_8: 0.2323  loss_cate_8: 0  loss_mask_8: 0.9351  loss_dice_8: 0.9918  time: 1.1015  data_time: 0.0122  lr: 9.4834e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:22:23 d2.utils.events]: \u001b[0m eta: 22:58:06  iter: 4599  total_loss: 22.26  loss_ce: 0.2379  loss_cate: 0.168  loss_mask: 0.9009  loss_dice: 1.026  loss_ce_0: 0.3803  loss_cate_0: 0  loss_mask_0: 0.881  loss_dice_0: 1.049  loss_ce_1: 0.2658  loss_cate_1: 0  loss_mask_1: 0.9001  loss_dice_1: 1.057  loss_ce_2: 0.2267  loss_cate_2: 0  loss_mask_2: 0.8697  loss_dice_2: 1.05  loss_ce_3: 0.2546  loss_cate_3: 0  loss_mask_3: 0.8598  loss_dice_3: 1.018  loss_ce_4: 0.2552  loss_cate_4: 0  loss_mask_4: 0.8594  loss_dice_4: 1.044  loss_ce_5: 0.2229  loss_cate_5: 0  loss_mask_5: 0.8684  loss_dice_5: 1.01  loss_ce_6: 0.2447  loss_cate_6: 0  loss_mask_6: 0.8559  loss_dice_6: 1.033  loss_ce_7: 0.2023  loss_cate_7: 0  loss_mask_7: 0.898  loss_dice_7: 1.024  loss_ce_8: 0.2233  loss_cate_8: 0  loss_mask_8: 0.9044  loss_dice_8: 1.032  time: 1.1015  data_time: 0.0129  lr: 9.4811e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:22:45 d2.utils.events]: \u001b[0m eta: 22:57:52  iter: 4619  total_loss: 22.13  loss_ce: 0.2427  loss_cate: 0.1986  loss_mask: 0.7958  loss_dice: 0.9753  loss_ce_0: 0.389  loss_cate_0: 0  loss_mask_0: 0.7885  loss_dice_0: 1.048  loss_ce_1: 0.3682  loss_cate_1: 0  loss_mask_1: 0.8288  loss_dice_1: 1.014  loss_ce_2: 0.3283  loss_cate_2: 0  loss_mask_2: 0.8294  loss_dice_2: 0.9517  loss_ce_3: 0.3302  loss_cate_3: 0  loss_mask_3: 0.837  loss_dice_3: 0.9981  loss_ce_4: 0.2702  loss_cate_4: 0  loss_mask_4: 0.8032  loss_dice_4: 0.9849  loss_ce_5: 0.2555  loss_cate_5: 0  loss_mask_5: 0.8165  loss_dice_5: 0.9588  loss_ce_6: 0.244  loss_cate_6: 0  loss_mask_6: 0.8188  loss_dice_6: 0.9862  loss_ce_7: 0.256  loss_cate_7: 0  loss_mask_7: 0.8121  loss_dice_7: 0.9942  loss_ce_8: 0.2797  loss_cate_8: 0  loss_mask_8: 0.7974  loss_dice_8: 0.9612  time: 1.1014  data_time: 0.0127  lr: 9.4788e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:23:07 d2.utils.events]: \u001b[0m eta: 22:57:41  iter: 4639  total_loss: 19.05  loss_ce: 0.3429  loss_cate: 0.1756  loss_mask: 0.6751  loss_dice: 0.924  loss_ce_0: 0.4733  loss_cate_0: 0  loss_mask_0: 0.7035  loss_dice_0: 0.9865  loss_ce_1: 0.392  loss_cate_1: 0  loss_mask_1: 0.6929  loss_dice_1: 0.9912  loss_ce_2: 0.4069  loss_cate_2: 0  loss_mask_2: 0.6709  loss_dice_2: 0.946  loss_ce_3: 0.3165  loss_cate_3: 0  loss_mask_3: 0.6563  loss_dice_3: 0.947  loss_ce_4: 0.3651  loss_cate_4: 0  loss_mask_4: 0.6799  loss_dice_4: 0.8891  loss_ce_5: 0.3847  loss_cate_5: 0  loss_mask_5: 0.6408  loss_dice_5: 0.9688  loss_ce_6: 0.3273  loss_cate_6: 0  loss_mask_6: 0.6601  loss_dice_6: 0.9132  loss_ce_7: 0.3897  loss_cate_7: 0  loss_mask_7: 0.699  loss_dice_7: 0.9348  loss_ce_8: 0.3862  loss_cate_8: 0  loss_mask_8: 0.6621  loss_dice_8: 0.9225  time: 1.1014  data_time: 0.0141  lr: 9.4766e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:23:30 d2.utils.events]: \u001b[0m eta: 22:57:21  iter: 4659  total_loss: 24.68  loss_ce: 0.2563  loss_cate: 0.1837  loss_mask: 1.033  loss_dice: 1.162  loss_ce_0: 0.4172  loss_cate_0: 0  loss_mask_0: 0.9764  loss_dice_0: 1.089  loss_ce_1: 0.324  loss_cate_1: 0  loss_mask_1: 1.044  loss_dice_1: 1.132  loss_ce_2: 0.2754  loss_cate_2: 0  loss_mask_2: 1.063  loss_dice_2: 1.132  loss_ce_3: 0.2545  loss_cate_3: 0  loss_mask_3: 1.028  loss_dice_3: 1.102  loss_ce_4: 0.2203  loss_cate_4: 0  loss_mask_4: 1.049  loss_dice_4: 1.131  loss_ce_5: 0.2716  loss_cate_5: 0  loss_mask_5: 1.01  loss_dice_5: 1.105  loss_ce_6: 0.234  loss_cate_6: 0  loss_mask_6: 1.039  loss_dice_6: 1.133  loss_ce_7: 0.2581  loss_cate_7: 0  loss_mask_7: 1.011  loss_dice_7: 1.149  loss_ce_8: 0.2393  loss_cate_8: 0  loss_mask_8: 1.037  loss_dice_8: 1.113  time: 1.1014  data_time: 0.0137  lr: 9.4743e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:23:52 d2.utils.events]: \u001b[0m eta: 22:57:06  iter: 4679  total_loss: 23.09  loss_ce: 0.365  loss_cate: 0.2047  loss_mask: 0.8251  loss_dice: 1.059  loss_ce_0: 0.5037  loss_cate_0: 0  loss_mask_0: 0.8388  loss_dice_0: 1.062  loss_ce_1: 0.399  loss_cate_1: 0  loss_mask_1: 0.8651  loss_dice_1: 1.061  loss_ce_2: 0.3893  loss_cate_2: 0  loss_mask_2: 0.8481  loss_dice_2: 1.048  loss_ce_3: 0.3634  loss_cate_3: 0  loss_mask_3: 0.8528  loss_dice_3: 1.013  loss_ce_4: 0.3794  loss_cate_4: 0  loss_mask_4: 0.8353  loss_dice_4: 1.015  loss_ce_5: 0.4172  loss_cate_5: 0  loss_mask_5: 0.8469  loss_dice_5: 1.049  loss_ce_6: 0.3454  loss_cate_6: 0  loss_mask_6: 0.8269  loss_dice_6: 0.9993  loss_ce_7: 0.3753  loss_cate_7: 0  loss_mask_7: 0.8112  loss_dice_7: 1.033  loss_ce_8: 0.3431  loss_cate_8: 0  loss_mask_8: 0.8209  loss_dice_8: 1.051  time: 1.1014  data_time: 0.0124  lr: 9.472e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:24:15 d2.utils.events]: \u001b[0m eta: 22:57:02  iter: 4699  total_loss: 24.13  loss_ce: 0.3007  loss_cate: 0.1985  loss_mask: 0.8863  loss_dice: 1.082  loss_ce_0: 0.4128  loss_cate_0: 0  loss_mask_0: 0.9195  loss_dice_0: 1.178  loss_ce_1: 0.2953  loss_cate_1: 0  loss_mask_1: 0.9276  loss_dice_1: 1.139  loss_ce_2: 0.3206  loss_cate_2: 0  loss_mask_2: 0.8825  loss_dice_2: 1.107  loss_ce_3: 0.3113  loss_cate_3: 0  loss_mask_3: 0.8872  loss_dice_3: 1.106  loss_ce_4: 0.3017  loss_cate_4: 0  loss_mask_4: 0.8984  loss_dice_4: 1.096  loss_ce_5: 0.2673  loss_cate_5: 0  loss_mask_5: 0.8908  loss_dice_5: 1.09  loss_ce_6: 0.2999  loss_cate_6: 0  loss_mask_6: 0.8839  loss_dice_6: 1.083  loss_ce_7: 0.3091  loss_cate_7: 0  loss_mask_7: 0.897  loss_dice_7: 1.089  loss_ce_8: 0.3275  loss_cate_8: 0  loss_mask_8: 0.8911  loss_dice_8: 1.083  time: 1.1015  data_time: 0.0131  lr: 9.4698e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:24:37 d2.utils.events]: \u001b[0m eta: 22:57:04  iter: 4719  total_loss: 22.39  loss_ce: 0.2528  loss_cate: 0.169  loss_mask: 0.8016  loss_dice: 0.9547  loss_ce_0: 0.3311  loss_cate_0: 0  loss_mask_0: 0.8262  loss_dice_0: 1.009  loss_ce_1: 0.2219  loss_cate_1: 0  loss_mask_1: 0.8268  loss_dice_1: 0.9819  loss_ce_2: 0.2226  loss_cate_2: 0  loss_mask_2: 0.7977  loss_dice_2: 1.015  loss_ce_3: 0.296  loss_cate_3: 0  loss_mask_3: 0.8251  loss_dice_3: 0.9683  loss_ce_4: 0.2645  loss_cate_4: 0  loss_mask_4: 0.7984  loss_dice_4: 0.9397  loss_ce_5: 0.2805  loss_cate_5: 0  loss_mask_5: 0.8258  loss_dice_5: 0.9556  loss_ce_6: 0.2836  loss_cate_6: 0  loss_mask_6: 0.8027  loss_dice_6: 0.9238  loss_ce_7: 0.2685  loss_cate_7: 0  loss_mask_7: 0.8108  loss_dice_7: 0.9684  loss_ce_8: 0.2472  loss_cate_8: 0  loss_mask_8: 0.8007  loss_dice_8: 0.9239  time: 1.1015  data_time: 0.0130  lr: 9.4675e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:25:00 d2.utils.events]: \u001b[0m eta: 22:56:51  iter: 4739  total_loss: 22.88  loss_ce: 0.2531  loss_cate: 0.1898  loss_mask: 0.8593  loss_dice: 1.04  loss_ce_0: 0.4174  loss_cate_0: 0  loss_mask_0: 0.9317  loss_dice_0: 1.157  loss_ce_1: 0.3453  loss_cate_1: 0  loss_mask_1: 0.8565  loss_dice_1: 1.062  loss_ce_2: 0.303  loss_cate_2: 0  loss_mask_2: 0.857  loss_dice_2: 1.062  loss_ce_3: 0.2815  loss_cate_3: 0  loss_mask_3: 0.8382  loss_dice_3: 1.065  loss_ce_4: 0.2698  loss_cate_4: 0  loss_mask_4: 0.8382  loss_dice_4: 1.048  loss_ce_5: 0.2914  loss_cate_5: 0  loss_mask_5: 0.878  loss_dice_5: 1.02  loss_ce_6: 0.2544  loss_cate_6: 0  loss_mask_6: 0.8529  loss_dice_6: 0.9926  loss_ce_7: 0.2624  loss_cate_7: 0  loss_mask_7: 0.8446  loss_dice_7: 1.057  loss_ce_8: 0.2647  loss_cate_8: 0  loss_mask_8: 0.8415  loss_dice_8: 1.057  time: 1.1015  data_time: 0.0129  lr: 9.4652e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:25:28 d2.utils.events]: \u001b[0m eta: 22:56:32  iter: 4759  total_loss: 22.25  loss_ce: 0.223  loss_cate: 0.1884  loss_mask: 0.8175  loss_dice: 1.066  loss_ce_0: 0.3995  loss_cate_0: 0  loss_mask_0: 0.8192  loss_dice_0: 1.096  loss_ce_1: 0.3475  loss_cate_1: 0  loss_mask_1: 0.8075  loss_dice_1: 1.029  loss_ce_2: 0.3464  loss_cate_2: 0  loss_mask_2: 0.7694  loss_dice_2: 1.047  loss_ce_3: 0.2862  loss_cate_3: 0  loss_mask_3: 0.7865  loss_dice_3: 0.9881  loss_ce_4: 0.3074  loss_cate_4: 0  loss_mask_4: 0.7821  loss_dice_4: 1.011  loss_ce_5: 0.262  loss_cate_5: 0  loss_mask_5: 0.7797  loss_dice_5: 0.9993  loss_ce_6: 0.2615  loss_cate_6: 0  loss_mask_6: 0.7666  loss_dice_6: 0.9917  loss_ce_7: 0.2911  loss_cate_7: 0  loss_mask_7: 0.7787  loss_dice_7: 1.042  loss_ce_8: 0.2386  loss_cate_8: 0  loss_mask_8: 0.7731  loss_dice_8: 1.047  time: 1.1021  data_time: 0.1741  lr: 9.463e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:25:52 d2.utils.events]: \u001b[0m eta: 22:56:10  iter: 4779  total_loss: 20.95  loss_ce: 0.3096  loss_cate: 0.1848  loss_mask: 0.8061  loss_dice: 0.974  loss_ce_0: 0.4404  loss_cate_0: 0  loss_mask_0: 0.9308  loss_dice_0: 1.01  loss_ce_1: 0.3022  loss_cate_1: 0  loss_mask_1: 0.8196  loss_dice_1: 0.9426  loss_ce_2: 0.3263  loss_cate_2: 0  loss_mask_2: 0.8082  loss_dice_2: 0.9441  loss_ce_3: 0.3295  loss_cate_3: 0  loss_mask_3: 0.768  loss_dice_3: 0.9405  loss_ce_4: 0.321  loss_cate_4: 0  loss_mask_4: 0.7862  loss_dice_4: 0.9136  loss_ce_5: 0.3048  loss_cate_5: 0  loss_mask_5: 0.7869  loss_dice_5: 0.9601  loss_ce_6: 0.3022  loss_cate_6: 0  loss_mask_6: 0.7984  loss_dice_6: 0.9386  loss_ce_7: 0.3117  loss_cate_7: 0  loss_mask_7: 0.8124  loss_dice_7: 0.9474  loss_ce_8: 0.342  loss_cate_8: 0  loss_mask_8: 0.814  loss_dice_8: 0.9539  time: 1.1021  data_time: 0.0127  lr: 9.4607e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:26:15 d2.utils.events]: \u001b[0m eta: 22:55:48  iter: 4799  total_loss: 23.18  loss_ce: 0.2655  loss_cate: 0.1714  loss_mask: 0.7662  loss_dice: 1.048  loss_ce_0: 0.3789  loss_cate_0: 0  loss_mask_0: 0.8314  loss_dice_0: 1.173  loss_ce_1: 0.301  loss_cate_1: 0  loss_mask_1: 0.7798  loss_dice_1: 1.036  loss_ce_2: 0.2689  loss_cate_2: 0  loss_mask_2: 0.796  loss_dice_2: 1.065  loss_ce_3: 0.3195  loss_cate_3: 0  loss_mask_3: 0.7512  loss_dice_3: 1.047  loss_ce_4: 0.2889  loss_cate_4: 0  loss_mask_4: 0.7649  loss_dice_4: 1.08  loss_ce_5: 0.2915  loss_cate_5: 0  loss_mask_5: 0.8017  loss_dice_5: 1.074  loss_ce_6: 0.2375  loss_cate_6: 0  loss_mask_6: 0.7748  loss_dice_6: 1.056  loss_ce_7: 0.2012  loss_cate_7: 0  loss_mask_7: 0.78  loss_dice_7: 1.055  loss_ce_8: 0.2223  loss_cate_8: 0  loss_mask_8: 0.7649  loss_dice_8: 1.088  time: 1.1021  data_time: 0.0120  lr: 9.4585e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:26:38 d2.utils.events]: \u001b[0m eta: 22:55:21  iter: 4819  total_loss: 24.48  loss_ce: 0.3351  loss_cate: 0.2015  loss_mask: 0.9202  loss_dice: 1.063  loss_ce_0: 0.4786  loss_cate_0: 0  loss_mask_0: 0.9484  loss_dice_0: 1.14  loss_ce_1: 0.4255  loss_cate_1: 0  loss_mask_1: 0.905  loss_dice_1: 1.107  loss_ce_2: 0.3361  loss_cate_2: 0  loss_mask_2: 0.8699  loss_dice_2: 1.086  loss_ce_3: 0.3311  loss_cate_3: 0  loss_mask_3: 0.8716  loss_dice_3: 1.054  loss_ce_4: 0.3456  loss_cate_4: 0  loss_mask_4: 0.9088  loss_dice_4: 1.078  loss_ce_5: 0.3442  loss_cate_5: 0  loss_mask_5: 0.9177  loss_dice_5: 1.092  loss_ce_6: 0.313  loss_cate_6: 0  loss_mask_6: 0.8806  loss_dice_6: 1.097  loss_ce_7: 0.367  loss_cate_7: 0  loss_mask_7: 0.8982  loss_dice_7: 1.102  loss_ce_8: 0.3599  loss_cate_8: 0  loss_mask_8: 0.8542  loss_dice_8: 1.116  time: 1.1021  data_time: 0.0132  lr: 9.4562e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:27:00 d2.utils.events]: \u001b[0m eta: 22:55:12  iter: 4839  total_loss: 22.81  loss_ce: 0.3985  loss_cate: 0.1845  loss_mask: 0.8228  loss_dice: 1.022  loss_ce_0: 0.4232  loss_cate_0: 0  loss_mask_0: 0.803  loss_dice_0: 1.19  loss_ce_1: 0.4077  loss_cate_1: 0  loss_mask_1: 0.8551  loss_dice_1: 1.083  loss_ce_2: 0.3351  loss_cate_2: 0  loss_mask_2: 0.7905  loss_dice_2: 1.058  loss_ce_3: 0.4104  loss_cate_3: 0  loss_mask_3: 0.7857  loss_dice_3: 1.029  loss_ce_4: 0.3919  loss_cate_4: 0  loss_mask_4: 0.8068  loss_dice_4: 1.049  loss_ce_5: 0.3845  loss_cate_5: 0  loss_mask_5: 0.774  loss_dice_5: 1.051  loss_ce_6: 0.3897  loss_cate_6: 0  loss_mask_6: 0.7776  loss_dice_6: 1.007  loss_ce_7: 0.4001  loss_cate_7: 0  loss_mask_7: 0.7583  loss_dice_7: 1.019  loss_ce_8: 0.3892  loss_cate_8: 0  loss_mask_8: 0.7647  loss_dice_8: 1.007  time: 1.1021  data_time: 0.0131  lr: 9.4539e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:27:23 d2.utils.events]: \u001b[0m eta: 22:55:00  iter: 4859  total_loss: 21.67  loss_ce: 0.2316  loss_cate: 0.1567  loss_mask: 0.8044  loss_dice: 1.045  loss_ce_0: 0.3822  loss_cate_0: 0  loss_mask_0: 0.8834  loss_dice_0: 1.086  loss_ce_1: 0.2858  loss_cate_1: 0  loss_mask_1: 0.8607  loss_dice_1: 1.048  loss_ce_2: 0.2786  loss_cate_2: 0  loss_mask_2: 0.8251  loss_dice_2: 1.054  loss_ce_3: 0.2418  loss_cate_3: 0  loss_mask_3: 0.8263  loss_dice_3: 0.9926  loss_ce_4: 0.2225  loss_cate_4: 0  loss_mask_4: 0.8314  loss_dice_4: 1.007  loss_ce_5: 0.2296  loss_cate_5: 0  loss_mask_5: 0.8339  loss_dice_5: 1.008  loss_ce_6: 0.2472  loss_cate_6: 0  loss_mask_6: 0.8137  loss_dice_6: 0.993  loss_ce_7: 0.2332  loss_cate_7: 0  loss_mask_7: 0.8158  loss_dice_7: 1.017  loss_ce_8: 0.2507  loss_cate_8: 0  loss_mask_8: 0.818  loss_dice_8: 1.036  time: 1.1021  data_time: 0.0123  lr: 9.4517e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:27:46 d2.utils.events]: \u001b[0m eta: 22:54:51  iter: 4879  total_loss: 21.57  loss_ce: 0.1671  loss_cate: 0.2035  loss_mask: 0.8549  loss_dice: 1.038  loss_ce_0: 0.3531  loss_cate_0: 0  loss_mask_0: 0.8733  loss_dice_0: 1.092  loss_ce_1: 0.1655  loss_cate_1: 0  loss_mask_1: 0.8613  loss_dice_1: 1.058  loss_ce_2: 0.168  loss_cate_2: 0  loss_mask_2: 0.8483  loss_dice_2: 1.04  loss_ce_3: 0.1648  loss_cate_3: 0  loss_mask_3: 0.8638  loss_dice_3: 1.001  loss_ce_4: 0.2028  loss_cate_4: 0  loss_mask_4: 0.8541  loss_dice_4: 0.9939  loss_ce_5: 0.1855  loss_cate_5: 0  loss_mask_5: 0.8612  loss_dice_5: 1  loss_ce_6: 0.1922  loss_cate_6: 0  loss_mask_6: 0.8675  loss_dice_6: 1.027  loss_ce_7: 0.22  loss_cate_7: 0  loss_mask_7: 0.8551  loss_dice_7: 1.038  loss_ce_8: 0.1722  loss_cate_8: 0  loss_mask_8: 0.8603  loss_dice_8: 1.005  time: 1.1021  data_time: 0.0125  lr: 9.4494e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:28:08 d2.utils.events]: \u001b[0m eta: 22:54:31  iter: 4899  total_loss: 23.7  loss_ce: 0.3194  loss_cate: 0.1799  loss_mask: 0.871  loss_dice: 1.106  loss_ce_0: 0.5595  loss_cate_0: 0  loss_mask_0: 0.8943  loss_dice_0: 1.223  loss_ce_1: 0.438  loss_cate_1: 0  loss_mask_1: 0.9315  loss_dice_1: 1.115  loss_ce_2: 0.4459  loss_cate_2: 0  loss_mask_2: 0.8964  loss_dice_2: 1.129  loss_ce_3: 0.3389  loss_cate_3: 0  loss_mask_3: 0.9193  loss_dice_3: 1.111  loss_ce_4: 0.2905  loss_cate_4: 0  loss_mask_4: 0.9274  loss_dice_4: 1.145  loss_ce_5: 0.2769  loss_cate_5: 0  loss_mask_5: 0.9179  loss_dice_5: 1.121  loss_ce_6: 0.4118  loss_cate_6: 0  loss_mask_6: 0.9193  loss_dice_6: 1.118  loss_ce_7: 0.407  loss_cate_7: 0  loss_mask_7: 0.9099  loss_dice_7: 1.11  loss_ce_8: 0.3214  loss_cate_8: 0  loss_mask_8: 0.8741  loss_dice_8: 1.112  time: 1.1021  data_time: 0.0152  lr: 9.4471e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:28:30 d2.utils.events]: \u001b[0m eta: 22:54:26  iter: 4919  total_loss: 23  loss_ce: 0.3154  loss_cate: 0.1869  loss_mask: 0.977  loss_dice: 1.066  loss_ce_0: 0.405  loss_cate_0: 0  loss_mask_0: 0.9441  loss_dice_0: 1.183  loss_ce_1: 0.4148  loss_cate_1: 0  loss_mask_1: 0.9379  loss_dice_1: 1.123  loss_ce_2: 0.3247  loss_cate_2: 0  loss_mask_2: 0.9512  loss_dice_2: 1.138  loss_ce_3: 0.3277  loss_cate_3: 0  loss_mask_3: 0.919  loss_dice_3: 1.056  loss_ce_4: 0.3668  loss_cate_4: 0  loss_mask_4: 0.9004  loss_dice_4: 1.075  loss_ce_5: 0.3112  loss_cate_5: 0  loss_mask_5: 0.9833  loss_dice_5: 1.09  loss_ce_6: 0.3356  loss_cate_6: 0  loss_mask_6: 0.9453  loss_dice_6: 1.076  loss_ce_7: 0.3123  loss_cate_7: 0  loss_mask_7: 0.965  loss_dice_7: 1.095  loss_ce_8: 0.3275  loss_cate_8: 0  loss_mask_8: 0.9837  loss_dice_8: 1.118  time: 1.1021  data_time: 0.0140  lr: 9.4449e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:28:53 d2.utils.events]: \u001b[0m eta: 22:54:02  iter: 4939  total_loss: 22.54  loss_ce: 0.3109  loss_cate: 0.1652  loss_mask: 0.7559  loss_dice: 1.152  loss_ce_0: 0.4243  loss_cate_0: 0  loss_mask_0: 0.7534  loss_dice_0: 1.194  loss_ce_1: 0.3481  loss_cate_1: 0  loss_mask_1: 0.7884  loss_dice_1: 1.163  loss_ce_2: 0.325  loss_cate_2: 0  loss_mask_2: 0.7491  loss_dice_2: 1.111  loss_ce_3: 0.3257  loss_cate_3: 0  loss_mask_3: 0.7442  loss_dice_3: 1.158  loss_ce_4: 0.3371  loss_cate_4: 0  loss_mask_4: 0.7821  loss_dice_4: 1.155  loss_ce_5: 0.2698  loss_cate_5: 0  loss_mask_5: 0.8006  loss_dice_5: 1.148  loss_ce_6: 0.2676  loss_cate_6: 0  loss_mask_6: 0.7335  loss_dice_6: 1.164  loss_ce_7: 0.2409  loss_cate_7: 0  loss_mask_7: 0.7033  loss_dice_7: 1.152  loss_ce_8: 0.2921  loss_cate_8: 0  loss_mask_8: 0.7069  loss_dice_8: 1.171  time: 1.1021  data_time: 0.0131  lr: 9.4426e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:29:15 d2.utils.events]: \u001b[0m eta: 22:53:36  iter: 4959  total_loss: 23.85  loss_ce: 0.2021  loss_cate: 0.1729  loss_mask: 0.9033  loss_dice: 1.139  loss_ce_0: 0.3806  loss_cate_0: 0  loss_mask_0: 0.9213  loss_dice_0: 1.214  loss_ce_1: 0.2547  loss_cate_1: 0  loss_mask_1: 0.8767  loss_dice_1: 1.146  loss_ce_2: 0.3095  loss_cate_2: 0  loss_mask_2: 0.8942  loss_dice_2: 1.138  loss_ce_3: 0.249  loss_cate_3: 0  loss_mask_3: 0.919  loss_dice_3: 1.155  loss_ce_4: 0.2489  loss_cate_4: 0  loss_mask_4: 0.8932  loss_dice_4: 1.16  loss_ce_5: 0.2492  loss_cate_5: 0  loss_mask_5: 0.8906  loss_dice_5: 1.144  loss_ce_6: 0.2292  loss_cate_6: 0  loss_mask_6: 0.9338  loss_dice_6: 1.132  loss_ce_7: 0.2715  loss_cate_7: 0  loss_mask_7: 0.9179  loss_dice_7: 1.156  loss_ce_8: 0.2657  loss_cate_8: 0  loss_mask_8: 0.9141  loss_dice_8: 1.135  time: 1.1021  data_time: 0.0136  lr: 9.4403e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:29:37 d2.utils.events]: \u001b[0m eta: 22:53:14  iter: 4979  total_loss: 24.33  loss_ce: 0.252  loss_cate: 0.1884  loss_mask: 0.9417  loss_dice: 1.144  loss_ce_0: 0.4338  loss_cate_0: 0  loss_mask_0: 0.9014  loss_dice_0: 1.152  loss_ce_1: 0.2875  loss_cate_1: 0  loss_mask_1: 0.9587  loss_dice_1: 1.2  loss_ce_2: 0.324  loss_cate_2: 0  loss_mask_2: 0.9908  loss_dice_2: 1.175  loss_ce_3: 0.2853  loss_cate_3: 0  loss_mask_3: 0.9359  loss_dice_3: 1.153  loss_ce_4: 0.3799  loss_cate_4: 0  loss_mask_4: 0.931  loss_dice_4: 1.098  loss_ce_5: 0.2722  loss_cate_5: 0  loss_mask_5: 0.943  loss_dice_5: 1.15  loss_ce_6: 0.2835  loss_cate_6: 0  loss_mask_6: 0.9099  loss_dice_6: 1.092  loss_ce_7: 0.264  loss_cate_7: 0  loss_mask_7: 0.9086  loss_dice_7: 1.131  loss_ce_8: 0.2525  loss_cate_8: 0  loss_mask_8: 0.9578  loss_dice_8: 1.157  time: 1.1021  data_time: 0.0126  lr: 9.4381e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:30:00 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to ./output/potsdam_contrast_experiment_wrt_boundary_loss/model_0004999.pth\n",
      "\u001b[32m[10/12 13:30:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 13:30:56 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 13:30:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 13:32:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 13:32:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0016 s/iter. Inference: 0.1626 s/iter. Eval: 0.0145 s/iter. Total: 0.1788 s/iter. ETA=0:05:58\n",
      "\u001b[32m[10/12 13:32:08 d2.evaluation.evaluator]: \u001b[0mInference done 37/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0255 s/iter. Total: 0.1921 s/iter. ETA=0:06:20\n",
      "\u001b[32m[10/12 13:32:13 d2.evaluation.evaluator]: \u001b[0mInference done 63/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0281 s/iter. Total: 0.1950 s/iter. ETA=0:06:20\n",
      "\u001b[32m[10/12 13:32:18 d2.evaluation.evaluator]: \u001b[0mInference done 89/2016. Dataloading: 0.0025 s/iter. Inference: 0.1636 s/iter. Eval: 0.0282 s/iter. Total: 0.1945 s/iter. ETA=0:06:14\n",
      "\u001b[32m[10/12 13:32:24 d2.evaluation.evaluator]: \u001b[0mInference done 115/2016. Dataloading: 0.0025 s/iter. Inference: 0.1633 s/iter. Eval: 0.0295 s/iter. Total: 0.1955 s/iter. ETA=0:06:11\n",
      "\u001b[32m[10/12 13:32:29 d2.evaluation.evaluator]: \u001b[0mInference done 142/2016. Dataloading: 0.0025 s/iter. Inference: 0.1632 s/iter. Eval: 0.0284 s/iter. Total: 0.1943 s/iter. ETA=0:06:04\n",
      "\u001b[32m[10/12 13:32:34 d2.evaluation.evaluator]: \u001b[0mInference done 168/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0285 s/iter. Total: 0.1943 s/iter. ETA=0:05:59\n",
      "\u001b[32m[10/12 13:32:39 d2.evaluation.evaluator]: \u001b[0mInference done 195/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1938 s/iter. ETA=0:05:52\n",
      "\u001b[32m[10/12 13:32:44 d2.evaluation.evaluator]: \u001b[0mInference done 221/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0283 s/iter. Total: 0.1941 s/iter. ETA=0:05:48\n",
      "\u001b[32m[10/12 13:32:49 d2.evaluation.evaluator]: \u001b[0mInference done 247/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0282 s/iter. Total: 0.1939 s/iter. ETA=0:05:43\n",
      "\u001b[32m[10/12 13:32:54 d2.evaluation.evaluator]: \u001b[0mInference done 273/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0281 s/iter. Total: 0.1939 s/iter. ETA=0:05:37\n",
      "\u001b[32m[10/12 13:32:59 d2.evaluation.evaluator]: \u001b[0mInference done 299/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0284 s/iter. Total: 0.1941 s/iter. ETA=0:05:33\n",
      "\u001b[32m[10/12 13:33:04 d2.evaluation.evaluator]: \u001b[0mInference done 325/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0284 s/iter. Total: 0.1942 s/iter. ETA=0:05:28\n",
      "\u001b[32m[10/12 13:33:10 d2.evaluation.evaluator]: \u001b[0mInference done 348/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0315 s/iter. Total: 0.1974 s/iter. ETA=0:05:29\n",
      "\u001b[32m[10/12 13:33:15 d2.evaluation.evaluator]: \u001b[0mInference done 373/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0318 s/iter. Total: 0.1977 s/iter. ETA=0:05:24\n",
      "\u001b[32m[10/12 13:33:20 d2.evaluation.evaluator]: \u001b[0mInference done 399/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0316 s/iter. Total: 0.1975 s/iter. ETA=0:05:19\n",
      "\u001b[32m[10/12 13:33:25 d2.evaluation.evaluator]: \u001b[0mInference done 426/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0311 s/iter. Total: 0.1969 s/iter. ETA=0:05:13\n",
      "\u001b[32m[10/12 13:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 452/2016. Dataloading: 0.0025 s/iter. Inference: 0.1633 s/iter. Eval: 0.0308 s/iter. Total: 0.1968 s/iter. ETA=0:05:07\n",
      "\u001b[32m[10/12 13:33:35 d2.evaluation.evaluator]: \u001b[0mInference done 478/2016. Dataloading: 0.0032 s/iter. Inference: 0.1632 s/iter. Eval: 0.0301 s/iter. Total: 0.1968 s/iter. ETA=0:05:02\n",
      "\u001b[32m[10/12 13:33:40 d2.evaluation.evaluator]: \u001b[0mInference done 503/2016. Dataloading: 0.0032 s/iter. Inference: 0.1632 s/iter. Eval: 0.0305 s/iter. Total: 0.1971 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 13:33:45 d2.evaluation.evaluator]: \u001b[0mInference done 529/2016. Dataloading: 0.0031 s/iter. Inference: 0.1633 s/iter. Eval: 0.0304 s/iter. Total: 0.1970 s/iter. ETA=0:04:52\n",
      "\u001b[32m[10/12 13:33:50 d2.evaluation.evaluator]: \u001b[0mInference done 555/2016. Dataloading: 0.0031 s/iter. Inference: 0.1633 s/iter. Eval: 0.0303 s/iter. Total: 0.1969 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 13:33:55 d2.evaluation.evaluator]: \u001b[0mInference done 580/2016. Dataloading: 0.0030 s/iter. Inference: 0.1635 s/iter. Eval: 0.0303 s/iter. Total: 0.1971 s/iter. ETA=0:04:42\n",
      "\u001b[32m[10/12 13:34:00 d2.evaluation.evaluator]: \u001b[0mInference done 607/2016. Dataloading: 0.0030 s/iter. Inference: 0.1635 s/iter. Eval: 0.0300 s/iter. Total: 0.1968 s/iter. ETA=0:04:37\n",
      "\u001b[32m[10/12 13:34:06 d2.evaluation.evaluator]: \u001b[0mInference done 634/2016. Dataloading: 0.0030 s/iter. Inference: 0.1635 s/iter. Eval: 0.0298 s/iter. Total: 0.1965 s/iter. ETA=0:04:31\n",
      "\u001b[32m[10/12 13:34:11 d2.evaluation.evaluator]: \u001b[0mInference done 661/2016. Dataloading: 0.0030 s/iter. Inference: 0.1635 s/iter. Eval: 0.0296 s/iter. Total: 0.1963 s/iter. ETA=0:04:25\n",
      "\u001b[32m[10/12 13:34:16 d2.evaluation.evaluator]: \u001b[0mInference done 687/2016. Dataloading: 0.0029 s/iter. Inference: 0.1634 s/iter. Eval: 0.0297 s/iter. Total: 0.1963 s/iter. ETA=0:04:20\n",
      "\u001b[32m[10/12 13:34:21 d2.evaluation.evaluator]: \u001b[0mInference done 715/2016. Dataloading: 0.0029 s/iter. Inference: 0.1634 s/iter. Eval: 0.0293 s/iter. Total: 0.1958 s/iter. ETA=0:04:14\n",
      "\u001b[32m[10/12 13:34:26 d2.evaluation.evaluator]: \u001b[0mInference done 740/2016. Dataloading: 0.0029 s/iter. Inference: 0.1635 s/iter. Eval: 0.0294 s/iter. Total: 0.1960 s/iter. ETA=0:04:10\n",
      "\u001b[32m[10/12 13:34:31 d2.evaluation.evaluator]: \u001b[0mInference done 767/2016. Dataloading: 0.0029 s/iter. Inference: 0.1634 s/iter. Eval: 0.0293 s/iter. Total: 0.1958 s/iter. ETA=0:04:04\n",
      "\u001b[32m[10/12 13:34:36 d2.evaluation.evaluator]: \u001b[0mInference done 794/2016. Dataloading: 0.0028 s/iter. Inference: 0.1634 s/iter. Eval: 0.0292 s/iter. Total: 0.1956 s/iter. ETA=0:03:59\n",
      "\u001b[32m[10/12 13:34:41 d2.evaluation.evaluator]: \u001b[0mInference done 820/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0292 s/iter. Total: 0.1956 s/iter. ETA=0:03:53\n",
      "\u001b[32m[10/12 13:34:47 d2.evaluation.evaluator]: \u001b[0mInference done 846/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0293 s/iter. Total: 0.1956 s/iter. ETA=0:03:48\n",
      "\u001b[32m[10/12 13:34:52 d2.evaluation.evaluator]: \u001b[0mInference done 874/2016. Dataloading: 0.0028 s/iter. Inference: 0.1632 s/iter. Eval: 0.0290 s/iter. Total: 0.1952 s/iter. ETA=0:03:42\n",
      "\u001b[32m[10/12 13:34:57 d2.evaluation.evaluator]: \u001b[0mInference done 900/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0291 s/iter. Total: 0.1952 s/iter. ETA=0:03:37\n",
      "\u001b[32m[10/12 13:35:02 d2.evaluation.evaluator]: \u001b[0mInference done 927/2016. Dataloading: 0.0027 s/iter. Inference: 0.1631 s/iter. Eval: 0.0290 s/iter. Total: 0.1950 s/iter. ETA=0:03:32\n",
      "\u001b[32m[10/12 13:35:07 d2.evaluation.evaluator]: \u001b[0mInference done 952/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0291 s/iter. Total: 0.1952 s/iter. ETA=0:03:27\n",
      "\u001b[32m[10/12 13:35:12 d2.evaluation.evaluator]: \u001b[0mInference done 980/2016. Dataloading: 0.0027 s/iter. Inference: 0.1631 s/iter. Eval: 0.0289 s/iter. Total: 0.1949 s/iter. ETA=0:03:21\n",
      "\u001b[32m[10/12 13:35:17 d2.evaluation.evaluator]: \u001b[0mInference done 1006/2016. Dataloading: 0.0027 s/iter. Inference: 0.1631 s/iter. Eval: 0.0289 s/iter. Total: 0.1949 s/iter. ETA=0:03:16\n",
      "\u001b[32m[10/12 13:35:22 d2.evaluation.evaluator]: \u001b[0mInference done 1033/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0287 s/iter. Total: 0.1948 s/iter. ETA=0:03:11\n",
      "\u001b[32m[10/12 13:35:27 d2.evaluation.evaluator]: \u001b[0mInference done 1058/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0289 s/iter. Total: 0.1950 s/iter. ETA=0:03:06\n",
      "\u001b[32m[10/12 13:35:32 d2.evaluation.evaluator]: \u001b[0mInference done 1085/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0287 s/iter. Total: 0.1948 s/iter. ETA=0:03:01\n",
      "\u001b[32m[10/12 13:35:37 d2.evaluation.evaluator]: \u001b[0mInference done 1111/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0286 s/iter. Total: 0.1948 s/iter. ETA=0:02:56\n",
      "\u001b[32m[10/12 13:35:42 d2.evaluation.evaluator]: \u001b[0mInference done 1138/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0285 s/iter. Total: 0.1946 s/iter. ETA=0:02:50\n",
      "\u001b[32m[10/12 13:35:48 d2.evaluation.evaluator]: \u001b[0mInference done 1164/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0285 s/iter. Total: 0.1947 s/iter. ETA=0:02:45\n",
      "\u001b[32m[10/12 13:35:53 d2.evaluation.evaluator]: \u001b[0mInference done 1191/2016. Dataloading: 0.0026 s/iter. Inference: 0.1633 s/iter. Eval: 0.0284 s/iter. Total: 0.1946 s/iter. ETA=0:02:40\n",
      "\u001b[32m[10/12 13:35:58 d2.evaluation.evaluator]: \u001b[0mInference done 1217/2016. Dataloading: 0.0026 s/iter. Inference: 0.1633 s/iter. Eval: 0.0285 s/iter. Total: 0.1945 s/iter. ETA=0:02:35\n",
      "\u001b[32m[10/12 13:36:03 d2.evaluation.evaluator]: \u001b[0mInference done 1243/2016. Dataloading: 0.0026 s/iter. Inference: 0.1632 s/iter. Eval: 0.0285 s/iter. Total: 0.1946 s/iter. ETA=0:02:30\n",
      "\u001b[32m[10/12 13:36:08 d2.evaluation.evaluator]: \u001b[0mInference done 1270/2016. Dataloading: 0.0026 s/iter. Inference: 0.1632 s/iter. Eval: 0.0284 s/iter. Total: 0.1944 s/iter. ETA=0:02:25\n",
      "\u001b[32m[10/12 13:36:13 d2.evaluation.evaluator]: \u001b[0mInference done 1297/2016. Dataloading: 0.0026 s/iter. Inference: 0.1632 s/iter. Eval: 0.0283 s/iter. Total: 0.1943 s/iter. ETA=0:02:19\n",
      "\u001b[32m[10/12 13:36:18 d2.evaluation.evaluator]: \u001b[0mInference done 1324/2016. Dataloading: 0.0026 s/iter. Inference: 0.1632 s/iter. Eval: 0.0282 s/iter. Total: 0.1941 s/iter. ETA=0:02:14\n",
      "\u001b[32m[10/12 13:36:23 d2.evaluation.evaluator]: \u001b[0mInference done 1350/2016. Dataloading: 0.0026 s/iter. Inference: 0.1631 s/iter. Eval: 0.0283 s/iter. Total: 0.1942 s/iter. ETA=0:02:09\n",
      "\u001b[32m[10/12 13:36:28 d2.evaluation.evaluator]: \u001b[0mInference done 1377/2016. Dataloading: 0.0026 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1941 s/iter. ETA=0:02:04\n",
      "\u001b[32m[10/12 13:36:33 d2.evaluation.evaluator]: \u001b[0mInference done 1403/2016. Dataloading: 0.0026 s/iter. Inference: 0.1631 s/iter. Eval: 0.0282 s/iter. Total: 0.1941 s/iter. ETA=0:01:58\n",
      "\u001b[32m[10/12 13:36:39 d2.evaluation.evaluator]: \u001b[0mInference done 1430/2016. Dataloading: 0.0026 s/iter. Inference: 0.1631 s/iter. Eval: 0.0281 s/iter. Total: 0.1941 s/iter. ETA=0:01:53\n",
      "\u001b[32m[10/12 13:36:44 d2.evaluation.evaluator]: \u001b[0mInference done 1454/2016. Dataloading: 0.0026 s/iter. Inference: 0.1632 s/iter. Eval: 0.0283 s/iter. Total: 0.1943 s/iter. ETA=0:01:49\n",
      "\u001b[32m[10/12 13:36:49 d2.evaluation.evaluator]: \u001b[0mInference done 1480/2016. Dataloading: 0.0026 s/iter. Inference: 0.1633 s/iter. Eval: 0.0283 s/iter. Total: 0.1943 s/iter. ETA=0:01:44\n",
      "\u001b[32m[10/12 13:36:54 d2.evaluation.evaluator]: \u001b[0mInference done 1505/2016. Dataloading: 0.0026 s/iter. Inference: 0.1633 s/iter. Eval: 0.0283 s/iter. Total: 0.1945 s/iter. ETA=0:01:39\n",
      "\u001b[32m[10/12 13:36:59 d2.evaluation.evaluator]: \u001b[0mInference done 1532/2016. Dataloading: 0.0026 s/iter. Inference: 0.1633 s/iter. Eval: 0.0282 s/iter. Total: 0.1944 s/iter. ETA=0:01:34\n",
      "\u001b[32m[10/12 13:37:04 d2.evaluation.evaluator]: \u001b[0mInference done 1557/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0283 s/iter. Total: 0.1945 s/iter. ETA=0:01:29\n",
      "\u001b[32m[10/12 13:37:09 d2.evaluation.evaluator]: \u001b[0mInference done 1581/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0285 s/iter. Total: 0.1947 s/iter. ETA=0:01:24\n",
      "\u001b[32m[10/12 13:37:14 d2.evaluation.evaluator]: \u001b[0mInference done 1605/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0287 s/iter. Total: 0.1950 s/iter. ETA=0:01:20\n",
      "\u001b[32m[10/12 13:37:19 d2.evaluation.evaluator]: \u001b[0mInference done 1631/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0287 s/iter. Total: 0.1949 s/iter. ETA=0:01:15\n",
      "\u001b[32m[10/12 13:37:24 d2.evaluation.evaluator]: \u001b[0mInference done 1657/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0286 s/iter. Total: 0.1949 s/iter. ETA=0:01:09\n",
      "\u001b[32m[10/12 13:37:29 d2.evaluation.evaluator]: \u001b[0mInference done 1684/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0286 s/iter. Total: 0.1949 s/iter. ETA=0:01:04\n",
      "\u001b[32m[10/12 13:37:34 d2.evaluation.evaluator]: \u001b[0mInference done 1711/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0285 s/iter. Total: 0.1948 s/iter. ETA=0:00:59\n",
      "\u001b[32m[10/12 13:37:39 d2.evaluation.evaluator]: \u001b[0mInference done 1737/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0285 s/iter. Total: 0.1948 s/iter. ETA=0:00:54\n",
      "\u001b[32m[10/12 13:37:44 d2.evaluation.evaluator]: \u001b[0mInference done 1763/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0285 s/iter. Total: 0.1947 s/iter. ETA=0:00:49\n",
      "\u001b[32m[10/12 13:37:49 d2.evaluation.evaluator]: \u001b[0mInference done 1788/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0286 s/iter. Total: 0.1948 s/iter. ETA=0:00:44\n",
      "\u001b[32m[10/12 13:37:55 d2.evaluation.evaluator]: \u001b[0mInference done 1815/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0289 s/iter. Total: 0.1951 s/iter. ETA=0:00:39\n",
      "\u001b[32m[10/12 13:38:00 d2.evaluation.evaluator]: \u001b[0mInference done 1842/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0288 s/iter. Total: 0.1950 s/iter. ETA=0:00:33\n",
      "\u001b[32m[10/12 13:38:05 d2.evaluation.evaluator]: \u001b[0mInference done 1868/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0288 s/iter. Total: 0.1951 s/iter. ETA=0:00:28\n",
      "\u001b[32m[10/12 13:38:10 d2.evaluation.evaluator]: \u001b[0mInference done 1894/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0288 s/iter. Total: 0.1950 s/iter. ETA=0:00:23\n",
      "\u001b[32m[10/12 13:38:16 d2.evaluation.evaluator]: \u001b[0mInference done 1922/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0286 s/iter. Total: 0.1949 s/iter. ETA=0:00:18\n",
      "\u001b[32m[10/12 13:38:21 d2.evaluation.evaluator]: \u001b[0mInference done 1948/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0286 s/iter. Total: 0.1949 s/iter. ETA=0:00:13\n",
      "\u001b[32m[10/12 13:38:26 d2.evaluation.evaluator]: \u001b[0mInference done 1975/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0286 s/iter. Total: 0.1948 s/iter. ETA=0:00:07\n",
      "\u001b[32m[10/12 13:38:31 d2.evaluation.evaluator]: \u001b[0mInference done 2001/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0286 s/iter. Total: 0.1949 s/iter. ETA=0:00:02\n",
      "\u001b[32m[10/12 13:38:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:31.712705 (0.194785 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 13:38:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:28 (0.163469 s / iter per device, on 1 devices)\n",
      "miou = 73.93542172479374\n",
      "OA = 88.44649224054247\n",
      "Kappa = 84.94029285225106\n",
      "F1_score = 69.76135808202876\n",
      "\u001b[32m[10/12 13:38:34 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 73.93542172479374, 'fwIoU': 79.84589214023985, 'IoU-Background': 35.71361704315171, 'IoU-Surfaces': 83.59387132352126, 'IoU-Building': 92.23627638682011, 'IoU-Low vegetation': 72.551197563656, 'IoU-tree': 76.17498554852644, 'IoU-Car': 83.34258248308703, 'mACC': 82.79911977271725, 'pACC': 88.44649224054247, 'ACC-Background': 43.202718592630326, 'ACC-Surfaces': 89.63629619167135, 'ACC-Building': 96.20796019274248, 'ACC-Low vegetation': 87.35090655102383, 'ACC-tree': 88.6196910079855, 'ACC-Car': 91.77714610024996})])\n",
      "\u001b[32m[10/12 13:38:34 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 13:38:34 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 13:38:34 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 13:38:34 d2.evaluation.testing]: \u001b[0mcopypaste: 73.9354,79.8459,82.7991,88.4465\n",
      "\u001b[32m[10/12 13:38:34 d2.utils.events]: \u001b[0m eta: 22:52:52  iter: 4999  total_loss: 21.18  loss_ce: 0.2211  loss_cate: 0.1563  loss_mask: 0.8998  loss_dice: 1.046  loss_ce_0: 0.3292  loss_cate_0: 0  loss_mask_0: 0.8479  loss_dice_0: 1.084  loss_ce_1: 0.2816  loss_cate_1: 0  loss_mask_1: 0.8565  loss_dice_1: 1.057  loss_ce_2: 0.2817  loss_cate_2: 0  loss_mask_2: 0.8607  loss_dice_2: 1.043  loss_ce_3: 0.2364  loss_cate_3: 0  loss_mask_3: 0.8881  loss_dice_3: 0.9901  loss_ce_4: 0.2398  loss_cate_4: 0  loss_mask_4: 0.9073  loss_dice_4: 1.031  loss_ce_5: 0.2397  loss_cate_5: 0  loss_mask_5: 0.9163  loss_dice_5: 1.047  loss_ce_6: 0.2511  loss_cate_6: 0  loss_mask_6: 0.9048  loss_dice_6: 1.04  loss_ce_7: 0.245  loss_cate_7: 0  loss_mask_7: 0.8673  loss_dice_7: 1.033  loss_ce_8: 0.2443  loss_cate_8: 0  loss_mask_8: 0.9064  loss_dice_8: 1.038  time: 1.1020  data_time: 0.0127  lr: 9.4358e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:38:57 d2.utils.events]: \u001b[0m eta: 22:52:30  iter: 5019  total_loss: 20.62  loss_ce: 0.2659  loss_cate: 0.1752  loss_mask: 0.7949  loss_dice: 0.9934  loss_ce_0: 0.4347  loss_cate_0: 0  loss_mask_0: 0.8517  loss_dice_0: 1.052  loss_ce_1: 0.3178  loss_cate_1: 0  loss_mask_1: 0.7911  loss_dice_1: 1.041  loss_ce_2: 0.2552  loss_cate_2: 0  loss_mask_2: 0.7966  loss_dice_2: 0.9919  loss_ce_3: 0.2834  loss_cate_3: 0  loss_mask_3: 0.7922  loss_dice_3: 0.982  loss_ce_4: 0.2566  loss_cate_4: 0  loss_mask_4: 0.7847  loss_dice_4: 1.003  loss_ce_5: 0.2953  loss_cate_5: 0  loss_mask_5: 0.8069  loss_dice_5: 1.001  loss_ce_6: 0.2757  loss_cate_6: 0  loss_mask_6: 0.8112  loss_dice_6: 0.972  loss_ce_7: 0.2633  loss_cate_7: 0  loss_mask_7: 0.8171  loss_dice_7: 1.012  loss_ce_8: 0.246  loss_cate_8: 0  loss_mask_8: 0.7988  loss_dice_8: 0.9844  time: 1.1020  data_time: 0.0130  lr: 9.4335e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:39:19 d2.utils.events]: \u001b[0m eta: 22:52:15  iter: 5039  total_loss: 24.93  loss_ce: 0.2411  loss_cate: 0.1941  loss_mask: 0.9451  loss_dice: 1.07  loss_ce_0: 0.424  loss_cate_0: 0  loss_mask_0: 0.9123  loss_dice_0: 1.193  loss_ce_1: 0.3216  loss_cate_1: 0  loss_mask_1: 0.8789  loss_dice_1: 1.069  loss_ce_2: 0.296  loss_cate_2: 0  loss_mask_2: 0.8864  loss_dice_2: 1.051  loss_ce_3: 0.288  loss_cate_3: 0  loss_mask_3: 0.9159  loss_dice_3: 1.096  loss_ce_4: 0.2436  loss_cate_4: 0  loss_mask_4: 0.9258  loss_dice_4: 1.034  loss_ce_5: 0.2299  loss_cate_5: 0  loss_mask_5: 0.9124  loss_dice_5: 1.09  loss_ce_6: 0.2373  loss_cate_6: 0  loss_mask_6: 0.9626  loss_dice_6: 1.063  loss_ce_7: 0.2669  loss_cate_7: 0  loss_mask_7: 0.9406  loss_dice_7: 1.031  loss_ce_8: 0.2366  loss_cate_8: 0  loss_mask_8: 0.9269  loss_dice_8: 1.093  time: 1.1020  data_time: 0.0121  lr: 9.4313e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:39:41 d2.utils.events]: \u001b[0m eta: 22:51:49  iter: 5059  total_loss: 21.57  loss_ce: 0.1731  loss_cate: 0.1722  loss_mask: 0.9441  loss_dice: 1.087  loss_ce_0: 0.335  loss_cate_0: 0  loss_mask_0: 0.9087  loss_dice_0: 1.062  loss_ce_1: 0.2424  loss_cate_1: 0  loss_mask_1: 0.9008  loss_dice_1: 1.057  loss_ce_2: 0.2014  loss_cate_2: 0  loss_mask_2: 0.9076  loss_dice_2: 1.03  loss_ce_3: 0.1927  loss_cate_3: 0  loss_mask_3: 0.8731  loss_dice_3: 1.002  loss_ce_4: 0.1329  loss_cate_4: 0  loss_mask_4: 0.8822  loss_dice_4: 1.044  loss_ce_5: 0.1596  loss_cate_5: 0  loss_mask_5: 0.8721  loss_dice_5: 0.9905  loss_ce_6: 0.1986  loss_cate_6: 0  loss_mask_6: 0.8924  loss_dice_6: 1.023  loss_ce_7: 0.2009  loss_cate_7: 0  loss_mask_7: 0.8739  loss_dice_7: 1.025  loss_ce_8: 0.1844  loss_cate_8: 0  loss_mask_8: 0.8861  loss_dice_8: 1.013  time: 1.1020  data_time: 0.0121  lr: 9.429e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:40:03 d2.utils.events]: \u001b[0m eta: 22:51:33  iter: 5079  total_loss: 22.98  loss_ce: 0.2571  loss_cate: 0.1707  loss_mask: 0.8948  loss_dice: 1.034  loss_ce_0: 0.3916  loss_cate_0: 0  loss_mask_0: 0.9175  loss_dice_0: 1.066  loss_ce_1: 0.2764  loss_cate_1: 0  loss_mask_1: 0.9147  loss_dice_1: 1.072  loss_ce_2: 0.2857  loss_cate_2: 0  loss_mask_2: 0.9422  loss_dice_2: 1.071  loss_ce_3: 0.3062  loss_cate_3: 0  loss_mask_3: 0.8716  loss_dice_3: 1.074  loss_ce_4: 0.264  loss_cate_4: 0  loss_mask_4: 0.8747  loss_dice_4: 1.087  loss_ce_5: 0.2915  loss_cate_5: 0  loss_mask_5: 0.9349  loss_dice_5: 1.058  loss_ce_6: 0.3298  loss_cate_6: 0  loss_mask_6: 0.966  loss_dice_6: 1.075  loss_ce_7: 0.242  loss_cate_7: 0  loss_mask_7: 0.8639  loss_dice_7: 1.036  loss_ce_8: 0.2892  loss_cate_8: 0  loss_mask_8: 0.8875  loss_dice_8: 1.041  time: 1.1020  data_time: 0.0150  lr: 9.4268e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:40:26 d2.utils.events]: \u001b[0m eta: 22:51:05  iter: 5099  total_loss: 24.68  loss_ce: 0.2545  loss_cate: 0.1885  loss_mask: 0.9205  loss_dice: 1.104  loss_ce_0: 0.3888  loss_cate_0: 0  loss_mask_0: 1.064  loss_dice_0: 1.216  loss_ce_1: 0.3363  loss_cate_1: 0  loss_mask_1: 0.9611  loss_dice_1: 1.208  loss_ce_2: 0.3276  loss_cate_2: 0  loss_mask_2: 0.934  loss_dice_2: 1.12  loss_ce_3: 0.319  loss_cate_3: 0  loss_mask_3: 0.9819  loss_dice_3: 1.099  loss_ce_4: 0.2702  loss_cate_4: 0  loss_mask_4: 0.9546  loss_dice_4: 1.121  loss_ce_5: 0.2217  loss_cate_5: 0  loss_mask_5: 0.9551  loss_dice_5: 1.129  loss_ce_6: 0.2601  loss_cate_6: 0  loss_mask_6: 0.9499  loss_dice_6: 1.133  loss_ce_7: 0.2773  loss_cate_7: 0  loss_mask_7: 0.9371  loss_dice_7: 1.116  loss_ce_8: 0.2862  loss_cate_8: 0  loss_mask_8: 0.9343  loss_dice_8: 1.115  time: 1.1020  data_time: 0.0128  lr: 9.4245e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:40:49 d2.utils.events]: \u001b[0m eta: 22:50:39  iter: 5119  total_loss: 23.83  loss_ce: 0.279  loss_cate: 0.202  loss_mask: 0.8127  loss_dice: 1.062  loss_ce_0: 0.402  loss_cate_0: 0  loss_mask_0: 0.9598  loss_dice_0: 1.152  loss_ce_1: 0.3799  loss_cate_1: 0  loss_mask_1: 0.8166  loss_dice_1: 1.128  loss_ce_2: 0.3132  loss_cate_2: 0  loss_mask_2: 0.829  loss_dice_2: 1.041  loss_ce_3: 0.3412  loss_cate_3: 0  loss_mask_3: 0.8341  loss_dice_3: 1.07  loss_ce_4: 0.3076  loss_cate_4: 0  loss_mask_4: 0.8018  loss_dice_4: 1.075  loss_ce_5: 0.3179  loss_cate_5: 0  loss_mask_5: 0.7909  loss_dice_5: 1.051  loss_ce_6: 0.3187  loss_cate_6: 0  loss_mask_6: 0.8018  loss_dice_6: 1.044  loss_ce_7: 0.2827  loss_cate_7: 0  loss_mask_7: 0.813  loss_dice_7: 1.055  loss_ce_8: 0.3043  loss_cate_8: 0  loss_mask_8: 0.8095  loss_dice_8: 1.024  time: 1.1020  data_time: 0.0121  lr: 9.4222e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:41:11 d2.utils.events]: \u001b[0m eta: 22:50:17  iter: 5139  total_loss: 23.76  loss_ce: 0.3905  loss_cate: 0.2002  loss_mask: 0.8027  loss_dice: 1.03  loss_ce_0: 0.3964  loss_cate_0: 0  loss_mask_0: 0.8238  loss_dice_0: 1.202  loss_ce_1: 0.3723  loss_cate_1: 0  loss_mask_1: 0.8011  loss_dice_1: 1.106  loss_ce_2: 0.3574  loss_cate_2: 0  loss_mask_2: 0.8257  loss_dice_2: 1.132  loss_ce_3: 0.3091  loss_cate_3: 0  loss_mask_3: 0.8644  loss_dice_3: 1.129  loss_ce_4: 0.3375  loss_cate_4: 0  loss_mask_4: 0.8503  loss_dice_4: 1.131  loss_ce_5: 0.2975  loss_cate_5: 0  loss_mask_5: 0.8378  loss_dice_5: 1.083  loss_ce_6: 0.2867  loss_cate_6: 0  loss_mask_6: 0.7975  loss_dice_6: 1.05  loss_ce_7: 0.3396  loss_cate_7: 0  loss_mask_7: 0.8265  loss_dice_7: 1.052  loss_ce_8: 0.3593  loss_cate_8: 0  loss_mask_8: 0.8296  loss_dice_8: 1.09  time: 1.1019  data_time: 0.0120  lr: 9.42e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:41:33 d2.utils.events]: \u001b[0m eta: 22:49:55  iter: 5159  total_loss: 24.12  loss_ce: 0.2431  loss_cate: 0.203  loss_mask: 0.858  loss_dice: 1.089  loss_ce_0: 0.4803  loss_cate_0: 0  loss_mask_0: 0.9  loss_dice_0: 1.164  loss_ce_1: 0.2814  loss_cate_1: 0  loss_mask_1: 0.8718  loss_dice_1: 1.11  loss_ce_2: 0.2539  loss_cate_2: 0  loss_mask_2: 0.8684  loss_dice_2: 1.08  loss_ce_3: 0.3103  loss_cate_3: 0  loss_mask_3: 0.8689  loss_dice_3: 1.121  loss_ce_4: 0.2326  loss_cate_4: 0  loss_mask_4: 0.8554  loss_dice_4: 1.149  loss_ce_5: 0.1963  loss_cate_5: 0  loss_mask_5: 0.8503  loss_dice_5: 1.142  loss_ce_6: 0.2518  loss_cate_6: 0  loss_mask_6: 0.8771  loss_dice_6: 1.061  loss_ce_7: 0.2213  loss_cate_7: 0  loss_mask_7: 0.8776  loss_dice_7: 1.08  loss_ce_8: 0.2784  loss_cate_8: 0  loss_mask_8: 0.8575  loss_dice_8: 1.083  time: 1.1019  data_time: 0.0133  lr: 9.4177e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:41:55 d2.utils.events]: \u001b[0m eta: 22:49:29  iter: 5179  total_loss: 22.47  loss_ce: 0.3111  loss_cate: 0.1825  loss_mask: 0.8256  loss_dice: 0.9178  loss_ce_0: 0.4803  loss_cate_0: 0  loss_mask_0: 0.8923  loss_dice_0: 0.9862  loss_ce_1: 0.3568  loss_cate_1: 0  loss_mask_1: 0.8363  loss_dice_1: 0.9523  loss_ce_2: 0.2864  loss_cate_2: 0  loss_mask_2: 0.8322  loss_dice_2: 1.011  loss_ce_3: 0.3995  loss_cate_3: 0  loss_mask_3: 0.8264  loss_dice_3: 0.9322  loss_ce_4: 0.3551  loss_cate_4: 0  loss_mask_4: 0.8277  loss_dice_4: 0.9485  loss_ce_5: 0.3276  loss_cate_5: 0  loss_mask_5: 0.8457  loss_dice_5: 0.8812  loss_ce_6: 0.3325  loss_cate_6: 0  loss_mask_6: 0.845  loss_dice_6: 0.9441  loss_ce_7: 0.325  loss_cate_7: 0  loss_mask_7: 0.8177  loss_dice_7: 0.8964  loss_ce_8: 0.3289  loss_cate_8: 0  loss_mask_8: 0.8413  loss_dice_8: 0.8824  time: 1.1019  data_time: 0.0134  lr: 9.4154e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:42:18 d2.utils.events]: \u001b[0m eta: 22:49:13  iter: 5199  total_loss: 22.53  loss_ce: 0.2568  loss_cate: 0.1846  loss_mask: 0.9088  loss_dice: 1.03  loss_ce_0: 0.4054  loss_cate_0: 0  loss_mask_0: 0.9553  loss_dice_0: 1.074  loss_ce_1: 0.2594  loss_cate_1: 0  loss_mask_1: 0.9576  loss_dice_1: 1.095  loss_ce_2: 0.3012  loss_cate_2: 0  loss_mask_2: 0.892  loss_dice_2: 1.053  loss_ce_3: 0.2877  loss_cate_3: 0  loss_mask_3: 0.9079  loss_dice_3: 1.015  loss_ce_4: 0.2607  loss_cate_4: 0  loss_mask_4: 0.9428  loss_dice_4: 1.039  loss_ce_5: 0.2481  loss_cate_5: 0  loss_mask_5: 0.9192  loss_dice_5: 1.04  loss_ce_6: 0.2387  loss_cate_6: 0  loss_mask_6: 0.9195  loss_dice_6: 1.032  loss_ce_7: 0.2303  loss_cate_7: 0  loss_mask_7: 0.9091  loss_dice_7: 1.056  loss_ce_8: 0.2636  loss_cate_8: 0  loss_mask_8: 0.9127  loss_dice_8: 1.039  time: 1.1019  data_time: 0.0138  lr: 9.4132e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:42:40 d2.utils.events]: \u001b[0m eta: 22:48:58  iter: 5219  total_loss: 21.99  loss_ce: 0.2878  loss_cate: 0.1734  loss_mask: 0.8303  loss_dice: 1.043  loss_ce_0: 0.4289  loss_cate_0: 0  loss_mask_0: 0.8546  loss_dice_0: 1.028  loss_ce_1: 0.331  loss_cate_1: 0  loss_mask_1: 0.896  loss_dice_1: 1.05  loss_ce_2: 0.2805  loss_cate_2: 0  loss_mask_2: 0.8241  loss_dice_2: 1.059  loss_ce_3: 0.2307  loss_cate_3: 0  loss_mask_3: 0.8155  loss_dice_3: 1.076  loss_ce_4: 0.2083  loss_cate_4: 0  loss_mask_4: 0.8507  loss_dice_4: 1.126  loss_ce_5: 0.211  loss_cate_5: 0  loss_mask_5: 0.8464  loss_dice_5: 1.032  loss_ce_6: 0.2782  loss_cate_6: 0  loss_mask_6: 0.8493  loss_dice_6: 1.054  loss_ce_7: 0.2384  loss_cate_7: 0  loss_mask_7: 0.8503  loss_dice_7: 1.076  loss_ce_8: 0.2664  loss_cate_8: 0  loss_mask_8: 0.8066  loss_dice_8: 1.053  time: 1.1020  data_time: 0.0170  lr: 9.4109e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:43:03 d2.utils.events]: \u001b[0m eta: 22:48:31  iter: 5239  total_loss: 24.2  loss_ce: 0.2674  loss_cate: 0.1983  loss_mask: 0.9067  loss_dice: 1.095  loss_ce_0: 0.3738  loss_cate_0: 0  loss_mask_0: 0.9717  loss_dice_0: 1.191  loss_ce_1: 0.2359  loss_cate_1: 0  loss_mask_1: 0.9645  loss_dice_1: 1.185  loss_ce_2: 0.2449  loss_cate_2: 0  loss_mask_2: 0.9311  loss_dice_2: 1.112  loss_ce_3: 0.2146  loss_cate_3: 0  loss_mask_3: 0.9045  loss_dice_3: 1.07  loss_ce_4: 0.2335  loss_cate_4: 0  loss_mask_4: 0.9012  loss_dice_4: 1.107  loss_ce_5: 0.2438  loss_cate_5: 0  loss_mask_5: 0.9463  loss_dice_5: 1.084  loss_ce_6: 0.2685  loss_cate_6: 0  loss_mask_6: 0.928  loss_dice_6: 1.093  loss_ce_7: 0.2517  loss_cate_7: 0  loss_mask_7: 0.9297  loss_dice_7: 1.125  loss_ce_8: 0.2324  loss_cate_8: 0  loss_mask_8: 0.9222  loss_dice_8: 1.129  time: 1.1019  data_time: 0.0126  lr: 9.4086e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:43:25 d2.utils.events]: \u001b[0m eta: 22:48:07  iter: 5259  total_loss: 24.06  loss_ce: 0.3065  loss_cate: 0.1868  loss_mask: 0.924  loss_dice: 1.099  loss_ce_0: 0.4028  loss_cate_0: 0  loss_mask_0: 0.9395  loss_dice_0: 1.169  loss_ce_1: 0.3256  loss_cate_1: 0  loss_mask_1: 0.9091  loss_dice_1: 1.081  loss_ce_2: 0.3636  loss_cate_2: 0  loss_mask_2: 0.8555  loss_dice_2: 1.039  loss_ce_3: 0.3621  loss_cate_3: 0  loss_mask_3: 0.8695  loss_dice_3: 1.047  loss_ce_4: 0.3408  loss_cate_4: 0  loss_mask_4: 0.849  loss_dice_4: 1.059  loss_ce_5: 0.3073  loss_cate_5: 0  loss_mask_5: 0.8998  loss_dice_5: 1.059  loss_ce_6: 0.3198  loss_cate_6: 0  loss_mask_6: 0.9186  loss_dice_6: 1.063  loss_ce_7: 0.3388  loss_cate_7: 0  loss_mask_7: 0.9104  loss_dice_7: 1.085  loss_ce_8: 0.3004  loss_cate_8: 0  loss_mask_8: 0.914  loss_dice_8: 1.072  time: 1.1019  data_time: 0.0130  lr: 9.4064e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:43:48 d2.utils.events]: \u001b[0m eta: 22:47:41  iter: 5279  total_loss: 19.56  loss_ce: 0.1981  loss_cate: 0.1517  loss_mask: 0.7139  loss_dice: 0.9356  loss_ce_0: 0.3752  loss_cate_0: 0  loss_mask_0: 0.7486  loss_dice_0: 0.9282  loss_ce_1: 0.2739  loss_cate_1: 0  loss_mask_1: 0.7355  loss_dice_1: 0.9212  loss_ce_2: 0.2139  loss_cate_2: 0  loss_mask_2: 0.7568  loss_dice_2: 0.9514  loss_ce_3: 0.2087  loss_cate_3: 0  loss_mask_3: 0.7587  loss_dice_3: 0.9515  loss_ce_4: 0.1984  loss_cate_4: 0  loss_mask_4: 0.7593  loss_dice_4: 0.9304  loss_ce_5: 0.2314  loss_cate_5: 0  loss_mask_5: 0.7359  loss_dice_5: 0.9142  loss_ce_6: 0.2139  loss_cate_6: 0  loss_mask_6: 0.7179  loss_dice_6: 0.9096  loss_ce_7: 0.1959  loss_cate_7: 0  loss_mask_7: 0.7492  loss_dice_7: 0.9223  loss_ce_8: 0.179  loss_cate_8: 0  loss_mask_8: 0.7327  loss_dice_8: 0.9609  time: 1.1019  data_time: 0.0137  lr: 9.4041e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:44:13 d2.utils.events]: \u001b[0m eta: 22:47:16  iter: 5299  total_loss: 23.29  loss_ce: 0.2612  loss_cate: 0.2027  loss_mask: 0.8518  loss_dice: 1.115  loss_ce_0: 0.415  loss_cate_0: 0  loss_mask_0: 0.8743  loss_dice_0: 1.172  loss_ce_1: 0.296  loss_cate_1: 0  loss_mask_1: 0.8702  loss_dice_1: 1.109  loss_ce_2: 0.215  loss_cate_2: 0  loss_mask_2: 0.9406  loss_dice_2: 1.145  loss_ce_3: 0.2854  loss_cate_3: 0  loss_mask_3: 0.9332  loss_dice_3: 1.09  loss_ce_4: 0.2516  loss_cate_4: 0  loss_mask_4: 0.8632  loss_dice_4: 1.1  loss_ce_5: 0.2795  loss_cate_5: 0  loss_mask_5: 0.8833  loss_dice_5: 1.107  loss_ce_6: 0.3004  loss_cate_6: 0  loss_mask_6: 0.8745  loss_dice_6: 1.089  loss_ce_7: 0.2919  loss_cate_7: 0  loss_mask_7: 0.8616  loss_dice_7: 1.099  loss_ce_8: 0.281  loss_cate_8: 0  loss_mask_8: 0.8697  loss_dice_8: 1.086  time: 1.1019  data_time: 0.0133  lr: 9.4018e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:44:35 d2.utils.events]: \u001b[0m eta: 22:46:52  iter: 5319  total_loss: 20.93  loss_ce: 0.2097  loss_cate: 0.1673  loss_mask: 0.8618  loss_dice: 1.005  loss_ce_0: 0.34  loss_cate_0: 0  loss_mask_0: 0.8461  loss_dice_0: 1.073  loss_ce_1: 0.2615  loss_cate_1: 0  loss_mask_1: 0.8177  loss_dice_1: 1.02  loss_ce_2: 0.2033  loss_cate_2: 0  loss_mask_2: 0.8314  loss_dice_2: 1.022  loss_ce_3: 0.2225  loss_cate_3: 0  loss_mask_3: 0.8404  loss_dice_3: 1.005  loss_ce_4: 0.212  loss_cate_4: 0  loss_mask_4: 0.8433  loss_dice_4: 1.027  loss_ce_5: 0.2029  loss_cate_5: 0  loss_mask_5: 0.8608  loss_dice_5: 1.035  loss_ce_6: 0.1683  loss_cate_6: 0  loss_mask_6: 0.8539  loss_dice_6: 1.027  loss_ce_7: 0.1787  loss_cate_7: 0  loss_mask_7: 0.8432  loss_dice_7: 1.005  loss_ce_8: 0.1526  loss_cate_8: 0  loss_mask_8: 0.8512  loss_dice_8: 0.9833  time: 1.1019  data_time: 0.0136  lr: 9.3996e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:44:57 d2.utils.events]: \u001b[0m eta: 22:46:25  iter: 5339  total_loss: 22.24  loss_ce: 0.2259  loss_cate: 0.1637  loss_mask: 0.784  loss_dice: 1.029  loss_ce_0: 0.3818  loss_cate_0: 0  loss_mask_0: 0.8198  loss_dice_0: 1.096  loss_ce_1: 0.325  loss_cate_1: 0  loss_mask_1: 0.8098  loss_dice_1: 1.097  loss_ce_2: 0.4133  loss_cate_2: 0  loss_mask_2: 0.7811  loss_dice_2: 0.9604  loss_ce_3: 0.2675  loss_cate_3: 0  loss_mask_3: 0.7862  loss_dice_3: 1.058  loss_ce_4: 0.2405  loss_cate_4: 0  loss_mask_4: 0.7793  loss_dice_4: 1.036  loss_ce_5: 0.254  loss_cate_5: 0  loss_mask_5: 0.8001  loss_dice_5: 1.048  loss_ce_6: 0.2304  loss_cate_6: 0  loss_mask_6: 0.8142  loss_dice_6: 1.021  loss_ce_7: 0.2071  loss_cate_7: 0  loss_mask_7: 0.8034  loss_dice_7: 1.061  loss_ce_8: 0.2167  loss_cate_8: 0  loss_mask_8: 0.7868  loss_dice_8: 1.075  time: 1.1019  data_time: 0.0121  lr: 9.3973e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:45:20 d2.utils.events]: \u001b[0m eta: 22:46:00  iter: 5359  total_loss: 20.39  loss_ce: 0.2229  loss_cate: 0.181  loss_mask: 0.7733  loss_dice: 0.9549  loss_ce_0: 0.3467  loss_cate_0: 0  loss_mask_0: 0.7927  loss_dice_0: 1.085  loss_ce_1: 0.2291  loss_cate_1: 0  loss_mask_1: 0.7886  loss_dice_1: 1.081  loss_ce_2: 0.2321  loss_cate_2: 0  loss_mask_2: 0.7707  loss_dice_2: 1.021  loss_ce_3: 0.2802  loss_cate_3: 0  loss_mask_3: 0.7651  loss_dice_3: 0.9623  loss_ce_4: 0.272  loss_cate_4: 0  loss_mask_4: 0.7894  loss_dice_4: 0.9955  loss_ce_5: 0.3273  loss_cate_5: 0  loss_mask_5: 0.7793  loss_dice_5: 1.024  loss_ce_6: 0.2984  loss_cate_6: 0  loss_mask_6: 0.8012  loss_dice_6: 1.006  loss_ce_7: 0.2503  loss_cate_7: 0  loss_mask_7: 0.7961  loss_dice_7: 1.04  loss_ce_8: 0.2006  loss_cate_8: 0  loss_mask_8: 0.7774  loss_dice_8: 1.044  time: 1.1019  data_time: 0.0146  lr: 9.395e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:45:42 d2.utils.events]: \u001b[0m eta: 22:45:42  iter: 5379  total_loss: 22.7  loss_ce: 0.2753  loss_cate: 0.1861  loss_mask: 0.8757  loss_dice: 1.018  loss_ce_0: 0.3974  loss_cate_0: 0  loss_mask_0: 0.9127  loss_dice_0: 1.144  loss_ce_1: 0.2103  loss_cate_1: 0  loss_mask_1: 0.8859  loss_dice_1: 1.103  loss_ce_2: 0.2696  loss_cate_2: 0  loss_mask_2: 0.8893  loss_dice_2: 0.9863  loss_ce_3: 0.2527  loss_cate_3: 0  loss_mask_3: 0.8728  loss_dice_3: 1.007  loss_ce_4: 0.2628  loss_cate_4: 0  loss_mask_4: 0.872  loss_dice_4: 1.057  loss_ce_5: 0.2669  loss_cate_5: 0  loss_mask_5: 0.8987  loss_dice_5: 1.043  loss_ce_6: 0.2317  loss_cate_6: 0  loss_mask_6: 0.8819  loss_dice_6: 1.008  loss_ce_7: 0.2629  loss_cate_7: 0  loss_mask_7: 0.8897  loss_dice_7: 1.088  loss_ce_8: 0.2582  loss_cate_8: 0  loss_mask_8: 0.8892  loss_dice_8: 1.031  time: 1.1019  data_time: 0.0131  lr: 9.3928e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:46:04 d2.utils.events]: \u001b[0m eta: 22:45:27  iter: 5399  total_loss: 22.92  loss_ce: 0.3014  loss_cate: 0.1659  loss_mask: 0.8578  loss_dice: 1.126  loss_ce_0: 0.3692  loss_cate_0: 0  loss_mask_0: 0.8714  loss_dice_0: 1.215  loss_ce_1: 0.3242  loss_cate_1: 0  loss_mask_1: 0.8986  loss_dice_1: 1.08  loss_ce_2: 0.3097  loss_cate_2: 0  loss_mask_2: 0.8655  loss_dice_2: 1.14  loss_ce_3: 0.3088  loss_cate_3: 0  loss_mask_3: 0.8623  loss_dice_3: 1.042  loss_ce_4: 0.3145  loss_cate_4: 0  loss_mask_4: 0.8988  loss_dice_4: 1.054  loss_ce_5: 0.3016  loss_cate_5: 0  loss_mask_5: 0.8786  loss_dice_5: 1.121  loss_ce_6: 0.2848  loss_cate_6: 0  loss_mask_6: 0.844  loss_dice_6: 1.109  loss_ce_7: 0.3086  loss_cate_7: 0  loss_mask_7: 0.8651  loss_dice_7: 1.099  loss_ce_8: 0.3253  loss_cate_8: 0  loss_mask_8: 0.8585  loss_dice_8: 1.133  time: 1.1019  data_time: 0.0134  lr: 9.3905e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:46:27 d2.utils.events]: \u001b[0m eta: 22:45:09  iter: 5419  total_loss: 20.87  loss_ce: 0.2367  loss_cate: 0.1848  loss_mask: 0.849  loss_dice: 0.9944  loss_ce_0: 0.3195  loss_cate_0: 0  loss_mask_0: 0.8341  loss_dice_0: 1.041  loss_ce_1: 0.2299  loss_cate_1: 0  loss_mask_1: 0.8459  loss_dice_1: 1.015  loss_ce_2: 0.2105  loss_cate_2: 0  loss_mask_2: 0.8317  loss_dice_2: 1.044  loss_ce_3: 0.2256  loss_cate_3: 0  loss_mask_3: 0.8601  loss_dice_3: 0.9895  loss_ce_4: 0.2367  loss_cate_4: 0  loss_mask_4: 0.8464  loss_dice_4: 1.018  loss_ce_5: 0.2364  loss_cate_5: 0  loss_mask_5: 0.8499  loss_dice_5: 0.9962  loss_ce_6: 0.2098  loss_cate_6: 0  loss_mask_6: 0.8479  loss_dice_6: 1.004  loss_ce_7: 0.2279  loss_cate_7: 0  loss_mask_7: 0.8378  loss_dice_7: 1.015  loss_ce_8: 0.2243  loss_cate_8: 0  loss_mask_8: 0.8481  loss_dice_8: 0.9973  time: 1.1019  data_time: 0.0126  lr: 9.3882e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:46:49 d2.utils.events]: \u001b[0m eta: 22:45:00  iter: 5439  total_loss: 23.95  loss_ce: 0.2525  loss_cate: 0.174  loss_mask: 0.8148  loss_dice: 1.185  loss_ce_0: 0.3805  loss_cate_0: 0  loss_mask_0: 0.8128  loss_dice_0: 1.261  loss_ce_1: 0.2403  loss_cate_1: 0  loss_mask_1: 0.8587  loss_dice_1: 1.201  loss_ce_2: 0.2681  loss_cate_2: 0  loss_mask_2: 0.837  loss_dice_2: 1.196  loss_ce_3: 0.248  loss_cate_3: 0  loss_mask_3: 0.8574  loss_dice_3: 1.231  loss_ce_4: 0.2928  loss_cate_4: 0  loss_mask_4: 0.8418  loss_dice_4: 1.178  loss_ce_5: 0.2927  loss_cate_5: 0  loss_mask_5: 0.825  loss_dice_5: 1.232  loss_ce_6: 0.2581  loss_cate_6: 0  loss_mask_6: 0.8676  loss_dice_6: 1.136  loss_ce_7: 0.2303  loss_cate_7: 0  loss_mask_7: 0.8478  loss_dice_7: 1.16  loss_ce_8: 0.2679  loss_cate_8: 0  loss_mask_8: 0.817  loss_dice_8: 1.153  time: 1.1019  data_time: 0.0128  lr: 9.386e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:47:11 d2.utils.events]: \u001b[0m eta: 22:44:38  iter: 5459  total_loss: 23.23  loss_ce: 0.2791  loss_cate: 0.1937  loss_mask: 0.9333  loss_dice: 1.038  loss_ce_0: 0.3574  loss_cate_0: 0  loss_mask_0: 0.9985  loss_dice_0: 1.161  loss_ce_1: 0.3858  loss_cate_1: 0  loss_mask_1: 0.9217  loss_dice_1: 1.029  loss_ce_2: 0.3886  loss_cate_2: 0  loss_mask_2: 0.9458  loss_dice_2: 1.021  loss_ce_3: 0.3372  loss_cate_3: 0  loss_mask_3: 0.9067  loss_dice_3: 1.048  loss_ce_4: 0.3038  loss_cate_4: 0  loss_mask_4: 0.9086  loss_dice_4: 1.069  loss_ce_5: 0.3129  loss_cate_5: 0  loss_mask_5: 0.9208  loss_dice_5: 1.038  loss_ce_6: 0.2697  loss_cate_6: 0  loss_mask_6: 0.9539  loss_dice_6: 1.053  loss_ce_7: 0.2711  loss_cate_7: 0  loss_mask_7: 0.9025  loss_dice_7: 1.008  loss_ce_8: 0.2815  loss_cate_8: 0  loss_mask_8: 0.9167  loss_dice_8: 1.043  time: 1.1019  data_time: 0.0125  lr: 9.3837e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:47:34 d2.utils.events]: \u001b[0m eta: 22:44:31  iter: 5479  total_loss: 22.14  loss_ce: 0.1732  loss_cate: 0.1668  loss_mask: 0.8914  loss_dice: 1.027  loss_ce_0: 0.412  loss_cate_0: 0  loss_mask_0: 0.9168  loss_dice_0: 1.098  loss_ce_1: 0.273  loss_cate_1: 0  loss_mask_1: 0.9079  loss_dice_1: 1.063  loss_ce_2: 0.2366  loss_cate_2: 0  loss_mask_2: 0.8895  loss_dice_2: 1.015  loss_ce_3: 0.203  loss_cate_3: 0  loss_mask_3: 0.9085  loss_dice_3: 1.059  loss_ce_4: 0.1665  loss_cate_4: 0  loss_mask_4: 0.8928  loss_dice_4: 1.104  loss_ce_5: 0.1413  loss_cate_5: 0  loss_mask_5: 0.8972  loss_dice_5: 1.093  loss_ce_6: 0.1639  loss_cate_6: 0  loss_mask_6: 0.8813  loss_dice_6: 1.059  loss_ce_7: 0.154  loss_cate_7: 0  loss_mask_7: 0.8954  loss_dice_7: 1.05  loss_ce_8: 0.1329  loss_cate_8: 0  loss_mask_8: 0.9001  loss_dice_8: 1.069  time: 1.1019  data_time: 0.0138  lr: 9.3814e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:47:57 d2.utils.events]: \u001b[0m eta: 22:44:17  iter: 5499  total_loss: 21.74  loss_ce: 0.2381  loss_cate: 0.1836  loss_mask: 0.8535  loss_dice: 1.02  loss_ce_0: 0.386  loss_cate_0: 0  loss_mask_0: 0.9388  loss_dice_0: 1.082  loss_ce_1: 0.3322  loss_cate_1: 0  loss_mask_1: 0.8708  loss_dice_1: 1.077  loss_ce_2: 0.29  loss_cate_2: 0  loss_mask_2: 0.8527  loss_dice_2: 1.01  loss_ce_3: 0.3148  loss_cate_3: 0  loss_mask_3: 0.8011  loss_dice_3: 0.9454  loss_ce_4: 0.3166  loss_cate_4: 0  loss_mask_4: 0.8024  loss_dice_4: 0.9938  loss_ce_5: 0.3116  loss_cate_5: 0  loss_mask_5: 0.814  loss_dice_5: 0.9749  loss_ce_6: 0.282  loss_cate_6: 0  loss_mask_6: 0.8383  loss_dice_6: 0.994  loss_ce_7: 0.2806  loss_cate_7: 0  loss_mask_7: 0.8512  loss_dice_7: 1.015  loss_ce_8: 0.2615  loss_cate_8: 0  loss_mask_8: 0.8509  loss_dice_8: 1.016  time: 1.1019  data_time: 0.0146  lr: 9.3792e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:48:19 d2.utils.events]: \u001b[0m eta: 22:44:01  iter: 5519  total_loss: 20.45  loss_ce: 0.1541  loss_cate: 0.1663  loss_mask: 0.878  loss_dice: 0.9614  loss_ce_0: 0.2598  loss_cate_0: 0  loss_mask_0: 0.8846  loss_dice_0: 1.019  loss_ce_1: 0.2076  loss_cate_1: 0  loss_mask_1: 0.8472  loss_dice_1: 0.9885  loss_ce_2: 0.2217  loss_cate_2: 0  loss_mask_2: 0.8954  loss_dice_2: 0.9636  loss_ce_3: 0.2088  loss_cate_3: 0  loss_mask_3: 0.8864  loss_dice_3: 0.9338  loss_ce_4: 0.1758  loss_cate_4: 0  loss_mask_4: 0.8973  loss_dice_4: 0.9583  loss_ce_5: 0.1611  loss_cate_5: 0  loss_mask_5: 0.8771  loss_dice_5: 0.9474  loss_ce_6: 0.1473  loss_cate_6: 0  loss_mask_6: 0.8755  loss_dice_6: 0.9505  loss_ce_7: 0.1406  loss_cate_7: 0  loss_mask_7: 0.879  loss_dice_7: 0.9843  loss_ce_8: 0.1811  loss_cate_8: 0  loss_mask_8: 0.8997  loss_dice_8: 0.9527  time: 1.1020  data_time: 0.0139  lr: 9.3769e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:48:42 d2.utils.events]: \u001b[0m eta: 22:43:53  iter: 5539  total_loss: 21.51  loss_ce: 0.2424  loss_cate: 0.1699  loss_mask: 0.9054  loss_dice: 0.9499  loss_ce_0: 0.3935  loss_cate_0: 0  loss_mask_0: 0.9183  loss_dice_0: 0.9685  loss_ce_1: 0.2277  loss_cate_1: 0  loss_mask_1: 0.9381  loss_dice_1: 1.017  loss_ce_2: 0.2259  loss_cate_2: 0  loss_mask_2: 0.8781  loss_dice_2: 0.9774  loss_ce_3: 0.2263  loss_cate_3: 0  loss_mask_3: 0.9102  loss_dice_3: 0.9634  loss_ce_4: 0.239  loss_cate_4: 0  loss_mask_4: 0.893  loss_dice_4: 0.94  loss_ce_5: 0.2391  loss_cate_5: 0  loss_mask_5: 0.915  loss_dice_5: 0.9886  loss_ce_6: 0.2148  loss_cate_6: 0  loss_mask_6: 0.8998  loss_dice_6: 0.95  loss_ce_7: 0.2416  loss_cate_7: 0  loss_mask_7: 0.917  loss_dice_7: 0.9415  loss_ce_8: 0.2663  loss_cate_8: 0  loss_mask_8: 0.9098  loss_dice_8: 0.9794  time: 1.1020  data_time: 0.0131  lr: 9.3746e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:49:04 d2.utils.events]: \u001b[0m eta: 22:43:31  iter: 5559  total_loss: 21.1  loss_ce: 0.1699  loss_cate: 0.1928  loss_mask: 0.8708  loss_dice: 1.035  loss_ce_0: 0.3853  loss_cate_0: 0  loss_mask_0: 0.877  loss_dice_0: 1.068  loss_ce_1: 0.1875  loss_cate_1: 0  loss_mask_1: 0.9015  loss_dice_1: 1.133  loss_ce_2: 0.2159  loss_cate_2: 0  loss_mask_2: 0.8743  loss_dice_2: 1.018  loss_ce_3: 0.215  loss_cate_3: 0  loss_mask_3: 0.8622  loss_dice_3: 1.026  loss_ce_4: 0.1783  loss_cate_4: 0  loss_mask_4: 0.9057  loss_dice_4: 1.026  loss_ce_5: 0.1945  loss_cate_5: 0  loss_mask_5: 0.8341  loss_dice_5: 1.047  loss_ce_6: 0.1333  loss_cate_6: 0  loss_mask_6: 0.8775  loss_dice_6: 1.072  loss_ce_7: 0.1786  loss_cate_7: 0  loss_mask_7: 0.875  loss_dice_7: 1.045  loss_ce_8: 0.1074  loss_cate_8: 0  loss_mask_8: 0.8705  loss_dice_8: 1.074  time: 1.1020  data_time: 0.0124  lr: 9.3724e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:49:26 d2.utils.events]: \u001b[0m eta: 22:43:14  iter: 5579  total_loss: 22.45  loss_ce: 0.2547  loss_cate: 0.163  loss_mask: 0.8459  loss_dice: 1.064  loss_ce_0: 0.3846  loss_cate_0: 0  loss_mask_0: 0.8582  loss_dice_0: 1.109  loss_ce_1: 0.2738  loss_cate_1: 0  loss_mask_1: 0.8431  loss_dice_1: 1.098  loss_ce_2: 0.2739  loss_cate_2: 0  loss_mask_2: 0.8607  loss_dice_2: 1.092  loss_ce_3: 0.2675  loss_cate_3: 0  loss_mask_3: 0.8623  loss_dice_3: 1.033  loss_ce_4: 0.2568  loss_cate_4: 0  loss_mask_4: 0.8667  loss_dice_4: 1.023  loss_ce_5: 0.2565  loss_cate_5: 0  loss_mask_5: 0.8535  loss_dice_5: 1.079  loss_ce_6: 0.2434  loss_cate_6: 0  loss_mask_6: 0.8468  loss_dice_6: 1.078  loss_ce_7: 0.2352  loss_cate_7: 0  loss_mask_7: 0.8455  loss_dice_7: 1.035  loss_ce_8: 0.2481  loss_cate_8: 0  loss_mask_8: 0.8537  loss_dice_8: 1.081  time: 1.1020  data_time: 0.0122  lr: 9.3701e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:49:48 d2.utils.events]: \u001b[0m eta: 22:42:56  iter: 5599  total_loss: 22.97  loss_ce: 0.2636  loss_cate: 0.1584  loss_mask: 0.9068  loss_dice: 1.053  loss_ce_0: 0.3711  loss_cate_0: 0  loss_mask_0: 0.8768  loss_dice_0: 1.087  loss_ce_1: 0.2333  loss_cate_1: 0  loss_mask_1: 0.8917  loss_dice_1: 1.042  loss_ce_2: 0.283  loss_cate_2: 0  loss_mask_2: 0.865  loss_dice_2: 1.089  loss_ce_3: 0.2051  loss_cate_3: 0  loss_mask_3: 0.8789  loss_dice_3: 1.067  loss_ce_4: 0.2111  loss_cate_4: 0  loss_mask_4: 0.8847  loss_dice_4: 1.049  loss_ce_5: 0.1821  loss_cate_5: 0  loss_mask_5: 0.8768  loss_dice_5: 1.054  loss_ce_6: 0.2422  loss_cate_6: 0  loss_mask_6: 0.8473  loss_dice_6: 1.022  loss_ce_7: 0.2336  loss_cate_7: 0  loss_mask_7: 0.8629  loss_dice_7: 1.03  loss_ce_8: 0.2062  loss_cate_8: 0  loss_mask_8: 0.8881  loss_dice_8: 1.057  time: 1.1020  data_time: 0.0128  lr: 9.3678e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:50:11 d2.utils.events]: \u001b[0m eta: 22:42:45  iter: 5619  total_loss: 21.8  loss_ce: 0.2635  loss_cate: 0.1612  loss_mask: 0.8355  loss_dice: 0.9522  loss_ce_0: 0.3989  loss_cate_0: 0  loss_mask_0: 0.8603  loss_dice_0: 1.044  loss_ce_1: 0.2845  loss_cate_1: 0  loss_mask_1: 0.8822  loss_dice_1: 0.9767  loss_ce_2: 0.2571  loss_cate_2: 0  loss_mask_2: 0.8879  loss_dice_2: 0.9701  loss_ce_3: 0.2768  loss_cate_3: 0  loss_mask_3: 0.884  loss_dice_3: 0.9521  loss_ce_4: 0.2617  loss_cate_4: 0  loss_mask_4: 0.8972  loss_dice_4: 0.972  loss_ce_5: 0.2542  loss_cate_5: 0  loss_mask_5: 0.8467  loss_dice_5: 0.9722  loss_ce_6: 0.2529  loss_cate_6: 0  loss_mask_6: 0.8937  loss_dice_6: 0.9584  loss_ce_7: 0.244  loss_cate_7: 0  loss_mask_7: 0.8946  loss_dice_7: 0.9757  loss_ce_8: 0.2476  loss_cate_8: 0  loss_mask_8: 0.8496  loss_dice_8: 0.9643  time: 1.1020  data_time: 0.0125  lr: 9.3656e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:50:33 d2.utils.events]: \u001b[0m eta: 22:42:28  iter: 5639  total_loss: 22.57  loss_ce: 0.2576  loss_cate: 0.1483  loss_mask: 0.7885  loss_dice: 1.118  loss_ce_0: 0.4059  loss_cate_0: 0  loss_mask_0: 0.8627  loss_dice_0: 1.196  loss_ce_1: 0.2693  loss_cate_1: 0  loss_mask_1: 0.8465  loss_dice_1: 1.091  loss_ce_2: 0.2251  loss_cate_2: 0  loss_mask_2: 0.8161  loss_dice_2: 1.116  loss_ce_3: 0.2934  loss_cate_3: 0  loss_mask_3: 0.814  loss_dice_3: 1.131  loss_ce_4: 0.2628  loss_cate_4: 0  loss_mask_4: 0.8128  loss_dice_4: 1.082  loss_ce_5: 0.2926  loss_cate_5: 0  loss_mask_5: 0.8091  loss_dice_5: 1.092  loss_ce_6: 0.2557  loss_cate_6: 0  loss_mask_6: 0.7782  loss_dice_6: 1.042  loss_ce_7: 0.2523  loss_cate_7: 0  loss_mask_7: 0.8037  loss_dice_7: 1.056  loss_ce_8: 0.2237  loss_cate_8: 0  loss_mask_8: 0.787  loss_dice_8: 1.109  time: 1.1020  data_time: 0.0134  lr: 9.3633e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:50:56 d2.utils.events]: \u001b[0m eta: 22:42:13  iter: 5659  total_loss: 21.99  loss_ce: 0.2639  loss_cate: 0.146  loss_mask: 0.7779  loss_dice: 0.9932  loss_ce_0: 0.4577  loss_cate_0: 0  loss_mask_0: 0.743  loss_dice_0: 0.9916  loss_ce_1: 0.3007  loss_cate_1: 0  loss_mask_1: 0.775  loss_dice_1: 1.055  loss_ce_2: 0.3111  loss_cate_2: 0  loss_mask_2: 0.7626  loss_dice_2: 0.9999  loss_ce_3: 0.247  loss_cate_3: 0  loss_mask_3: 0.7958  loss_dice_3: 0.9953  loss_ce_4: 0.2503  loss_cate_4: 0  loss_mask_4: 0.7684  loss_dice_4: 1.042  loss_ce_5: 0.266  loss_cate_5: 0  loss_mask_5: 0.8032  loss_dice_5: 1.004  loss_ce_6: 0.2607  loss_cate_6: 0  loss_mask_6: 0.7952  loss_dice_6: 0.9706  loss_ce_7: 0.244  loss_cate_7: 0  loss_mask_7: 0.8185  loss_dice_7: 0.9896  loss_ce_8: 0.2189  loss_cate_8: 0  loss_mask_8: 0.7984  loss_dice_8: 0.9956  time: 1.1020  data_time: 0.0136  lr: 9.3611e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:51:19 d2.utils.events]: \u001b[0m eta: 22:41:53  iter: 5679  total_loss: 23.7  loss_ce: 0.2039  loss_cate: 0.1697  loss_mask: 0.9696  loss_dice: 1.14  loss_ce_0: 0.4246  loss_cate_0: 0  loss_mask_0: 0.932  loss_dice_0: 1.106  loss_ce_1: 0.2825  loss_cate_1: 0  loss_mask_1: 0.9599  loss_dice_1: 1.123  loss_ce_2: 0.2886  loss_cate_2: 0  loss_mask_2: 0.9758  loss_dice_2: 1.131  loss_ce_3: 0.2972  loss_cate_3: 0  loss_mask_3: 0.9393  loss_dice_3: 1.108  loss_ce_4: 0.2486  loss_cate_4: 0  loss_mask_4: 0.9092  loss_dice_4: 1.12  loss_ce_5: 0.199  loss_cate_5: 0  loss_mask_5: 0.9528  loss_dice_5: 1.12  loss_ce_6: 0.2011  loss_cate_6: 0  loss_mask_6: 0.9223  loss_dice_6: 1.136  loss_ce_7: 0.1914  loss_cate_7: 0  loss_mask_7: 0.9135  loss_dice_7: 1.137  loss_ce_8: 0.2314  loss_cate_8: 0  loss_mask_8: 0.9265  loss_dice_8: 1.123  time: 1.1020  data_time: 0.0128  lr: 9.3588e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:51:41 d2.utils.events]: \u001b[0m eta: 22:41:32  iter: 5699  total_loss: 22.03  loss_ce: 0.2016  loss_cate: 0.1638  loss_mask: 0.8852  loss_dice: 1.177  loss_ce_0: 0.3206  loss_cate_0: 0  loss_mask_0: 0.8773  loss_dice_0: 1.158  loss_ce_1: 0.2537  loss_cate_1: 0  loss_mask_1: 0.8701  loss_dice_1: 1.107  loss_ce_2: 0.3006  loss_cate_2: 0  loss_mask_2: 0.779  loss_dice_2: 1.073  loss_ce_3: 0.2603  loss_cate_3: 0  loss_mask_3: 0.7851  loss_dice_3: 1.107  loss_ce_4: 0.2446  loss_cate_4: 0  loss_mask_4: 0.795  loss_dice_4: 1.093  loss_ce_5: 0.2371  loss_cate_5: 0  loss_mask_5: 0.7993  loss_dice_5: 1.089  loss_ce_6: 0.2741  loss_cate_6: 0  loss_mask_6: 0.8693  loss_dice_6: 1.14  loss_ce_7: 0.2534  loss_cate_7: 0  loss_mask_7: 0.8475  loss_dice_7: 1.111  loss_ce_8: 0.2357  loss_cate_8: 0  loss_mask_8: 0.8392  loss_dice_8: 1.123  time: 1.1020  data_time: 0.0122  lr: 9.3565e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:52:04 d2.utils.events]: \u001b[0m eta: 22:41:09  iter: 5719  total_loss: 22.35  loss_ce: 0.2571  loss_cate: 0.1615  loss_mask: 0.9277  loss_dice: 1.138  loss_ce_0: 0.4048  loss_cate_0: 0  loss_mask_0: 0.8336  loss_dice_0: 1.163  loss_ce_1: 0.2604  loss_cate_1: 0  loss_mask_1: 0.9031  loss_dice_1: 1.1  loss_ce_2: 0.3193  loss_cate_2: 0  loss_mask_2: 0.9197  loss_dice_2: 1.1  loss_ce_3: 0.2916  loss_cate_3: 0  loss_mask_3: 0.9247  loss_dice_3: 1.127  loss_ce_4: 0.2999  loss_cate_4: 0  loss_mask_4: 0.9287  loss_dice_4: 1.151  loss_ce_5: 0.2764  loss_cate_5: 0  loss_mask_5: 0.9381  loss_dice_5: 1.139  loss_ce_6: 0.2112  loss_cate_6: 0  loss_mask_6: 0.9186  loss_dice_6: 1.179  loss_ce_7: 0.2426  loss_cate_7: 0  loss_mask_7: 0.9289  loss_dice_7: 1.153  loss_ce_8: 0.2178  loss_cate_8: 0  loss_mask_8: 0.9332  loss_dice_8: 1.113  time: 1.1020  data_time: 0.0140  lr: 9.3543e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:52:26 d2.utils.events]: \u001b[0m eta: 22:40:53  iter: 5739  total_loss: 23.32  loss_ce: 0.2348  loss_cate: 0.1821  loss_mask: 0.859  loss_dice: 1.026  loss_ce_0: 0.3984  loss_cate_0: 0  loss_mask_0: 0.9561  loss_dice_0: 1.055  loss_ce_1: 0.2958  loss_cate_1: 0  loss_mask_1: 0.7812  loss_dice_1: 1.036  loss_ce_2: 0.3014  loss_cate_2: 0  loss_mask_2: 0.7783  loss_dice_2: 1.096  loss_ce_3: 0.3242  loss_cate_3: 0  loss_mask_3: 0.7618  loss_dice_3: 1.012  loss_ce_4: 0.3309  loss_cate_4: 0  loss_mask_4: 0.7795  loss_dice_4: 0.9788  loss_ce_5: 0.2349  loss_cate_5: 0  loss_mask_5: 0.8308  loss_dice_5: 1.033  loss_ce_6: 0.2426  loss_cate_6: 0  loss_mask_6: 0.8168  loss_dice_6: 1.01  loss_ce_7: 0.2374  loss_cate_7: 0  loss_mask_7: 0.8043  loss_dice_7: 1.038  loss_ce_8: 0.2339  loss_cate_8: 0  loss_mask_8: 0.8204  loss_dice_8: 1.039  time: 1.1020  data_time: 0.0133  lr: 9.352e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:52:49 d2.utils.events]: \u001b[0m eta: 22:40:42  iter: 5759  total_loss: 25.54  loss_ce: 0.3511  loss_cate: 0.1946  loss_mask: 1.045  loss_dice: 1.088  loss_ce_0: 0.5181  loss_cate_0: 0  loss_mask_0: 1.051  loss_dice_0: 1.151  loss_ce_1: 0.4422  loss_cate_1: 0  loss_mask_1: 1.034  loss_dice_1: 1.164  loss_ce_2: 0.2945  loss_cate_2: 0  loss_mask_2: 0.9673  loss_dice_2: 1.153  loss_ce_3: 0.3595  loss_cate_3: 0  loss_mask_3: 1.006  loss_dice_3: 1.077  loss_ce_4: 0.3891  loss_cate_4: 0  loss_mask_4: 0.9933  loss_dice_4: 1.129  loss_ce_5: 0.3524  loss_cate_5: 0  loss_mask_5: 0.9786  loss_dice_5: 1.117  loss_ce_6: 0.3488  loss_cate_6: 0  loss_mask_6: 1.008  loss_dice_6: 1.082  loss_ce_7: 0.3623  loss_cate_7: 0  loss_mask_7: 1.002  loss_dice_7: 1.116  loss_ce_8: 0.3532  loss_cate_8: 0  loss_mask_8: 1.045  loss_dice_8: 1.105  time: 1.1020  data_time: 0.0154  lr: 9.3497e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:53:11 d2.utils.events]: \u001b[0m eta: 22:40:24  iter: 5779  total_loss: 21.93  loss_ce: 0.2315  loss_cate: 0.1672  loss_mask: 0.8958  loss_dice: 1.05  loss_ce_0: 0.3712  loss_cate_0: 0  loss_mask_0: 0.9357  loss_dice_0: 1.091  loss_ce_1: 0.2762  loss_cate_1: 0  loss_mask_1: 0.9211  loss_dice_1: 1.058  loss_ce_2: 0.2769  loss_cate_2: 0  loss_mask_2: 0.8601  loss_dice_2: 1.069  loss_ce_3: 0.2481  loss_cate_3: 0  loss_mask_3: 0.8908  loss_dice_3: 1.031  loss_ce_4: 0.2588  loss_cate_4: 0  loss_mask_4: 0.8907  loss_dice_4: 1.027  loss_ce_5: 0.2395  loss_cate_5: 0  loss_mask_5: 0.881  loss_dice_5: 1.028  loss_ce_6: 0.2463  loss_cate_6: 0  loss_mask_6: 0.8924  loss_dice_6: 1.055  loss_ce_7: 0.218  loss_cate_7: 0  loss_mask_7: 0.8854  loss_dice_7: 1.059  loss_ce_8: 0.2344  loss_cate_8: 0  loss_mask_8: 0.9016  loss_dice_8: 1.054  time: 1.1020  data_time: 0.0140  lr: 9.3474e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:53:33 d2.utils.events]: \u001b[0m eta: 22:40:05  iter: 5799  total_loss: 25.92  loss_ce: 0.2086  loss_cate: 0.2089  loss_mask: 0.9379  loss_dice: 1.198  loss_ce_0: 0.3958  loss_cate_0: 0  loss_mask_0: 0.9303  loss_dice_0: 1.264  loss_ce_1: 0.3177  loss_cate_1: 0  loss_mask_1: 0.9595  loss_dice_1: 1.221  loss_ce_2: 0.2637  loss_cate_2: 0  loss_mask_2: 0.9562  loss_dice_2: 1.229  loss_ce_3: 0.2863  loss_cate_3: 0  loss_mask_3: 0.9318  loss_dice_3: 1.166  loss_ce_4: 0.2773  loss_cate_4: 0  loss_mask_4: 0.8978  loss_dice_4: 1.18  loss_ce_5: 0.2541  loss_cate_5: 0  loss_mask_5: 0.9408  loss_dice_5: 1.184  loss_ce_6: 0.3073  loss_cate_6: 0  loss_mask_6: 0.9589  loss_dice_6: 1.154  loss_ce_7: 0.2993  loss_cate_7: 0  loss_mask_7: 0.9164  loss_dice_7: 1.213  loss_ce_8: 0.2734  loss_cate_8: 0  loss_mask_8: 0.9294  loss_dice_8: 1.193  time: 1.1020  data_time: 0.0135  lr: 9.3452e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:53:56 d2.utils.events]: \u001b[0m eta: 22:39:48  iter: 5819  total_loss: 20.29  loss_ce: 0.211  loss_cate: 0.1482  loss_mask: 0.8367  loss_dice: 0.975  loss_ce_0: 0.335  loss_cate_0: 0  loss_mask_0: 0.8987  loss_dice_0: 1.025  loss_ce_1: 0.2794  loss_cate_1: 0  loss_mask_1: 0.8452  loss_dice_1: 1.014  loss_ce_2: 0.258  loss_cate_2: 0  loss_mask_2: 0.7915  loss_dice_2: 0.978  loss_ce_3: 0.243  loss_cate_3: 0  loss_mask_3: 0.8174  loss_dice_3: 0.947  loss_ce_4: 0.2215  loss_cate_4: 0  loss_mask_4: 0.8233  loss_dice_4: 0.9725  loss_ce_5: 0.2315  loss_cate_5: 0  loss_mask_5: 0.8306  loss_dice_5: 0.9794  loss_ce_6: 0.2249  loss_cate_6: 0  loss_mask_6: 0.8186  loss_dice_6: 0.9446  loss_ce_7: 0.1726  loss_cate_7: 0  loss_mask_7: 0.7916  loss_dice_7: 1.016  loss_ce_8: 0.201  loss_cate_8: 0  loss_mask_8: 0.7725  loss_dice_8: 0.9602  time: 1.1020  data_time: 0.0124  lr: 9.3429e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:54:18 d2.utils.events]: \u001b[0m eta: 22:39:28  iter: 5839  total_loss: 21.45  loss_ce: 0.2118  loss_cate: 0.1906  loss_mask: 0.8235  loss_dice: 0.9943  loss_ce_0: 0.3771  loss_cate_0: 0  loss_mask_0: 0.8741  loss_dice_0: 1.203  loss_ce_1: 0.2521  loss_cate_1: 0  loss_mask_1: 0.8326  loss_dice_1: 1.113  loss_ce_2: 0.2356  loss_cate_2: 0  loss_mask_2: 0.8123  loss_dice_2: 1.076  loss_ce_3: 0.2004  loss_cate_3: 0  loss_mask_3: 0.8112  loss_dice_3: 1.022  loss_ce_4: 0.1793  loss_cate_4: 0  loss_mask_4: 0.7979  loss_dice_4: 1.066  loss_ce_5: 0.2417  loss_cate_5: 0  loss_mask_5: 0.8289  loss_dice_5: 1.02  loss_ce_6: 0.2371  loss_cate_6: 0  loss_mask_6: 0.7995  loss_dice_6: 1.005  loss_ce_7: 0.2399  loss_cate_7: 0  loss_mask_7: 0.8317  loss_dice_7: 0.972  loss_ce_8: 0.2277  loss_cate_8: 0  loss_mask_8: 0.8283  loss_dice_8: 1.042  time: 1.1021  data_time: 0.0137  lr: 9.3406e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:54:40 d2.utils.events]: \u001b[0m eta: 22:39:02  iter: 5859  total_loss: 21  loss_ce: 0.2085  loss_cate: 0.1715  loss_mask: 0.8849  loss_dice: 0.9377  loss_ce_0: 0.3527  loss_cate_0: 0  loss_mask_0: 0.8944  loss_dice_0: 0.9038  loss_ce_1: 0.2342  loss_cate_1: 0  loss_mask_1: 0.862  loss_dice_1: 1.005  loss_ce_2: 0.2508  loss_cate_2: 0  loss_mask_2: 0.8716  loss_dice_2: 0.944  loss_ce_3: 0.1879  loss_cate_3: 0  loss_mask_3: 0.8691  loss_dice_3: 0.9305  loss_ce_4: 0.172  loss_cate_4: 0  loss_mask_4: 0.8774  loss_dice_4: 0.9364  loss_ce_5: 0.26  loss_cate_5: 0  loss_mask_5: 0.868  loss_dice_5: 0.9436  loss_ce_6: 0.229  loss_cate_6: 0  loss_mask_6: 0.8697  loss_dice_6: 0.9051  loss_ce_7: 0.1922  loss_cate_7: 0  loss_mask_7: 0.8762  loss_dice_7: 0.9409  loss_ce_8: 0.1943  loss_cate_8: 0  loss_mask_8: 0.8898  loss_dice_8: 0.9761  time: 1.1020  data_time: 0.0126  lr: 9.3384e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:55:04 d2.utils.events]: \u001b[0m eta: 22:38:36  iter: 5879  total_loss: 20.26  loss_ce: 0.2538  loss_cate: 0.1484  loss_mask: 0.7777  loss_dice: 0.9962  loss_ce_0: 0.358  loss_cate_0: 0  loss_mask_0: 0.8245  loss_dice_0: 1.045  loss_ce_1: 0.312  loss_cate_1: 0  loss_mask_1: 0.8036  loss_dice_1: 1.056  loss_ce_2: 0.3382  loss_cate_2: 0  loss_mask_2: 0.8594  loss_dice_2: 1.011  loss_ce_3: 0.2913  loss_cate_3: 0  loss_mask_3: 0.7733  loss_dice_3: 0.9701  loss_ce_4: 0.292  loss_cate_4: 0  loss_mask_4: 0.7134  loss_dice_4: 1.007  loss_ce_5: 0.2518  loss_cate_5: 0  loss_mask_5: 0.7994  loss_dice_5: 1.01  loss_ce_6: 0.237  loss_cate_6: 0  loss_mask_6: 0.7791  loss_dice_6: 0.9857  loss_ce_7: 0.2467  loss_cate_7: 0  loss_mask_7: 0.7427  loss_dice_7: 1.026  loss_ce_8: 0.2476  loss_cate_8: 0  loss_mask_8: 0.7626  loss_dice_8: 1.003  time: 1.1020  data_time: 0.0135  lr: 9.3361e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:55:26 d2.utils.events]: \u001b[0m eta: 22:38:15  iter: 5899  total_loss: 22.51  loss_ce: 0.127  loss_cate: 0.1632  loss_mask: 0.8332  loss_dice: 1.058  loss_ce_0: 0.2973  loss_cate_0: 0  loss_mask_0: 0.9068  loss_dice_0: 1.149  loss_ce_1: 0.1841  loss_cate_1: 0  loss_mask_1: 0.8884  loss_dice_1: 1.139  loss_ce_2: 0.1821  loss_cate_2: 0  loss_mask_2: 0.85  loss_dice_2: 1.073  loss_ce_3: 0.1702  loss_cate_3: 0  loss_mask_3: 0.867  loss_dice_3: 1.057  loss_ce_4: 0.2203  loss_cate_4: 0  loss_mask_4: 0.8557  loss_dice_4: 1.072  loss_ce_5: 0.1387  loss_cate_5: 0  loss_mask_5: 0.899  loss_dice_5: 1.093  loss_ce_6: 0.1391  loss_cate_6: 0  loss_mask_6: 0.9206  loss_dice_6: 1.092  loss_ce_7: 0.1499  loss_cate_7: 0  loss_mask_7: 0.9354  loss_dice_7: 1.093  loss_ce_8: 0.1486  loss_cate_8: 0  loss_mask_8: 0.8905  loss_dice_8: 1.051  time: 1.1020  data_time: 0.0133  lr: 9.3338e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:55:49 d2.utils.events]: \u001b[0m eta: 22:37:53  iter: 5919  total_loss: 21.9  loss_ce: 0.3307  loss_cate: 0.153  loss_mask: 0.8482  loss_dice: 0.9767  loss_ce_0: 0.3539  loss_cate_0: 0  loss_mask_0: 0.795  loss_dice_0: 1.139  loss_ce_1: 0.3094  loss_cate_1: 0  loss_mask_1: 0.8296  loss_dice_1: 1.057  loss_ce_2: 0.3742  loss_cate_2: 0  loss_mask_2: 0.8195  loss_dice_2: 0.9896  loss_ce_3: 0.3449  loss_cate_3: 0  loss_mask_3: 0.8207  loss_dice_3: 1.022  loss_ce_4: 0.3202  loss_cate_4: 0  loss_mask_4: 0.8112  loss_dice_4: 0.9863  loss_ce_5: 0.3159  loss_cate_5: 0  loss_mask_5: 0.8174  loss_dice_5: 0.9873  loss_ce_6: 0.3304  loss_cate_6: 0  loss_mask_6: 0.8133  loss_dice_6: 1.017  loss_ce_7: 0.3161  loss_cate_7: 0  loss_mask_7: 0.8361  loss_dice_7: 1.034  loss_ce_8: 0.3073  loss_cate_8: 0  loss_mask_8: 0.8377  loss_dice_8: 1.02  time: 1.1020  data_time: 0.0127  lr: 9.3316e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:56:12 d2.utils.events]: \u001b[0m eta: 22:37:34  iter: 5939  total_loss: 23.55  loss_ce: 0.3132  loss_cate: 0.1917  loss_mask: 0.85  loss_dice: 1.082  loss_ce_0: 0.3341  loss_cate_0: 0  loss_mask_0: 0.8737  loss_dice_0: 1.186  loss_ce_1: 0.2687  loss_cate_1: 0  loss_mask_1: 0.8448  loss_dice_1: 1.104  loss_ce_2: 0.2501  loss_cate_2: 0  loss_mask_2: 0.8735  loss_dice_2: 1.108  loss_ce_3: 0.248  loss_cate_3: 0  loss_mask_3: 0.8658  loss_dice_3: 1.106  loss_ce_4: 0.2247  loss_cate_4: 0  loss_mask_4: 0.8683  loss_dice_4: 1.13  loss_ce_5: 0.229  loss_cate_5: 0  loss_mask_5: 0.8454  loss_dice_5: 1.142  loss_ce_6: 0.2452  loss_cate_6: 0  loss_mask_6: 0.8795  loss_dice_6: 1.094  loss_ce_7: 0.2587  loss_cate_7: 0  loss_mask_7: 0.87  loss_dice_7: 1.087  loss_ce_8: 0.325  loss_cate_8: 0  loss_mask_8: 0.8506  loss_dice_8: 1.15  time: 1.1021  data_time: 0.0140  lr: 9.3293e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:56:34 d2.utils.events]: \u001b[0m eta: 22:37:14  iter: 5959  total_loss: 20.75  loss_ce: 0.18  loss_cate: 0.1651  loss_mask: 0.8359  loss_dice: 1.008  loss_ce_0: 0.3183  loss_cate_0: 0  loss_mask_0: 0.8258  loss_dice_0: 1.084  loss_ce_1: 0.2458  loss_cate_1: 0  loss_mask_1: 0.8213  loss_dice_1: 1.057  loss_ce_2: 0.2321  loss_cate_2: 0  loss_mask_2: 0.8055  loss_dice_2: 1.029  loss_ce_3: 0.2215  loss_cate_3: 0  loss_mask_3: 0.8377  loss_dice_3: 1.045  loss_ce_4: 0.2113  loss_cate_4: 0  loss_mask_4: 0.8063  loss_dice_4: 1.045  loss_ce_5: 0.1864  loss_cate_5: 0  loss_mask_5: 0.8292  loss_dice_5: 1.054  loss_ce_6: 0.2051  loss_cate_6: 0  loss_mask_6: 0.8307  loss_dice_6: 1.019  loss_ce_7: 0.1886  loss_cate_7: 0  loss_mask_7: 0.8302  loss_dice_7: 1.017  loss_ce_8: 0.201  loss_cate_8: 0  loss_mask_8: 0.8383  loss_dice_8: 1.034  time: 1.1021  data_time: 0.0136  lr: 9.327e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:56:57 d2.utils.events]: \u001b[0m eta: 22:36:52  iter: 5979  total_loss: 23.19  loss_ce: 0.2449  loss_cate: 0.1828  loss_mask: 0.9318  loss_dice: 1.157  loss_ce_0: 0.4635  loss_cate_0: 0  loss_mask_0: 0.9303  loss_dice_0: 1.177  loss_ce_1: 0.3386  loss_cate_1: 0  loss_mask_1: 0.8998  loss_dice_1: 1.09  loss_ce_2: 0.3209  loss_cate_2: 0  loss_mask_2: 0.9201  loss_dice_2: 1.097  loss_ce_3: 0.2855  loss_cate_3: 0  loss_mask_3: 0.8582  loss_dice_3: 1.064  loss_ce_4: 0.2836  loss_cate_4: 0  loss_mask_4: 0.8976  loss_dice_4: 1.07  loss_ce_5: 0.2569  loss_cate_5: 0  loss_mask_5: 0.9059  loss_dice_5: 1.074  loss_ce_6: 0.2843  loss_cate_6: 0  loss_mask_6: 0.9279  loss_dice_6: 1.084  loss_ce_7: 0.2919  loss_cate_7: 0  loss_mask_7: 0.9625  loss_dice_7: 1.129  loss_ce_8: 0.2643  loss_cate_8: 0  loss_mask_8: 0.9523  loss_dice_8: 1.175  time: 1.1020  data_time: 0.0128  lr: 9.3248e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 13:58:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 13:58:29 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 13:58:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 13:59:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 13:59:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0015 s/iter. Inference: 0.1611 s/iter. Eval: 0.0118 s/iter. Total: 0.1744 s/iter. ETA=0:05:49\n",
      "\u001b[32m[10/12 13:59:55 d2.evaluation.evaluator]: \u001b[0mInference done 36/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0313 s/iter. Total: 0.1964 s/iter. ETA=0:06:28\n",
      "\u001b[32m[10/12 14:00:00 d2.evaluation.evaluator]: \u001b[0mInference done 62/2016. Dataloading: 0.0023 s/iter. Inference: 0.1631 s/iter. Eval: 0.0300 s/iter. Total: 0.1955 s/iter. ETA=0:06:22\n",
      "\u001b[32m[10/12 14:00:05 d2.evaluation.evaluator]: \u001b[0mInference done 88/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0297 s/iter. Total: 0.1949 s/iter. ETA=0:06:15\n",
      "\u001b[32m[10/12 14:00:10 d2.evaluation.evaluator]: \u001b[0mInference done 115/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0277 s/iter. Total: 0.1928 s/iter. ETA=0:06:06\n",
      "\u001b[32m[10/12 14:00:15 d2.evaluation.evaluator]: \u001b[0mInference done 141/2016. Dataloading: 0.0024 s/iter. Inference: 0.1625 s/iter. Eval: 0.0279 s/iter. Total: 0.1929 s/iter. ETA=0:06:01\n",
      "\u001b[32m[10/12 14:00:20 d2.evaluation.evaluator]: \u001b[0mInference done 167/2016. Dataloading: 0.0024 s/iter. Inference: 0.1625 s/iter. Eval: 0.0283 s/iter. Total: 0.1933 s/iter. ETA=0:05:57\n",
      "\u001b[32m[10/12 14:00:26 d2.evaluation.evaluator]: \u001b[0mInference done 192/2016. Dataloading: 0.0024 s/iter. Inference: 0.1625 s/iter. Eval: 0.0301 s/iter. Total: 0.1951 s/iter. ETA=0:05:55\n",
      "\u001b[32m[10/12 14:00:31 d2.evaluation.evaluator]: \u001b[0mInference done 217/2016. Dataloading: 0.0024 s/iter. Inference: 0.1625 s/iter. Eval: 0.0307 s/iter. Total: 0.1958 s/iter. ETA=0:05:52\n",
      "\u001b[32m[10/12 14:00:36 d2.evaluation.evaluator]: \u001b[0mInference done 242/2016. Dataloading: 0.0024 s/iter. Inference: 0.1625 s/iter. Eval: 0.0313 s/iter. Total: 0.1964 s/iter. ETA=0:05:48\n",
      "\u001b[32m[10/12 14:00:41 d2.evaluation.evaluator]: \u001b[0mInference done 268/2016. Dataloading: 0.0024 s/iter. Inference: 0.1626 s/iter. Eval: 0.0309 s/iter. Total: 0.1961 s/iter. ETA=0:05:42\n",
      "\u001b[32m[10/12 14:00:46 d2.evaluation.evaluator]: \u001b[0mInference done 294/2016. Dataloading: 0.0024 s/iter. Inference: 0.1626 s/iter. Eval: 0.0322 s/iter. Total: 0.1973 s/iter. ETA=0:05:39\n",
      "\u001b[32m[10/12 14:00:51 d2.evaluation.evaluator]: \u001b[0mInference done 318/2016. Dataloading: 0.0024 s/iter. Inference: 0.1626 s/iter. Eval: 0.0331 s/iter. Total: 0.1984 s/iter. ETA=0:05:36\n",
      "\u001b[32m[10/12 14:00:56 d2.evaluation.evaluator]: \u001b[0mInference done 343/2016. Dataloading: 0.0024 s/iter. Inference: 0.1626 s/iter. Eval: 0.0336 s/iter. Total: 0.1989 s/iter. ETA=0:05:32\n",
      "\u001b[32m[10/12 14:01:01 d2.evaluation.evaluator]: \u001b[0mInference done 368/2016. Dataloading: 0.0024 s/iter. Inference: 0.1626 s/iter. Eval: 0.0340 s/iter. Total: 0.1993 s/iter. ETA=0:05:28\n",
      "\u001b[32m[10/12 14:01:06 d2.evaluation.evaluator]: \u001b[0mInference done 393/2016. Dataloading: 0.0024 s/iter. Inference: 0.1627 s/iter. Eval: 0.0341 s/iter. Total: 0.1994 s/iter. ETA=0:05:23\n",
      "\u001b[32m[10/12 14:01:12 d2.evaluation.evaluator]: \u001b[0mInference done 416/2016. Dataloading: 0.0024 s/iter. Inference: 0.1629 s/iter. Eval: 0.0350 s/iter. Total: 0.2005 s/iter. ETA=0:05:20\n",
      "\u001b[32m[10/12 14:01:17 d2.evaluation.evaluator]: \u001b[0mInference done 442/2016. Dataloading: 0.0024 s/iter. Inference: 0.1629 s/iter. Eval: 0.0347 s/iter. Total: 0.2002 s/iter. ETA=0:05:15\n",
      "\u001b[32m[10/12 14:01:22 d2.evaluation.evaluator]: \u001b[0mInference done 467/2016. Dataloading: 0.0024 s/iter. Inference: 0.1629 s/iter. Eval: 0.0347 s/iter. Total: 0.2002 s/iter. ETA=0:05:10\n",
      "\u001b[32m[10/12 14:01:27 d2.evaluation.evaluator]: \u001b[0mInference done 493/2016. Dataloading: 0.0024 s/iter. Inference: 0.1630 s/iter. Eval: 0.0345 s/iter. Total: 0.2002 s/iter. ETA=0:05:04\n",
      "\u001b[32m[10/12 14:01:32 d2.evaluation.evaluator]: \u001b[0mInference done 520/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0337 s/iter. Total: 0.1995 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 14:01:37 d2.evaluation.evaluator]: \u001b[0mInference done 546/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0337 s/iter. Total: 0.1995 s/iter. ETA=0:04:53\n",
      "\u001b[32m[10/12 14:01:42 d2.evaluation.evaluator]: \u001b[0mInference done 572/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0336 s/iter. Total: 0.1993 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 14:01:47 d2.evaluation.evaluator]: \u001b[0mInference done 599/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0332 s/iter. Total: 0.1989 s/iter. ETA=0:04:41\n",
      "\u001b[32m[10/12 14:01:52 d2.evaluation.evaluator]: \u001b[0mInference done 624/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0332 s/iter. Total: 0.1990 s/iter. ETA=0:04:37\n",
      "\u001b[32m[10/12 14:01:57 d2.evaluation.evaluator]: \u001b[0mInference done 651/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0329 s/iter. Total: 0.1987 s/iter. ETA=0:04:31\n",
      "\u001b[32m[10/12 14:02:02 d2.evaluation.evaluator]: \u001b[0mInference done 676/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0330 s/iter. Total: 0.1988 s/iter. ETA=0:04:26\n",
      "\u001b[32m[10/12 14:02:08 d2.evaluation.evaluator]: \u001b[0mInference done 702/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0329 s/iter. Total: 0.1987 s/iter. ETA=0:04:21\n",
      "\u001b[32m[10/12 14:02:13 d2.evaluation.evaluator]: \u001b[0mInference done 729/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0326 s/iter. Total: 0.1984 s/iter. ETA=0:04:15\n",
      "\u001b[32m[10/12 14:02:18 d2.evaluation.evaluator]: \u001b[0mInference done 755/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0326 s/iter. Total: 0.1984 s/iter. ETA=0:04:10\n",
      "\u001b[32m[10/12 14:02:23 d2.evaluation.evaluator]: \u001b[0mInference done 782/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0323 s/iter. Total: 0.1982 s/iter. ETA=0:04:04\n",
      "\u001b[32m[10/12 14:02:28 d2.evaluation.evaluator]: \u001b[0mInference done 808/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0323 s/iter. Total: 0.1981 s/iter. ETA=0:03:59\n",
      "\u001b[32m[10/12 14:02:33 d2.evaluation.evaluator]: \u001b[0mInference done 834/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0321 s/iter. Total: 0.1980 s/iter. ETA=0:03:54\n",
      "\u001b[32m[10/12 14:02:38 d2.evaluation.evaluator]: \u001b[0mInference done 861/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0319 s/iter. Total: 0.1978 s/iter. ETA=0:03:48\n",
      "\u001b[32m[10/12 14:02:44 d2.evaluation.evaluator]: \u001b[0mInference done 887/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0320 s/iter. Total: 0.1978 s/iter. ETA=0:03:43\n",
      "\u001b[32m[10/12 14:02:49 d2.evaluation.evaluator]: \u001b[0mInference done 911/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0322 s/iter. Total: 0.1981 s/iter. ETA=0:03:38\n",
      "\u001b[32m[10/12 14:02:54 d2.evaluation.evaluator]: \u001b[0mInference done 936/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0324 s/iter. Total: 0.1983 s/iter. ETA=0:03:34\n",
      "\u001b[32m[10/12 14:02:59 d2.evaluation.evaluator]: \u001b[0mInference done 961/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0325 s/iter. Total: 0.1984 s/iter. ETA=0:03:29\n",
      "\u001b[32m[10/12 14:03:04 d2.evaluation.evaluator]: \u001b[0mInference done 988/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0322 s/iter. Total: 0.1982 s/iter. ETA=0:03:23\n",
      "\u001b[32m[10/12 14:03:09 d2.evaluation.evaluator]: \u001b[0mInference done 1014/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0321 s/iter. Total: 0.1980 s/iter. ETA=0:03:18\n",
      "\u001b[32m[10/12 14:03:14 d2.evaluation.evaluator]: \u001b[0mInference done 1040/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0320 s/iter. Total: 0.1980 s/iter. ETA=0:03:13\n",
      "\u001b[32m[10/12 14:03:19 d2.evaluation.evaluator]: \u001b[0mInference done 1066/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0320 s/iter. Total: 0.1980 s/iter. ETA=0:03:08\n",
      "\u001b[32m[10/12 14:03:24 d2.evaluation.evaluator]: \u001b[0mInference done 1086/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0329 s/iter. Total: 0.1989 s/iter. ETA=0:03:05\n",
      "\u001b[32m[10/12 14:03:29 d2.evaluation.evaluator]: \u001b[0mInference done 1113/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0327 s/iter. Total: 0.1987 s/iter. ETA=0:02:59\n",
      "\u001b[32m[10/12 14:03:34 d2.evaluation.evaluator]: \u001b[0mInference done 1139/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0326 s/iter. Total: 0.1986 s/iter. ETA=0:02:54\n",
      "\u001b[32m[10/12 14:03:39 d2.evaluation.evaluator]: \u001b[0mInference done 1165/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0325 s/iter. Total: 0.1985 s/iter. ETA=0:02:48\n",
      "\u001b[32m[10/12 14:03:44 d2.evaluation.evaluator]: \u001b[0mInference done 1191/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0325 s/iter. Total: 0.1984 s/iter. ETA=0:02:43\n",
      "\u001b[32m[10/12 14:03:49 d2.evaluation.evaluator]: \u001b[0mInference done 1216/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0325 s/iter. Total: 0.1985 s/iter. ETA=0:02:38\n",
      "\u001b[32m[10/12 14:03:55 d2.evaluation.evaluator]: \u001b[0mInference done 1242/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0324 s/iter. Total: 0.1984 s/iter. ETA=0:02:33\n",
      "\u001b[32m[10/12 14:04:00 d2.evaluation.evaluator]: \u001b[0mInference done 1268/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0323 s/iter. Total: 0.1983 s/iter. ETA=0:02:28\n",
      "\u001b[32m[10/12 14:04:05 d2.evaluation.evaluator]: \u001b[0mInference done 1294/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0323 s/iter. Total: 0.1982 s/iter. ETA=0:02:23\n",
      "\u001b[32m[10/12 14:04:10 d2.evaluation.evaluator]: \u001b[0mInference done 1320/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0321 s/iter. Total: 0.1981 s/iter. ETA=0:02:17\n",
      "\u001b[32m[10/12 14:04:15 d2.evaluation.evaluator]: \u001b[0mInference done 1346/2016. Dataloading: 0.0023 s/iter. Inference: 0.1634 s/iter. Eval: 0.0321 s/iter. Total: 0.1981 s/iter. ETA=0:02:12\n",
      "\u001b[32m[10/12 14:04:20 d2.evaluation.evaluator]: \u001b[0mInference done 1371/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0321 s/iter. Total: 0.1982 s/iter. ETA=0:02:07\n",
      "\u001b[32m[10/12 14:04:25 d2.evaluation.evaluator]: \u001b[0mInference done 1397/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0320 s/iter. Total: 0.1981 s/iter. ETA=0:02:02\n",
      "\u001b[32m[10/12 14:04:30 d2.evaluation.evaluator]: \u001b[0mInference done 1422/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0321 s/iter. Total: 0.1982 s/iter. ETA=0:01:57\n",
      "\u001b[32m[10/12 14:04:35 d2.evaluation.evaluator]: \u001b[0mInference done 1448/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0320 s/iter. Total: 0.1981 s/iter. ETA=0:01:52\n",
      "\u001b[32m[10/12 14:04:40 d2.evaluation.evaluator]: \u001b[0mInference done 1473/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0321 s/iter. Total: 0.1982 s/iter. ETA=0:01:47\n",
      "\u001b[32m[10/12 14:04:45 d2.evaluation.evaluator]: \u001b[0mInference done 1498/2016. Dataloading: 0.0023 s/iter. Inference: 0.1635 s/iter. Eval: 0.0322 s/iter. Total: 0.1983 s/iter. ETA=0:01:42\n",
      "\u001b[32m[10/12 14:04:50 d2.evaluation.evaluator]: \u001b[0mInference done 1524/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0321 s/iter. Total: 0.1982 s/iter. ETA=0:01:37\n",
      "\u001b[32m[10/12 14:04:55 d2.evaluation.evaluator]: \u001b[0mInference done 1549/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0322 s/iter. Total: 0.1983 s/iter. ETA=0:01:32\n",
      "\u001b[32m[10/12 14:05:00 d2.evaluation.evaluator]: \u001b[0mInference done 1576/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0319 s/iter. Total: 0.1981 s/iter. ETA=0:01:27\n",
      "\u001b[32m[10/12 14:05:05 d2.evaluation.evaluator]: \u001b[0mInference done 1597/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0326 s/iter. Total: 0.1987 s/iter. ETA=0:01:23\n",
      "\u001b[32m[10/12 14:05:11 d2.evaluation.evaluator]: \u001b[0mInference done 1617/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0330 s/iter. Total: 0.1994 s/iter. ETA=0:01:19\n",
      "\u001b[32m[10/12 14:05:16 d2.evaluation.evaluator]: \u001b[0mInference done 1642/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0331 s/iter. Total: 0.1995 s/iter. ETA=0:01:14\n",
      "\u001b[32m[10/12 14:05:21 d2.evaluation.evaluator]: \u001b[0mInference done 1667/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0331 s/iter. Total: 0.1995 s/iter. ETA=0:01:09\n",
      "\u001b[32m[10/12 14:05:26 d2.evaluation.evaluator]: \u001b[0mInference done 1693/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0330 s/iter. Total: 0.1994 s/iter. ETA=0:01:04\n",
      "\u001b[32m[10/12 14:05:31 d2.evaluation.evaluator]: \u001b[0mInference done 1718/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0332 s/iter. Total: 0.1996 s/iter. ETA=0:00:59\n",
      "\u001b[32m[10/12 14:05:36 d2.evaluation.evaluator]: \u001b[0mInference done 1742/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0334 s/iter. Total: 0.1998 s/iter. ETA=0:00:54\n",
      "\u001b[32m[10/12 14:05:41 d2.evaluation.evaluator]: \u001b[0mInference done 1765/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2000 s/iter. ETA=0:00:50\n",
      "\u001b[32m[10/12 14:05:46 d2.evaluation.evaluator]: \u001b[0mInference done 1790/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2001 s/iter. ETA=0:00:45\n",
      "\u001b[32m[10/12 14:05:51 d2.evaluation.evaluator]: \u001b[0mInference done 1816/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2000 s/iter. ETA=0:00:40\n",
      "\u001b[32m[10/12 14:05:56 d2.evaluation.evaluator]: \u001b[0mInference done 1841/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2000 s/iter. ETA=0:00:35\n",
      "\u001b[32m[10/12 14:06:02 d2.evaluation.evaluator]: \u001b[0mInference done 1867/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2000 s/iter. ETA=0:00:29\n",
      "\u001b[32m[10/12 14:06:07 d2.evaluation.evaluator]: \u001b[0mInference done 1892/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2001 s/iter. ETA=0:00:24\n",
      "\u001b[32m[10/12 14:06:12 d2.evaluation.evaluator]: \u001b[0mInference done 1918/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0337 s/iter. Total: 0.2002 s/iter. ETA=0:00:19\n",
      "\u001b[32m[10/12 14:06:17 d2.evaluation.evaluator]: \u001b[0mInference done 1944/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0337 s/iter. Total: 0.2002 s/iter. ETA=0:00:14\n",
      "\u001b[32m[10/12 14:06:22 d2.evaluation.evaluator]: \u001b[0mInference done 1971/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2000 s/iter. ETA=0:00:09\n",
      "\u001b[32m[10/12 14:06:27 d2.evaluation.evaluator]: \u001b[0mInference done 1997/2016. Dataloading: 0.0023 s/iter. Inference: 0.1636 s/iter. Eval: 0.0336 s/iter. Total: 0.2000 s/iter. ETA=0:00:03\n",
      "\u001b[32m[10/12 14:06:31 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:42.103545 (0.199952 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 14:06:31 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:29 (0.163608 s / iter per device, on 1 devices)\n",
      "miou = 74.84708496846454\n",
      "OA = 88.62624054863339\n",
      "Kappa = 85.20573071827421\n",
      "F1_score = 71.1590990937219\n",
      "\u001b[32m[10/12 14:06:33 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 74.84708496846454, 'fwIoU': 80.36678130077328, 'IoU-Background': 39.06221465488389, 'IoU-Surfaces': 83.60854342979668, 'IoU-Building': 91.69055028703073, 'IoU-Low vegetation': 74.50497357222345, 'IoU-tree': 76.61849046557785, 'IoU-Car': 83.59773740127477, 'mACC': 84.35252224285509, 'pACC': 88.62624054863339, 'ACC-Background': 56.49652429215215, 'ACC-Surfaces': 90.67355206266737, 'ACC-Building': 96.41127310207989, 'ACC-Low vegetation': 84.84416170916965, 'ACC-tree': 87.11459663083906, 'ACC-Car': 90.57502566022244})])\n",
      "\u001b[32m[10/12 14:06:33 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 14:06:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 14:06:33 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 14:06:33 d2.evaluation.testing]: \u001b[0mcopypaste: 74.8471,80.3668,84.3525,88.6262\n",
      "\u001b[32m[10/12 14:06:33 d2.utils.events]: \u001b[0m eta: 22:36:28  iter: 5999  total_loss: 20.58  loss_ce: 0.2258  loss_cate: 0.1413  loss_mask: 0.7589  loss_dice: 1.002  loss_ce_0: 0.3325  loss_cate_0: 0  loss_mask_0: 0.8086  loss_dice_0: 1.076  loss_ce_1: 0.2029  loss_cate_1: 0  loss_mask_1: 0.7867  loss_dice_1: 1.083  loss_ce_2: 0.2694  loss_cate_2: 0  loss_mask_2: 0.8091  loss_dice_2: 1.062  loss_ce_3: 0.1979  loss_cate_3: 0  loss_mask_3: 0.8267  loss_dice_3: 0.9823  loss_ce_4: 0.1703  loss_cate_4: 0  loss_mask_4: 0.804  loss_dice_4: 1.061  loss_ce_5: 0.1584  loss_cate_5: 0  loss_mask_5: 0.7942  loss_dice_5: 1.049  loss_ce_6: 0.1622  loss_cate_6: 0  loss_mask_6: 0.7848  loss_dice_6: 1.012  loss_ce_7: 0.1541  loss_cate_7: 0  loss_mask_7: 0.8  loss_dice_7: 0.9985  loss_ce_8: 0.1956  loss_cate_8: 0  loss_mask_8: 0.7901  loss_dice_8: 1.013  time: 1.1020  data_time: 0.0123  lr: 9.3225e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:07:01 d2.utils.events]: \u001b[0m eta: 22:36:19  iter: 6019  total_loss: 23.45  loss_ce: 0.1643  loss_cate: 0.1794  loss_mask: 0.8874  loss_dice: 1.044  loss_ce_0: 0.3767  loss_cate_0: 0  loss_mask_0: 0.9334  loss_dice_0: 1.073  loss_ce_1: 0.3429  loss_cate_1: 0  loss_mask_1: 0.8859  loss_dice_1: 1.088  loss_ce_2: 0.3055  loss_cate_2: 0  loss_mask_2: 0.8526  loss_dice_2: 1.029  loss_ce_3: 0.2779  loss_cate_3: 0  loss_mask_3: 0.8867  loss_dice_3: 1.05  loss_ce_4: 0.2494  loss_cate_4: 0  loss_mask_4: 0.8869  loss_dice_4: 1.073  loss_ce_5: 0.2772  loss_cate_5: 0  loss_mask_5: 0.8792  loss_dice_5: 1.008  loss_ce_6: 0.2078  loss_cate_6: 0  loss_mask_6: 0.9101  loss_dice_6: 1.024  loss_ce_7: 0.2008  loss_cate_7: 0  loss_mask_7: 0.9094  loss_dice_7: 1.004  loss_ce_8: 0.2262  loss_cate_8: 0  loss_mask_8: 0.9314  loss_dice_8: 1.048  time: 1.1020  data_time: 0.0135  lr: 9.3202e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:07:24 d2.utils.events]: \u001b[0m eta: 22:35:53  iter: 6039  total_loss: 22.01  loss_ce: 0.2647  loss_cate: 0.1672  loss_mask: 0.8341  loss_dice: 1.041  loss_ce_0: 0.336  loss_cate_0: 0  loss_mask_0: 0.7929  loss_dice_0: 1.123  loss_ce_1: 0.2619  loss_cate_1: 0  loss_mask_1: 0.7921  loss_dice_1: 1.027  loss_ce_2: 0.2829  loss_cate_2: 0  loss_mask_2: 0.7649  loss_dice_2: 0.993  loss_ce_3: 0.247  loss_cate_3: 0  loss_mask_3: 0.7543  loss_dice_3: 1.027  loss_ce_4: 0.2439  loss_cate_4: 0  loss_mask_4: 0.7631  loss_dice_4: 1.05  loss_ce_5: 0.2099  loss_cate_5: 0  loss_mask_5: 0.7822  loss_dice_5: 1.048  loss_ce_6: 0.209  loss_cate_6: 0  loss_mask_6: 0.7806  loss_dice_6: 1.039  loss_ce_7: 0.2057  loss_cate_7: 0  loss_mask_7: 0.7732  loss_dice_7: 1.056  loss_ce_8: 0.2202  loss_cate_8: 0  loss_mask_8: 0.7833  loss_dice_8: 1.037  time: 1.1020  data_time: 0.0129  lr: 9.318e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:07:47 d2.utils.events]: \u001b[0m eta: 22:35:24  iter: 6059  total_loss: 22.51  loss_ce: 0.3094  loss_cate: 0.1581  loss_mask: 0.7966  loss_dice: 0.9952  loss_ce_0: 0.4856  loss_cate_0: 0  loss_mask_0: 0.7831  loss_dice_0: 1.156  loss_ce_1: 0.3686  loss_cate_1: 0  loss_mask_1: 0.7827  loss_dice_1: 1.044  loss_ce_2: 0.3931  loss_cate_2: 0  loss_mask_2: 0.7561  loss_dice_2: 1.07  loss_ce_3: 0.3297  loss_cate_3: 0  loss_mask_3: 0.7342  loss_dice_3: 1.004  loss_ce_4: 0.2812  loss_cate_4: 0  loss_mask_4: 0.8212  loss_dice_4: 1.001  loss_ce_5: 0.3701  loss_cate_5: 0  loss_mask_5: 0.795  loss_dice_5: 0.9797  loss_ce_6: 0.3675  loss_cate_6: 0  loss_mask_6: 0.7995  loss_dice_6: 1.015  loss_ce_7: 0.3405  loss_cate_7: 0  loss_mask_7: 0.8018  loss_dice_7: 0.9958  loss_ce_8: 0.3392  loss_cate_8: 0  loss_mask_8: 0.8043  loss_dice_8: 1.058  time: 1.1019  data_time: 0.0133  lr: 9.3157e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:08:12 d2.utils.events]: \u001b[0m eta: 22:34:59  iter: 6079  total_loss: 20.91  loss_ce: 0.2542  loss_cate: 0.1775  loss_mask: 0.7949  loss_dice: 0.9485  loss_ce_0: 0.3903  loss_cate_0: 0  loss_mask_0: 0.7527  loss_dice_0: 0.992  loss_ce_1: 0.282  loss_cate_1: 0  loss_mask_1: 0.8016  loss_dice_1: 0.9707  loss_ce_2: 0.2716  loss_cate_2: 0  loss_mask_2: 0.7568  loss_dice_2: 0.9677  loss_ce_3: 0.2851  loss_cate_3: 0  loss_mask_3: 0.7503  loss_dice_3: 0.9439  loss_ce_4: 0.2598  loss_cate_4: 0  loss_mask_4: 0.7336  loss_dice_4: 0.9378  loss_ce_5: 0.244  loss_cate_5: 0  loss_mask_5: 0.7304  loss_dice_5: 0.9526  loss_ce_6: 0.2567  loss_cate_6: 0  loss_mask_6: 0.7764  loss_dice_6: 0.9317  loss_ce_7: 0.2401  loss_cate_7: 0  loss_mask_7: 0.7737  loss_dice_7: 0.9631  loss_ce_8: 0.237  loss_cate_8: 0  loss_mask_8: 0.7664  loss_dice_8: 0.949  time: 1.1019  data_time: 0.0137  lr: 9.3134e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:08:35 d2.utils.events]: \u001b[0m eta: 22:34:36  iter: 6099  total_loss: 23.7  loss_ce: 0.2771  loss_cate: 0.1881  loss_mask: 0.8825  loss_dice: 1.09  loss_ce_0: 0.4137  loss_cate_0: 0  loss_mask_0: 0.9292  loss_dice_0: 1.152  loss_ce_1: 0.3351  loss_cate_1: 0  loss_mask_1: 0.8673  loss_dice_1: 1.09  loss_ce_2: 0.3264  loss_cate_2: 0  loss_mask_2: 0.9071  loss_dice_2: 1.186  loss_ce_3: 0.3122  loss_cate_3: 0  loss_mask_3: 0.8457  loss_dice_3: 1.068  loss_ce_4: 0.2747  loss_cate_4: 0  loss_mask_4: 0.85  loss_dice_4: 1.129  loss_ce_5: 0.3006  loss_cate_5: 0  loss_mask_5: 0.8491  loss_dice_5: 1.099  loss_ce_6: 0.2954  loss_cate_6: 0  loss_mask_6: 0.8441  loss_dice_6: 1.07  loss_ce_7: 0.2793  loss_cate_7: 0  loss_mask_7: 0.8497  loss_dice_7: 1.126  loss_ce_8: 0.2596  loss_cate_8: 0  loss_mask_8: 0.8589  loss_dice_8: 1.097  time: 1.1019  data_time: 0.0134  lr: 9.3112e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:09:01 d2.utils.events]: \u001b[0m eta: 22:34:15  iter: 6119  total_loss: 19.93  loss_ce: 0.204  loss_cate: 0.1799  loss_mask: 0.7384  loss_dice: 0.9876  loss_ce_0: 0.3887  loss_cate_0: 0  loss_mask_0: 0.7326  loss_dice_0: 1.061  loss_ce_1: 0.2838  loss_cate_1: 0  loss_mask_1: 0.7298  loss_dice_1: 1.05  loss_ce_2: 0.2612  loss_cate_2: 0  loss_mask_2: 0.7286  loss_dice_2: 1.044  loss_ce_3: 0.2404  loss_cate_3: 0  loss_mask_3: 0.7254  loss_dice_3: 1.055  loss_ce_4: 0.2764  loss_cate_4: 0  loss_mask_4: 0.7411  loss_dice_4: 1.034  loss_ce_5: 0.2071  loss_cate_5: 0  loss_mask_5: 0.7594  loss_dice_5: 1.058  loss_ce_6: 0.2123  loss_cate_6: 0  loss_mask_6: 0.7841  loss_dice_6: 0.991  loss_ce_7: 0.2212  loss_cate_7: 0  loss_mask_7: 0.7803  loss_dice_7: 1.011  loss_ce_8: 0.2328  loss_cate_8: 0  loss_mask_8: 0.7433  loss_dice_8: 0.9992  time: 1.1019  data_time: 0.0134  lr: 9.3089e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:09:26 d2.utils.events]: \u001b[0m eta: 22:33:54  iter: 6139  total_loss: 23.2  loss_ce: 0.2021  loss_cate: 0.1799  loss_mask: 0.9858  loss_dice: 1.132  loss_ce_0: 0.3128  loss_cate_0: 0  loss_mask_0: 0.9997  loss_dice_0: 1.131  loss_ce_1: 0.1915  loss_cate_1: 0  loss_mask_1: 0.9561  loss_dice_1: 1.059  loss_ce_2: 0.2605  loss_cate_2: 0  loss_mask_2: 0.9808  loss_dice_2: 1.064  loss_ce_3: 0.226  loss_cate_3: 0  loss_mask_3: 0.9854  loss_dice_3: 1.061  loss_ce_4: 0.2012  loss_cate_4: 0  loss_mask_4: 0.9898  loss_dice_4: 1.042  loss_ce_5: 0.1657  loss_cate_5: 0  loss_mask_5: 0.9859  loss_dice_5: 1.059  loss_ce_6: 0.2154  loss_cate_6: 0  loss_mask_6: 0.9898  loss_dice_6: 1.064  loss_ce_7: 0.2298  loss_cate_7: 0  loss_mask_7: 0.9787  loss_dice_7: 1.07  loss_ce_8: 0.2366  loss_cate_8: 0  loss_mask_8: 0.985  loss_dice_8: 1.088  time: 1.1019  data_time: 0.0129  lr: 9.3066e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:09:51 d2.utils.events]: \u001b[0m eta: 22:33:32  iter: 6159  total_loss: 21.86  loss_ce: 0.2515  loss_cate: 0.1839  loss_mask: 0.787  loss_dice: 0.9435  loss_ce_0: 0.453  loss_cate_0: 0  loss_mask_0: 0.8078  loss_dice_0: 1.035  loss_ce_1: 0.3018  loss_cate_1: 0  loss_mask_1: 0.7904  loss_dice_1: 1.085  loss_ce_2: 0.3013  loss_cate_2: 0  loss_mask_2: 0.7711  loss_dice_2: 0.9879  loss_ce_3: 0.2549  loss_cate_3: 0  loss_mask_3: 0.7942  loss_dice_3: 1.054  loss_ce_4: 0.2602  loss_cate_4: 0  loss_mask_4: 0.7926  loss_dice_4: 1.026  loss_ce_5: 0.2633  loss_cate_5: 0  loss_mask_5: 0.7754  loss_dice_5: 1.031  loss_ce_6: 0.3004  loss_cate_6: 0  loss_mask_6: 0.7925  loss_dice_6: 1.008  loss_ce_7: 0.2573  loss_cate_7: 0  loss_mask_7: 0.7869  loss_dice_7: 0.9672  loss_ce_8: 0.2564  loss_cate_8: 0  loss_mask_8: 0.7971  loss_dice_8: 1.028  time: 1.1019  data_time: 0.0131  lr: 9.3044e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:10:14 d2.utils.events]: \u001b[0m eta: 22:33:08  iter: 6179  total_loss: 22.88  loss_ce: 0.2838  loss_cate: 0.176  loss_mask: 0.8185  loss_dice: 1.057  loss_ce_0: 0.3672  loss_cate_0: 0  loss_mask_0: 0.9107  loss_dice_0: 1.121  loss_ce_1: 0.2072  loss_cate_1: 0  loss_mask_1: 0.8453  loss_dice_1: 1.114  loss_ce_2: 0.2609  loss_cate_2: 0  loss_mask_2: 0.8117  loss_dice_2: 1.081  loss_ce_3: 0.2371  loss_cate_3: 0  loss_mask_3: 0.8481  loss_dice_3: 1.087  loss_ce_4: 0.2005  loss_cate_4: 0  loss_mask_4: 0.8839  loss_dice_4: 1.109  loss_ce_5: 0.2562  loss_cate_5: 0  loss_mask_5: 0.8464  loss_dice_5: 1.065  loss_ce_6: 0.2749  loss_cate_6: 0  loss_mask_6: 0.815  loss_dice_6: 1.055  loss_ce_7: 0.2982  loss_cate_7: 0  loss_mask_7: 0.8167  loss_dice_7: 1.082  loss_ce_8: 0.3015  loss_cate_8: 0  loss_mask_8: 0.7907  loss_dice_8: 1.065  time: 1.1019  data_time: 0.0129  lr: 9.3021e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:10:42 d2.utils.events]: \u001b[0m eta: 22:32:41  iter: 6199  total_loss: 22.19  loss_ce: 0.2609  loss_cate: 0.1646  loss_mask: 0.8577  loss_dice: 0.9754  loss_ce_0: 0.4306  loss_cate_0: 0  loss_mask_0: 0.9091  loss_dice_0: 1.121  loss_ce_1: 0.3824  loss_cate_1: 0  loss_mask_1: 0.8886  loss_dice_1: 1.114  loss_ce_2: 0.338  loss_cate_2: 0  loss_mask_2: 0.8814  loss_dice_2: 0.977  loss_ce_3: 0.2819  loss_cate_3: 0  loss_mask_3: 0.9063  loss_dice_3: 0.9575  loss_ce_4: 0.3115  loss_cate_4: 0  loss_mask_4: 0.8818  loss_dice_4: 0.9928  loss_ce_5: 0.292  loss_cate_5: 0  loss_mask_5: 0.9195  loss_dice_5: 0.9788  loss_ce_6: 0.2682  loss_cate_6: 0  loss_mask_6: 0.9022  loss_dice_6: 1.006  loss_ce_7: 0.2617  loss_cate_7: 0  loss_mask_7: 0.897  loss_dice_7: 1.003  loss_ce_8: 0.2557  loss_cate_8: 0  loss_mask_8: 0.9068  loss_dice_8: 0.962  time: 1.1018  data_time: 0.0129  lr: 9.2998e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:11:07 d2.utils.events]: \u001b[0m eta: 22:32:13  iter: 6219  total_loss: 23.11  loss_ce: 0.3352  loss_cate: 0.186  loss_mask: 0.8195  loss_dice: 1.091  loss_ce_0: 0.341  loss_cate_0: 0  loss_mask_0: 0.7881  loss_dice_0: 1.233  loss_ce_1: 0.2599  loss_cate_1: 0  loss_mask_1: 0.819  loss_dice_1: 1.156  loss_ce_2: 0.3097  loss_cate_2: 0  loss_mask_2: 0.7487  loss_dice_2: 1.082  loss_ce_3: 0.3592  loss_cate_3: 0  loss_mask_3: 0.72  loss_dice_3: 1.06  loss_ce_4: 0.3165  loss_cate_4: 0  loss_mask_4: 0.7283  loss_dice_4: 1.119  loss_ce_5: 0.3403  loss_cate_5: 0  loss_mask_5: 0.7404  loss_dice_5: 1.109  loss_ce_6: 0.2723  loss_cate_6: 0  loss_mask_6: 0.8272  loss_dice_6: 1.094  loss_ce_7: 0.2945  loss_cate_7: 0  loss_mask_7: 0.7612  loss_dice_7: 1.137  loss_ce_8: 0.287  loss_cate_8: 0  loss_mask_8: 0.8133  loss_dice_8: 1.078  time: 1.1018  data_time: 0.0136  lr: 9.2976e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:11:31 d2.utils.events]: \u001b[0m eta: 22:31:53  iter: 6239  total_loss: 21.94  loss_ce: 0.3061  loss_cate: 0.1716  loss_mask: 0.8065  loss_dice: 1.045  loss_ce_0: 0.5378  loss_cate_0: 0  loss_mask_0: 0.8388  loss_dice_0: 1.083  loss_ce_1: 0.3311  loss_cate_1: 0  loss_mask_1: 0.8329  loss_dice_1: 1.062  loss_ce_2: 0.3282  loss_cate_2: 0  loss_mask_2: 0.8161  loss_dice_2: 1.001  loss_ce_3: 0.3058  loss_cate_3: 0  loss_mask_3: 0.7951  loss_dice_3: 0.996  loss_ce_4: 0.2809  loss_cate_4: 0  loss_mask_4: 0.8054  loss_dice_4: 0.9756  loss_ce_5: 0.2909  loss_cate_5: 0  loss_mask_5: 0.8154  loss_dice_5: 1.028  loss_ce_6: 0.3105  loss_cate_6: 0  loss_mask_6: 0.8073  loss_dice_6: 1.038  loss_ce_7: 0.3098  loss_cate_7: 0  loss_mask_7: 0.8064  loss_dice_7: 1.035  loss_ce_8: 0.3158  loss_cate_8: 0  loss_mask_8: 0.8133  loss_dice_8: 1.045  time: 1.1018  data_time: 0.0142  lr: 9.2953e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:11:57 d2.utils.events]: \u001b[0m eta: 22:31:22  iter: 6259  total_loss: 21.89  loss_ce: 0.1828  loss_cate: 0.1661  loss_mask: 0.8268  loss_dice: 0.9986  loss_ce_0: 0.3284  loss_cate_0: 0  loss_mask_0: 0.8658  loss_dice_0: 1.081  loss_ce_1: 0.2271  loss_cate_1: 0  loss_mask_1: 0.865  loss_dice_1: 1.046  loss_ce_2: 0.2106  loss_cate_2: 0  loss_mask_2: 0.8408  loss_dice_2: 1.038  loss_ce_3: 0.2196  loss_cate_3: 0  loss_mask_3: 0.8559  loss_dice_3: 0.9867  loss_ce_4: 0.1821  loss_cate_4: 0  loss_mask_4: 0.863  loss_dice_4: 1.018  loss_ce_5: 0.2121  loss_cate_5: 0  loss_mask_5: 0.818  loss_dice_5: 0.9957  loss_ce_6: 0.2306  loss_cate_6: 0  loss_mask_6: 0.8034  loss_dice_6: 0.9761  loss_ce_7: 0.2069  loss_cate_7: 0  loss_mask_7: 0.8668  loss_dice_7: 0.9719  loss_ce_8: 0.1886  loss_cate_8: 0  loss_mask_8: 0.8458  loss_dice_8: 1.007  time: 1.1019  data_time: 0.0331  lr: 9.293e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:12:22 d2.utils.events]: \u001b[0m eta: 22:31:05  iter: 6279  total_loss: 23.19  loss_ce: 0.4165  loss_cate: 0.1904  loss_mask: 0.733  loss_dice: 1.048  loss_ce_0: 0.5801  loss_cate_0: 0  loss_mask_0: 0.7874  loss_dice_0: 1.115  loss_ce_1: 0.4682  loss_cate_1: 0  loss_mask_1: 0.7748  loss_dice_1: 1.164  loss_ce_2: 0.5014  loss_cate_2: 0  loss_mask_2: 0.7674  loss_dice_2: 1.171  loss_ce_3: 0.4269  loss_cate_3: 0  loss_mask_3: 0.743  loss_dice_3: 1.135  loss_ce_4: 0.3316  loss_cate_4: 0  loss_mask_4: 0.7384  loss_dice_4: 1.117  loss_ce_5: 0.4093  loss_cate_5: 0  loss_mask_5: 0.7469  loss_dice_5: 1.116  loss_ce_6: 0.5149  loss_cate_6: 0  loss_mask_6: 0.7427  loss_dice_6: 1.076  loss_ce_7: 0.43  loss_cate_7: 0  loss_mask_7: 0.7387  loss_dice_7: 1.074  loss_ce_8: 0.4062  loss_cate_8: 0  loss_mask_8: 0.731  loss_dice_8: 1.059  time: 1.1019  data_time: 0.0156  lr: 9.2908e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:12:53 d2.utils.events]: \u001b[0m eta: 22:30:37  iter: 6299  total_loss: 24.41  loss_ce: 0.2301  loss_cate: 0.1955  loss_mask: 0.8371  loss_dice: 1.141  loss_ce_0: 0.4127  loss_cate_0: 0  loss_mask_0: 0.9039  loss_dice_0: 1.201  loss_ce_1: 0.3258  loss_cate_1: 0  loss_mask_1: 0.8813  loss_dice_1: 1.164  loss_ce_2: 0.2773  loss_cate_2: 0  loss_mask_2: 0.8802  loss_dice_2: 1.177  loss_ce_3: 0.2126  loss_cate_3: 0  loss_mask_3: 0.8823  loss_dice_3: 1.146  loss_ce_4: 0.2419  loss_cate_4: 0  loss_mask_4: 0.8836  loss_dice_4: 1.148  loss_ce_5: 0.2245  loss_cate_5: 0  loss_mask_5: 0.9141  loss_dice_5: 1.195  loss_ce_6: 0.303  loss_cate_6: 0  loss_mask_6: 0.873  loss_dice_6: 1.127  loss_ce_7: 0.2403  loss_cate_7: 0  loss_mask_7: 0.8703  loss_dice_7: 1.172  loss_ce_8: 0.2191  loss_cate_8: 0  loss_mask_8: 0.8479  loss_dice_8: 1.144  time: 1.1019  data_time: 0.0128  lr: 9.2885e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:13:20 d2.utils.events]: \u001b[0m eta: 22:29:54  iter: 6319  total_loss: 24.13  loss_ce: 0.3081  loss_cate: 0.1685  loss_mask: 0.9176  loss_dice: 1.063  loss_ce_0: 0.5219  loss_cate_0: 0  loss_mask_0: 1.022  loss_dice_0: 1.048  loss_ce_1: 0.3312  loss_cate_1: 0  loss_mask_1: 0.9371  loss_dice_1: 1.076  loss_ce_2: 0.3708  loss_cate_2: 0  loss_mask_2: 0.9163  loss_dice_2: 1.12  loss_ce_3: 0.317  loss_cate_3: 0  loss_mask_3: 0.9012  loss_dice_3: 1.078  loss_ce_4: 0.3522  loss_cate_4: 0  loss_mask_4: 0.951  loss_dice_4: 1.111  loss_ce_5: 0.33  loss_cate_5: 0  loss_mask_5: 0.9522  loss_dice_5: 1.065  loss_ce_6: 0.3677  loss_cate_6: 0  loss_mask_6: 0.8854  loss_dice_6: 1.053  loss_ce_7: 0.3605  loss_cate_7: 0  loss_mask_7: 0.893  loss_dice_7: 1.065  loss_ce_8: 0.3554  loss_cate_8: 0  loss_mask_8: 0.956  loss_dice_8: 1.078  time: 1.1018  data_time: 0.0122  lr: 9.2862e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:13:44 d2.utils.events]: \u001b[0m eta: 22:29:53  iter: 6339  total_loss: 21.77  loss_ce: 0.3097  loss_cate: 0.1837  loss_mask: 0.8017  loss_dice: 1.007  loss_ce_0: 0.3529  loss_cate_0: 0  loss_mask_0: 0.84  loss_dice_0: 1.076  loss_ce_1: 0.2856  loss_cate_1: 0  loss_mask_1: 0.7902  loss_dice_1: 1.047  loss_ce_2: 0.2953  loss_cate_2: 0  loss_mask_2: 0.8105  loss_dice_2: 0.988  loss_ce_3: 0.2723  loss_cate_3: 0  loss_mask_3: 0.7989  loss_dice_3: 0.986  loss_ce_4: 0.2501  loss_cate_4: 0  loss_mask_4: 0.7956  loss_dice_4: 1.007  loss_ce_5: 0.2514  loss_cate_5: 0  loss_mask_5: 0.8017  loss_dice_5: 1.028  loss_ce_6: 0.2684  loss_cate_6: 0  loss_mask_6: 0.8025  loss_dice_6: 0.9994  loss_ce_7: 0.2812  loss_cate_7: 0  loss_mask_7: 0.7984  loss_dice_7: 0.9944  loss_ce_8: 0.2901  loss_cate_8: 0  loss_mask_8: 0.7968  loss_dice_8: 0.9977  time: 1.1019  data_time: 0.0131  lr: 9.284e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:14:08 d2.utils.events]: \u001b[0m eta: 22:29:25  iter: 6359  total_loss: 22.41  loss_ce: 0.2365  loss_cate: 0.1843  loss_mask: 0.8736  loss_dice: 1.08  loss_ce_0: 0.4069  loss_cate_0: 0  loss_mask_0: 0.851  loss_dice_0: 1.081  loss_ce_1: 0.2551  loss_cate_1: 0  loss_mask_1: 0.8965  loss_dice_1: 1.082  loss_ce_2: 0.2597  loss_cate_2: 0  loss_mask_2: 0.9088  loss_dice_2: 1.102  loss_ce_3: 0.2187  loss_cate_3: 0  loss_mask_3: 0.9056  loss_dice_3: 1.058  loss_ce_4: 0.2809  loss_cate_4: 0  loss_mask_4: 0.8815  loss_dice_4: 1.067  loss_ce_5: 0.2219  loss_cate_5: 0  loss_mask_5: 0.879  loss_dice_5: 1.084  loss_ce_6: 0.2219  loss_cate_6: 0  loss_mask_6: 0.8696  loss_dice_6: 1.09  loss_ce_7: 0.2059  loss_cate_7: 0  loss_mask_7: 0.9179  loss_dice_7: 1.074  loss_ce_8: 0.2187  loss_cate_8: 0  loss_mask_8: 0.8678  loss_dice_8: 1.088  time: 1.1018  data_time: 0.0132  lr: 9.2817e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:14:31 d2.utils.events]: \u001b[0m eta: 22:28:52  iter: 6379  total_loss: 21.37  loss_ce: 0.2445  loss_cate: 0.1705  loss_mask: 0.8124  loss_dice: 0.9849  loss_ce_0: 0.2896  loss_cate_0: 0  loss_mask_0: 0.8784  loss_dice_0: 1.062  loss_ce_1: 0.2245  loss_cate_1: 0  loss_mask_1: 0.852  loss_dice_1: 1.055  loss_ce_2: 0.2359  loss_cate_2: 0  loss_mask_2: 0.8847  loss_dice_2: 1.015  loss_ce_3: 0.2628  loss_cate_3: 0  loss_mask_3: 0.838  loss_dice_3: 1.009  loss_ce_4: 0.2806  loss_cate_4: 0  loss_mask_4: 0.8673  loss_dice_4: 1.034  loss_ce_5: 0.2598  loss_cate_5: 0  loss_mask_5: 0.8492  loss_dice_5: 0.9848  loss_ce_6: 0.2104  loss_cate_6: 0  loss_mask_6: 0.8176  loss_dice_6: 0.997  loss_ce_7: 0.229  loss_cate_7: 0  loss_mask_7: 0.8217  loss_dice_7: 0.9912  loss_ce_8: 0.2519  loss_cate_8: 0  loss_mask_8: 0.827  loss_dice_8: 0.9884  time: 1.1018  data_time: 0.0131  lr: 9.2794e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:14:55 d2.utils.events]: \u001b[0m eta: 22:28:36  iter: 6399  total_loss: 19.37  loss_ce: 0.1528  loss_cate: 0.1297  loss_mask: 0.8301  loss_dice: 0.9458  loss_ce_0: 0.331  loss_cate_0: 0  loss_mask_0: 0.8479  loss_dice_0: 0.9625  loss_ce_1: 0.2609  loss_cate_1: 0  loss_mask_1: 0.8174  loss_dice_1: 0.9458  loss_ce_2: 0.1799  loss_cate_2: 0  loss_mask_2: 0.8137  loss_dice_2: 1.007  loss_ce_3: 0.1775  loss_cate_3: 0  loss_mask_3: 0.8582  loss_dice_3: 0.9459  loss_ce_4: 0.1937  loss_cate_4: 0  loss_mask_4: 0.8452  loss_dice_4: 0.957  loss_ce_5: 0.1848  loss_cate_5: 0  loss_mask_5: 0.8193  loss_dice_5: 0.9462  loss_ce_6: 0.1349  loss_cate_6: 0  loss_mask_6: 0.8189  loss_dice_6: 0.9458  loss_ce_7: 0.1904  loss_cate_7: 0  loss_mask_7: 0.8267  loss_dice_7: 0.9316  loss_ce_8: 0.1768  loss_cate_8: 0  loss_mask_8: 0.8235  loss_dice_8: 0.9551  time: 1.1018  data_time: 0.0168  lr: 9.2771e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:15:19 d2.utils.events]: \u001b[0m eta: 22:28:19  iter: 6419  total_loss: 21.39  loss_ce: 0.2634  loss_cate: 0.1611  loss_mask: 0.7663  loss_dice: 1.012  loss_ce_0: 0.3958  loss_cate_0: 0  loss_mask_0: 0.7633  loss_dice_0: 0.9895  loss_ce_1: 0.2384  loss_cate_1: 0  loss_mask_1: 0.745  loss_dice_1: 0.9817  loss_ce_2: 0.2748  loss_cate_2: 0  loss_mask_2: 0.7763  loss_dice_2: 0.9846  loss_ce_3: 0.2547  loss_cate_3: 0  loss_mask_3: 0.7653  loss_dice_3: 0.9658  loss_ce_4: 0.2298  loss_cate_4: 0  loss_mask_4: 0.7447  loss_dice_4: 0.9878  loss_ce_5: 0.2434  loss_cate_5: 0  loss_mask_5: 0.756  loss_dice_5: 1.023  loss_ce_6: 0.2605  loss_cate_6: 0  loss_mask_6: 0.76  loss_dice_6: 0.9666  loss_ce_7: 0.2145  loss_cate_7: 0  loss_mask_7: 0.7668  loss_dice_7: 1.109  loss_ce_8: 0.2043  loss_cate_8: 0  loss_mask_8: 0.7534  loss_dice_8: 1.036  time: 1.1019  data_time: 0.0154  lr: 9.2749e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:15:41 d2.utils.events]: \u001b[0m eta: 22:27:52  iter: 6439  total_loss: 22.54  loss_ce: 0.3324  loss_cate: 0.2037  loss_mask: 0.895  loss_dice: 0.9601  loss_ce_0: 0.4611  loss_cate_0: 0  loss_mask_0: 0.9326  loss_dice_0: 0.9824  loss_ce_1: 0.3356  loss_cate_1: 0  loss_mask_1: 0.8969  loss_dice_1: 0.8925  loss_ce_2: 0.3654  loss_cate_2: 0  loss_mask_2: 0.9136  loss_dice_2: 0.9569  loss_ce_3: 0.3284  loss_cate_3: 0  loss_mask_3: 0.9151  loss_dice_3: 1.043  loss_ce_4: 0.3065  loss_cate_4: 0  loss_mask_4: 0.8952  loss_dice_4: 0.9936  loss_ce_5: 0.3102  loss_cate_5: 0  loss_mask_5: 0.9125  loss_dice_5: 1.025  loss_ce_6: 0.3455  loss_cate_6: 0  loss_mask_6: 0.8744  loss_dice_6: 0.9308  loss_ce_7: 0.3232  loss_cate_7: 0  loss_mask_7: 0.8877  loss_dice_7: 0.9543  loss_ce_8: 0.3369  loss_cate_8: 0  loss_mask_8: 0.9247  loss_dice_8: 0.9903  time: 1.1019  data_time: 0.0131  lr: 9.2726e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:16:05 d2.utils.events]: \u001b[0m eta: 22:27:24  iter: 6459  total_loss: 21.54  loss_ce: 0.3091  loss_cate: 0.1603  loss_mask: 0.7992  loss_dice: 0.9827  loss_ce_0: 0.3263  loss_cate_0: 0  loss_mask_0: 0.7828  loss_dice_0: 1.055  loss_ce_1: 0.2291  loss_cate_1: 0  loss_mask_1: 0.7869  loss_dice_1: 1.022  loss_ce_2: 0.2515  loss_cate_2: 0  loss_mask_2: 0.8326  loss_dice_2: 0.9959  loss_ce_3: 0.2379  loss_cate_3: 0  loss_mask_3: 0.7624  loss_dice_3: 1.016  loss_ce_4: 0.2959  loss_cate_4: 0  loss_mask_4: 0.7825  loss_dice_4: 1.017  loss_ce_5: 0.2615  loss_cate_5: 0  loss_mask_5: 0.8105  loss_dice_5: 1.018  loss_ce_6: 0.2706  loss_cate_6: 0  loss_mask_6: 0.8324  loss_dice_6: 0.9946  loss_ce_7: 0.2298  loss_cate_7: 0  loss_mask_7: 0.8647  loss_dice_7: 0.9851  loss_ce_8: 0.2186  loss_cate_8: 0  loss_mask_8: 0.8294  loss_dice_8: 0.997  time: 1.1018  data_time: 0.0131  lr: 9.2703e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:16:28 d2.utils.events]: \u001b[0m eta: 22:27:08  iter: 6479  total_loss: 20.95  loss_ce: 0.2732  loss_cate: 0.1659  loss_mask: 0.7422  loss_dice: 0.9601  loss_ce_0: 0.3472  loss_cate_0: 0  loss_mask_0: 0.7677  loss_dice_0: 1.061  loss_ce_1: 0.2747  loss_cate_1: 0  loss_mask_1: 0.7401  loss_dice_1: 0.944  loss_ce_2: 0.3165  loss_cate_2: 0  loss_mask_2: 0.7597  loss_dice_2: 0.9644  loss_ce_3: 0.2679  loss_cate_3: 0  loss_mask_3: 0.75  loss_dice_3: 0.9775  loss_ce_4: 0.267  loss_cate_4: 0  loss_mask_4: 0.7289  loss_dice_4: 0.9806  loss_ce_5: 0.2604  loss_cate_5: 0  loss_mask_5: 0.7439  loss_dice_5: 0.9949  loss_ce_6: 0.2713  loss_cate_6: 0  loss_mask_6: 0.7346  loss_dice_6: 0.9764  loss_ce_7: 0.2559  loss_cate_7: 0  loss_mask_7: 0.7249  loss_dice_7: 0.9695  loss_ce_8: 0.2869  loss_cate_8: 0  loss_mask_8: 0.7381  loss_dice_8: 0.9559  time: 1.1019  data_time: 0.0146  lr: 9.2681e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:16:50 d2.utils.events]: \u001b[0m eta: 22:26:39  iter: 6499  total_loss: 22.41  loss_ce: 0.317  loss_cate: 0.1721  loss_mask: 0.8907  loss_dice: 1.016  loss_ce_0: 0.3679  loss_cate_0: 0  loss_mask_0: 0.8894  loss_dice_0: 1.061  loss_ce_1: 0.2835  loss_cate_1: 0  loss_mask_1: 0.898  loss_dice_1: 1.032  loss_ce_2: 0.3115  loss_cate_2: 0  loss_mask_2: 0.8832  loss_dice_2: 1.002  loss_ce_3: 0.3055  loss_cate_3: 0  loss_mask_3: 0.8995  loss_dice_3: 1.039  loss_ce_4: 0.3071  loss_cate_4: 0  loss_mask_4: 0.902  loss_dice_4: 1.006  loss_ce_5: 0.2631  loss_cate_5: 0  loss_mask_5: 0.897  loss_dice_5: 1.068  loss_ce_6: 0.2897  loss_cate_6: 0  loss_mask_6: 0.8904  loss_dice_6: 1.02  loss_ce_7: 0.2738  loss_cate_7: 0  loss_mask_7: 0.8934  loss_dice_7: 1.031  loss_ce_8: 0.2816  loss_cate_8: 0  loss_mask_8: 0.8868  loss_dice_8: 1.017  time: 1.1019  data_time: 0.0122  lr: 9.2658e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:17:14 d2.utils.events]: \u001b[0m eta: 22:26:38  iter: 6519  total_loss: 24.72  loss_ce: 0.2349  loss_cate: 0.2066  loss_mask: 0.901  loss_dice: 1.042  loss_ce_0: 0.3626  loss_cate_0: 0  loss_mask_0: 1.009  loss_dice_0: 1.128  loss_ce_1: 0.3357  loss_cate_1: 0  loss_mask_1: 1.03  loss_dice_1: 1.09  loss_ce_2: 0.3042  loss_cate_2: 0  loss_mask_2: 1.005  loss_dice_2: 1.035  loss_ce_3: 0.2785  loss_cate_3: 0  loss_mask_3: 0.9588  loss_dice_3: 1.084  loss_ce_4: 0.2727  loss_cate_4: 0  loss_mask_4: 0.9246  loss_dice_4: 1.042  loss_ce_5: 0.2335  loss_cate_5: 0  loss_mask_5: 0.9238  loss_dice_5: 1.059  loss_ce_6: 0.2657  loss_cate_6: 0  loss_mask_6: 0.9118  loss_dice_6: 1.016  loss_ce_7: 0.3029  loss_cate_7: 0  loss_mask_7: 0.9106  loss_dice_7: 1.027  loss_ce_8: 0.2482  loss_cate_8: 0  loss_mask_8: 0.9063  loss_dice_8: 1.042  time: 1.1019  data_time: 0.0151  lr: 9.2635e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:17:38 d2.utils.events]: \u001b[0m eta: 22:26:18  iter: 6539  total_loss: 18.79  loss_ce: 0.2694  loss_cate: 0.1587  loss_mask: 0.734  loss_dice: 0.8902  loss_ce_0: 0.3727  loss_cate_0: 0  loss_mask_0: 0.7645  loss_dice_0: 0.9614  loss_ce_1: 0.2879  loss_cate_1: 0  loss_mask_1: 0.7032  loss_dice_1: 0.8953  loss_ce_2: 0.2384  loss_cate_2: 0  loss_mask_2: 0.709  loss_dice_2: 0.9031  loss_ce_3: 0.2192  loss_cate_3: 0  loss_mask_3: 0.7254  loss_dice_3: 0.9008  loss_ce_4: 0.2115  loss_cate_4: 0  loss_mask_4: 0.7046  loss_dice_4: 0.9179  loss_ce_5: 0.2184  loss_cate_5: 0  loss_mask_5: 0.7221  loss_dice_5: 0.9111  loss_ce_6: 0.2624  loss_cate_6: 0  loss_mask_6: 0.6991  loss_dice_6: 0.9044  loss_ce_7: 0.2693  loss_cate_7: 0  loss_mask_7: 0.7423  loss_dice_7: 0.9378  loss_ce_8: 0.2515  loss_cate_8: 0  loss_mask_8: 0.7161  loss_dice_8: 0.9184  time: 1.1019  data_time: 0.0140  lr: 9.2613e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:18:02 d2.utils.events]: \u001b[0m eta: 22:26:01  iter: 6559  total_loss: 22.06  loss_ce: 0.2264  loss_cate: 0.1609  loss_mask: 0.8122  loss_dice: 1.033  loss_ce_0: 0.321  loss_cate_0: 0  loss_mask_0: 0.8017  loss_dice_0: 1.034  loss_ce_1: 0.2186  loss_cate_1: 0  loss_mask_1: 0.8747  loss_dice_1: 1.037  loss_ce_2: 0.2337  loss_cate_2: 0  loss_mask_2: 0.808  loss_dice_2: 1.007  loss_ce_3: 0.2254  loss_cate_3: 0  loss_mask_3: 0.8194  loss_dice_3: 1.067  loss_ce_4: 0.2058  loss_cate_4: 0  loss_mask_4: 0.8248  loss_dice_4: 1.047  loss_ce_5: 0.2163  loss_cate_5: 0  loss_mask_5: 0.8072  loss_dice_5: 1.019  loss_ce_6: 0.2149  loss_cate_6: 0  loss_mask_6: 0.81  loss_dice_6: 1.025  loss_ce_7: 0.2219  loss_cate_7: 0  loss_mask_7: 0.7986  loss_dice_7: 1.012  loss_ce_8: 0.2201  loss_cate_8: 0  loss_mask_8: 0.818  loss_dice_8: 1.018  time: 1.1019  data_time: 0.0149  lr: 9.259e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:18:58 d2.utils.events]: \u001b[0m eta: 22:25:15  iter: 6599  total_loss: 22.7  loss_ce: 0.2453  loss_cate: 0.1789  loss_mask: 0.8798  loss_dice: 1.081  loss_ce_0: 0.3271  loss_cate_0: 0  loss_mask_0: 0.9355  loss_dice_0: 1.153  loss_ce_1: 0.2516  loss_cate_1: 0  loss_mask_1: 0.9549  loss_dice_1: 1.126  loss_ce_2: 0.2314  loss_cate_2: 0  loss_mask_2: 0.956  loss_dice_2: 1.159  loss_ce_3: 0.234  loss_cate_3: 0  loss_mask_3: 1.009  loss_dice_3: 1.089  loss_ce_4: 0.2327  loss_cate_4: 0  loss_mask_4: 0.9572  loss_dice_4: 1.143  loss_ce_5: 0.2612  loss_cate_5: 0  loss_mask_5: 0.9728  loss_dice_5: 1.166  loss_ce_6: 0.2487  loss_cate_6: 0  loss_mask_6: 0.9491  loss_dice_6: 1.126  loss_ce_7: 0.1614  loss_cate_7: 0  loss_mask_7: 0.9473  loss_dice_7: 1.107  loss_ce_8: 0.1746  loss_cate_8: 0  loss_mask_8: 0.8981  loss_dice_8: 1.108  time: 1.1019  data_time: 0.0131  lr: 9.2545e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:19:21 d2.utils.events]: \u001b[0m eta: 22:24:47  iter: 6619  total_loss: 21.77  loss_ce: 0.2636  loss_cate: 0.1628  loss_mask: 0.7499  loss_dice: 1.098  loss_ce_0: 0.3777  loss_cate_0: 0  loss_mask_0: 0.774  loss_dice_0: 1.103  loss_ce_1: 0.2477  loss_cate_1: 0  loss_mask_1: 0.7405  loss_dice_1: 1.024  loss_ce_2: 0.2603  loss_cate_2: 0  loss_mask_2: 0.7682  loss_dice_2: 1.047  loss_ce_3: 0.2422  loss_cate_3: 0  loss_mask_3: 0.7804  loss_dice_3: 1.074  loss_ce_4: 0.2452  loss_cate_4: 0  loss_mask_4: 0.7665  loss_dice_4: 1.044  loss_ce_5: 0.2374  loss_cate_5: 0  loss_mask_5: 0.7567  loss_dice_5: 1.065  loss_ce_6: 0.225  loss_cate_6: 0  loss_mask_6: 0.7674  loss_dice_6: 1.109  loss_ce_7: 0.2366  loss_cate_7: 0  loss_mask_7: 0.7519  loss_dice_7: 1.092  loss_ce_8: 0.2383  loss_cate_8: 0  loss_mask_8: 0.749  loss_dice_8: 1.087  time: 1.1019  data_time: 0.0148  lr: 9.2522e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:19:49 d2.utils.events]: \u001b[0m eta: 22:24:28  iter: 6639  total_loss: 24.64  loss_ce: 0.2636  loss_cate: 0.1636  loss_mask: 0.8432  loss_dice: 1.123  loss_ce_0: 0.4876  loss_cate_0: 0  loss_mask_0: 0.8341  loss_dice_0: 1.19  loss_ce_1: 0.3005  loss_cate_1: 0  loss_mask_1: 0.842  loss_dice_1: 1.241  loss_ce_2: 0.3442  loss_cate_2: 0  loss_mask_2: 0.8784  loss_dice_2: 1.164  loss_ce_3: 0.2933  loss_cate_3: 0  loss_mask_3: 0.8313  loss_dice_3: 1.154  loss_ce_4: 0.3145  loss_cate_4: 0  loss_mask_4: 0.8266  loss_dice_4: 1.177  loss_ce_5: 0.3131  loss_cate_5: 0  loss_mask_5: 0.8384  loss_dice_5: 1.147  loss_ce_6: 0.2949  loss_cate_6: 0  loss_mask_6: 0.8025  loss_dice_6: 1.126  loss_ce_7: 0.2854  loss_cate_7: 0  loss_mask_7: 0.8199  loss_dice_7: 1.142  loss_ce_8: 0.2458  loss_cate_8: 0  loss_mask_8: 0.8266  loss_dice_8: 1.162  time: 1.1019  data_time: 0.0145  lr: 9.2499e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:20:14 d2.utils.events]: \u001b[0m eta: 22:23:53  iter: 6659  total_loss: 20.42  loss_ce: 0.1921  loss_cate: 0.1658  loss_mask: 0.8469  loss_dice: 0.9398  loss_ce_0: 0.3365  loss_cate_0: 0  loss_mask_0: 0.8505  loss_dice_0: 0.9543  loss_ce_1: 0.2935  loss_cate_1: 0  loss_mask_1: 0.8434  loss_dice_1: 0.9759  loss_ce_2: 0.1808  loss_cate_2: 0  loss_mask_2: 0.8082  loss_dice_2: 0.979  loss_ce_3: 0.1767  loss_cate_3: 0  loss_mask_3: 0.8272  loss_dice_3: 0.9488  loss_ce_4: 0.1598  loss_cate_4: 0  loss_mask_4: 0.8181  loss_dice_4: 0.9366  loss_ce_5: 0.206  loss_cate_5: 0  loss_mask_5: 0.8347  loss_dice_5: 0.9608  loss_ce_6: 0.1757  loss_cate_6: 0  loss_mask_6: 0.835  loss_dice_6: 0.962  loss_ce_7: 0.2352  loss_cate_7: 0  loss_mask_7: 0.8664  loss_dice_7: 1.047  loss_ce_8: 0.1982  loss_cate_8: 0  loss_mask_8: 0.8278  loss_dice_8: 0.9724  time: 1.1019  data_time: 0.0146  lr: 9.2476e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:20:37 d2.utils.events]: \u001b[0m eta: 22:23:35  iter: 6679  total_loss: 21.83  loss_ce: 0.2433  loss_cate: 0.1636  loss_mask: 0.8286  loss_dice: 1.064  loss_ce_0: 0.3797  loss_cate_0: 0  loss_mask_0: 0.8775  loss_dice_0: 1.145  loss_ce_1: 0.247  loss_cate_1: 0  loss_mask_1: 0.8212  loss_dice_1: 1.111  loss_ce_2: 0.2447  loss_cate_2: 0  loss_mask_2: 0.8204  loss_dice_2: 1.069  loss_ce_3: 0.2806  loss_cate_3: 0  loss_mask_3: 0.8215  loss_dice_3: 1.069  loss_ce_4: 0.2465  loss_cate_4: 0  loss_mask_4: 0.7874  loss_dice_4: 1.077  loss_ce_5: 0.2653  loss_cate_5: 0  loss_mask_5: 0.8337  loss_dice_5: 0.9687  loss_ce_6: 0.247  loss_cate_6: 0  loss_mask_6: 0.8085  loss_dice_6: 1.042  loss_ce_7: 0.2365  loss_cate_7: 0  loss_mask_7: 0.8143  loss_dice_7: 1.029  loss_ce_8: 0.2091  loss_cate_8: 0  loss_mask_8: 0.8422  loss_dice_8: 1.064  time: 1.1019  data_time: 0.0140  lr: 9.2454e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:21:00 d2.utils.events]: \u001b[0m eta: 22:22:48  iter: 6699  total_loss: 23.47  loss_ce: 0.2415  loss_cate: 0.182  loss_mask: 0.9355  loss_dice: 1.053  loss_ce_0: 0.4499  loss_cate_0: 0  loss_mask_0: 0.9215  loss_dice_0: 1.072  loss_ce_1: 0.3329  loss_cate_1: 0  loss_mask_1: 0.9522  loss_dice_1: 1.037  loss_ce_2: 0.3226  loss_cate_2: 0  loss_mask_2: 0.9299  loss_dice_2: 1.016  loss_ce_3: 0.3393  loss_cate_3: 0  loss_mask_3: 0.9068  loss_dice_3: 1.003  loss_ce_4: 0.294  loss_cate_4: 0  loss_mask_4: 0.9172  loss_dice_4: 0.9989  loss_ce_5: 0.3045  loss_cate_5: 0  loss_mask_5: 0.9447  loss_dice_5: 0.9683  loss_ce_6: 0.2874  loss_cate_6: 0  loss_mask_6: 0.9416  loss_dice_6: 0.9773  loss_ce_7: 0.2689  loss_cate_7: 0  loss_mask_7: 0.9429  loss_dice_7: 0.9997  loss_ce_8: 0.2721  loss_cate_8: 0  loss_mask_8: 0.9523  loss_dice_8: 1.025  time: 1.1019  data_time: 0.0123  lr: 9.2431e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:21:24 d2.utils.events]: \u001b[0m eta: 22:22:26  iter: 6719  total_loss: 19.95  loss_ce: 0.353  loss_cate: 0.1443  loss_mask: 0.7575  loss_dice: 1.006  loss_ce_0: 0.3597  loss_cate_0: 0  loss_mask_0: 0.7432  loss_dice_0: 1.078  loss_ce_1: 0.3751  loss_cate_1: 0  loss_mask_1: 0.7682  loss_dice_1: 0.986  loss_ce_2: 0.3569  loss_cate_2: 0  loss_mask_2: 0.7371  loss_dice_2: 1.038  loss_ce_3: 0.3463  loss_cate_3: 0  loss_mask_3: 0.7755  loss_dice_3: 0.9607  loss_ce_4: 0.3345  loss_cate_4: 0  loss_mask_4: 0.7625  loss_dice_4: 0.9726  loss_ce_5: 0.3239  loss_cate_5: 0  loss_mask_5: 0.7743  loss_dice_5: 0.993  loss_ce_6: 0.3086  loss_cate_6: 0  loss_mask_6: 0.7641  loss_dice_6: 1.035  loss_ce_7: 0.346  loss_cate_7: 0  loss_mask_7: 0.7473  loss_dice_7: 0.9831  loss_ce_8: 0.3165  loss_cate_8: 0  loss_mask_8: 0.779  loss_dice_8: 1.019  time: 1.1019  data_time: 0.0148  lr: 9.2408e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:21:48 d2.utils.events]: \u001b[0m eta: 22:22:03  iter: 6739  total_loss: 21.33  loss_ce: 0.2504  loss_cate: 0.1749  loss_mask: 0.9337  loss_dice: 1.043  loss_ce_0: 0.3185  loss_cate_0: 0  loss_mask_0: 0.954  loss_dice_0: 1.071  loss_ce_1: 0.2253  loss_cate_1: 0  loss_mask_1: 0.8724  loss_dice_1: 1.039  loss_ce_2: 0.3385  loss_cate_2: 0  loss_mask_2: 0.8338  loss_dice_2: 0.9994  loss_ce_3: 0.2267  loss_cate_3: 0  loss_mask_3: 0.8683  loss_dice_3: 1.035  loss_ce_4: 0.255  loss_cate_4: 0  loss_mask_4: 0.8963  loss_dice_4: 1.05  loss_ce_5: 0.2599  loss_cate_5: 0  loss_mask_5: 0.8974  loss_dice_5: 1.058  loss_ce_6: 0.282  loss_cate_6: 0  loss_mask_6: 0.9102  loss_dice_6: 1.042  loss_ce_7: 0.2264  loss_cate_7: 0  loss_mask_7: 0.956  loss_dice_7: 1.055  loss_ce_8: 0.2604  loss_cate_8: 0  loss_mask_8: 0.9301  loss_dice_8: 1.039  time: 1.1021  data_time: 0.0747  lr: 9.2386e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:22:11 d2.utils.events]: \u001b[0m eta: 22:21:41  iter: 6759  total_loss: 21.71  loss_ce: 0.2353  loss_cate: 0.1406  loss_mask: 0.8234  loss_dice: 1.062  loss_ce_0: 0.2871  loss_cate_0: 0  loss_mask_0: 0.8194  loss_dice_0: 1.093  loss_ce_1: 0.2415  loss_cate_1: 0  loss_mask_1: 0.8353  loss_dice_1: 1.081  loss_ce_2: 0.2225  loss_cate_2: 0  loss_mask_2: 0.8235  loss_dice_2: 1.059  loss_ce_3: 0.1997  loss_cate_3: 0  loss_mask_3: 0.8168  loss_dice_3: 1.089  loss_ce_4: 0.2595  loss_cate_4: 0  loss_mask_4: 0.8641  loss_dice_4: 1.083  loss_ce_5: 0.1934  loss_cate_5: 0  loss_mask_5: 0.839  loss_dice_5: 1.101  loss_ce_6: 0.2052  loss_cate_6: 0  loss_mask_6: 0.8332  loss_dice_6: 1.121  loss_ce_7: 0.1739  loss_cate_7: 0  loss_mask_7: 0.8228  loss_dice_7: 1.096  loss_ce_8: 0.1816  loss_cate_8: 0  loss_mask_8: 0.8433  loss_dice_8: 1.058  time: 1.1021  data_time: 0.0139  lr: 9.2363e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:22:34 d2.utils.events]: \u001b[0m eta: 22:21:16  iter: 6779  total_loss: 20.39  loss_ce: 0.1728  loss_cate: 0.1606  loss_mask: 0.7658  loss_dice: 1.024  loss_ce_0: 0.2968  loss_cate_0: 0  loss_mask_0: 0.8361  loss_dice_0: 1.08  loss_ce_1: 0.1809  loss_cate_1: 0  loss_mask_1: 0.7721  loss_dice_1: 1.072  loss_ce_2: 0.1671  loss_cate_2: 0  loss_mask_2: 0.7344  loss_dice_2: 1.048  loss_ce_3: 0.1451  loss_cate_3: 0  loss_mask_3: 0.7725  loss_dice_3: 1.025  loss_ce_4: 0.1628  loss_cate_4: 0  loss_mask_4: 0.7847  loss_dice_4: 1.026  loss_ce_5: 0.2039  loss_cate_5: 0  loss_mask_5: 0.7641  loss_dice_5: 1.05  loss_ce_6: 0.1791  loss_cate_6: 0  loss_mask_6: 0.7731  loss_dice_6: 1.025  loss_ce_7: 0.1871  loss_cate_7: 0  loss_mask_7: 0.7606  loss_dice_7: 1.044  loss_ce_8: 0.1804  loss_cate_8: 0  loss_mask_8: 0.7954  loss_dice_8: 1.03  time: 1.1021  data_time: 0.0135  lr: 9.234e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:22:58 d2.utils.events]: \u001b[0m eta: 22:20:52  iter: 6799  total_loss: 19.74  loss_ce: 0.2206  loss_cate: 0.1532  loss_mask: 0.7681  loss_dice: 1.097  loss_ce_0: 0.3974  loss_cate_0: 0  loss_mask_0: 0.8208  loss_dice_0: 1.111  loss_ce_1: 0.266  loss_cate_1: 0  loss_mask_1: 0.7916  loss_dice_1: 1.076  loss_ce_2: 0.2272  loss_cate_2: 0  loss_mask_2: 0.7878  loss_dice_2: 1.054  loss_ce_3: 0.229  loss_cate_3: 0  loss_mask_3: 0.7899  loss_dice_3: 1.091  loss_ce_4: 0.194  loss_cate_4: 0  loss_mask_4: 0.7912  loss_dice_4: 1.099  loss_ce_5: 0.2778  loss_cate_5: 0  loss_mask_5: 0.7916  loss_dice_5: 1.08  loss_ce_6: 0.2606  loss_cate_6: 0  loss_mask_6: 0.7917  loss_dice_6: 1.066  loss_ce_7: 0.2318  loss_cate_7: 0  loss_mask_7: 0.777  loss_dice_7: 1.084  loss_ce_8: 0.2038  loss_cate_8: 0  loss_mask_8: 0.7981  loss_dice_8: 1.065  time: 1.1022  data_time: 0.0574  lr: 9.2318e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:23:25 d2.utils.events]: \u001b[0m eta: 22:20:14  iter: 6819  total_loss: 23.65  loss_ce: 0.2955  loss_cate: 0.1864  loss_mask: 0.9289  loss_dice: 0.9788  loss_ce_0: 0.4402  loss_cate_0: 0  loss_mask_0: 0.9528  loss_dice_0: 1.079  loss_ce_1: 0.4577  loss_cate_1: 0  loss_mask_1: 0.9147  loss_dice_1: 0.9575  loss_ce_2: 0.3725  loss_cate_2: 0  loss_mask_2: 0.9245  loss_dice_2: 0.9488  loss_ce_3: 0.3697  loss_cate_3: 0  loss_mask_3: 0.9335  loss_dice_3: 0.984  loss_ce_4: 0.4081  loss_cate_4: 0  loss_mask_4: 0.9239  loss_dice_4: 0.9841  loss_ce_5: 0.3683  loss_cate_5: 0  loss_mask_5: 0.9298  loss_dice_5: 1.005  loss_ce_6: 0.3305  loss_cate_6: 0  loss_mask_6: 0.9423  loss_dice_6: 0.9927  loss_ce_7: 0.3297  loss_cate_7: 0  loss_mask_7: 0.9336  loss_dice_7: 1.01  loss_ce_8: 0.3315  loss_cate_8: 0  loss_mask_8: 0.944  loss_dice_8: 1.007  time: 1.1024  data_time: 0.0857  lr: 9.2295e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:23:50 d2.utils.events]: \u001b[0m eta: 22:19:51  iter: 6839  total_loss: 22.23  loss_ce: 0.2886  loss_cate: 0.2004  loss_mask: 0.7313  loss_dice: 0.9458  loss_ce_0: 0.3997  loss_cate_0: 0  loss_mask_0: 0.7552  loss_dice_0: 1.054  loss_ce_1: 0.3418  loss_cate_1: 0  loss_mask_1: 0.7177  loss_dice_1: 1.025  loss_ce_2: 0.4027  loss_cate_2: 0  loss_mask_2: 0.7536  loss_dice_2: 0.9655  loss_ce_3: 0.3264  loss_cate_3: 0  loss_mask_3: 0.7364  loss_dice_3: 1.014  loss_ce_4: 0.2847  loss_cate_4: 0  loss_mask_4: 0.7243  loss_dice_4: 1.008  loss_ce_5: 0.3233  loss_cate_5: 0  loss_mask_5: 0.7209  loss_dice_5: 1.009  loss_ce_6: 0.3078  loss_cate_6: 0  loss_mask_6: 0.7375  loss_dice_6: 0.9965  loss_ce_7: 0.2878  loss_cate_7: 0  loss_mask_7: 0.7292  loss_dice_7: 0.9826  loss_ce_8: 0.2853  loss_cate_8: 0  loss_mask_8: 0.7288  loss_dice_8: 0.9813  time: 1.1024  data_time: 0.0145  lr: 9.2272e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:24:14 d2.utils.events]: \u001b[0m eta: 22:19:30  iter: 6859  total_loss: 22.74  loss_ce: 0.2306  loss_cate: 0.1679  loss_mask: 0.8827  loss_dice: 1.036  loss_ce_0: 0.3851  loss_cate_0: 0  loss_mask_0: 0.9027  loss_dice_0: 1.133  loss_ce_1: 0.2511  loss_cate_1: 0  loss_mask_1: 0.9115  loss_dice_1: 1.1  loss_ce_2: 0.2566  loss_cate_2: 0  loss_mask_2: 0.8723  loss_dice_2: 1.108  loss_ce_3: 0.2254  loss_cate_3: 0  loss_mask_3: 0.8709  loss_dice_3: 1.088  loss_ce_4: 0.2503  loss_cate_4: 0  loss_mask_4: 0.8628  loss_dice_4: 1.056  loss_ce_5: 0.2207  loss_cate_5: 0  loss_mask_5: 0.878  loss_dice_5: 1.072  loss_ce_6: 0.2279  loss_cate_6: 0  loss_mask_6: 0.8869  loss_dice_6: 1.047  loss_ce_7: 0.2558  loss_cate_7: 0  loss_mask_7: 0.8643  loss_dice_7: 1.064  loss_ce_8: 0.2474  loss_cate_8: 0  loss_mask_8: 0.8919  loss_dice_8: 1.048  time: 1.1024  data_time: 0.0136  lr: 9.2249e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:24:38 d2.utils.events]: \u001b[0m eta: 22:19:13  iter: 6879  total_loss: 22.38  loss_ce: 0.2399  loss_cate: 0.1849  loss_mask: 0.8473  loss_dice: 1.024  loss_ce_0: 0.3117  loss_cate_0: 0  loss_mask_0: 0.9359  loss_dice_0: 1.138  loss_ce_1: 0.2389  loss_cate_1: 0  loss_mask_1: 0.9023  loss_dice_1: 1.108  loss_ce_2: 0.2291  loss_cate_2: 0  loss_mask_2: 0.8804  loss_dice_2: 1.039  loss_ce_3: 0.2296  loss_cate_3: 0  loss_mask_3: 0.8574  loss_dice_3: 0.9921  loss_ce_4: 0.2221  loss_cate_4: 0  loss_mask_4: 0.8574  loss_dice_4: 1.012  loss_ce_5: 0.2463  loss_cate_5: 0  loss_mask_5: 0.8688  loss_dice_5: 1.017  loss_ce_6: 0.2204  loss_cate_6: 0  loss_mask_6: 0.8603  loss_dice_6: 1.028  loss_ce_7: 0.2208  loss_cate_7: 0  loss_mask_7: 0.8658  loss_dice_7: 1.026  loss_ce_8: 0.2253  loss_cate_8: 0  loss_mask_8: 0.8598  loss_dice_8: 0.9943  time: 1.1024  data_time: 0.0139  lr: 9.2227e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:25:02 d2.utils.events]: \u001b[0m eta: 22:18:44  iter: 6899  total_loss: 23.17  loss_ce: 0.2601  loss_cate: 0.1696  loss_mask: 0.8464  loss_dice: 1.1  loss_ce_0: 0.3031  loss_cate_0: 0  loss_mask_0: 0.8929  loss_dice_0: 1.197  loss_ce_1: 0.3343  loss_cate_1: 0  loss_mask_1: 0.8882  loss_dice_1: 1.125  loss_ce_2: 0.3488  loss_cate_2: 0  loss_mask_2: 0.837  loss_dice_2: 1.072  loss_ce_3: 0.2409  loss_cate_3: 0  loss_mask_3: 0.8879  loss_dice_3: 1.085  loss_ce_4: 0.2726  loss_cate_4: 0  loss_mask_4: 0.8735  loss_dice_4: 1.081  loss_ce_5: 0.2845  loss_cate_5: 0  loss_mask_5: 0.8228  loss_dice_5: 1.094  loss_ce_6: 0.2805  loss_cate_6: 0  loss_mask_6: 0.8399  loss_dice_6: 1.079  loss_ce_7: 0.2596  loss_cate_7: 0  loss_mask_7: 0.8384  loss_dice_7: 1.108  loss_ce_8: 0.2795  loss_cate_8: 0  loss_mask_8: 0.8461  loss_dice_8: 1.1  time: 1.1024  data_time: 0.0128  lr: 9.2204e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:25:27 d2.utils.events]: \u001b[0m eta: 22:18:25  iter: 6919  total_loss: 23.22  loss_ce: 0.2571  loss_cate: 0.1748  loss_mask: 0.8266  loss_dice: 1.013  loss_ce_0: 0.3848  loss_cate_0: 0  loss_mask_0: 0.9189  loss_dice_0: 1.068  loss_ce_1: 0.2523  loss_cate_1: 0  loss_mask_1: 0.8845  loss_dice_1: 1.068  loss_ce_2: 0.2391  loss_cate_2: 0  loss_mask_2: 0.8795  loss_dice_2: 1.065  loss_ce_3: 0.2487  loss_cate_3: 0  loss_mask_3: 0.8733  loss_dice_3: 1.048  loss_ce_4: 0.2414  loss_cate_4: 0  loss_mask_4: 0.8793  loss_dice_4: 1.069  loss_ce_5: 0.2697  loss_cate_5: 0  loss_mask_5: 0.8925  loss_dice_5: 1.074  loss_ce_6: 0.256  loss_cate_6: 0  loss_mask_6: 0.8763  loss_dice_6: 0.9831  loss_ce_7: 0.2232  loss_cate_7: 0  loss_mask_7: 0.8902  loss_dice_7: 1.015  loss_ce_8: 0.2659  loss_cate_8: 0  loss_mask_8: 0.8314  loss_dice_8: 1.015  time: 1.1024  data_time: 0.0146  lr: 9.2181e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:25:53 d2.utils.events]: \u001b[0m eta: 22:18:04  iter: 6939  total_loss: 21.62  loss_ce: 0.2287  loss_cate: 0.1704  loss_mask: 0.852  loss_dice: 1.038  loss_ce_0: 0.3482  loss_cate_0: 0  loss_mask_0: 0.8059  loss_dice_0: 1.092  loss_ce_1: 0.2592  loss_cate_1: 0  loss_mask_1: 0.8031  loss_dice_1: 1.055  loss_ce_2: 0.2749  loss_cate_2: 0  loss_mask_2: 0.8296  loss_dice_2: 1.036  loss_ce_3: 0.2463  loss_cate_3: 0  loss_mask_3: 0.8373  loss_dice_3: 1.054  loss_ce_4: 0.2284  loss_cate_4: 0  loss_mask_4: 0.8711  loss_dice_4: 1.035  loss_ce_5: 0.2085  loss_cate_5: 0  loss_mask_5: 0.8421  loss_dice_5: 1.035  loss_ce_6: 0.2308  loss_cate_6: 0  loss_mask_6: 0.8378  loss_dice_6: 0.997  loss_ce_7: 0.2301  loss_cate_7: 0  loss_mask_7: 0.8591  loss_dice_7: 0.9929  loss_ce_8: 0.2314  loss_cate_8: 0  loss_mask_8: 0.8676  loss_dice_8: 1.017  time: 1.1024  data_time: 0.0135  lr: 9.2159e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:26:21 d2.utils.events]: \u001b[0m eta: 22:17:39  iter: 6959  total_loss: 19.17  loss_ce: 0.2243  loss_cate: 0.1571  loss_mask: 0.8117  loss_dice: 0.9034  loss_ce_0: 0.3711  loss_cate_0: 0  loss_mask_0: 0.8069  loss_dice_0: 0.8733  loss_ce_1: 0.2785  loss_cate_1: 0  loss_mask_1: 0.8209  loss_dice_1: 0.9066  loss_ce_2: 0.2233  loss_cate_2: 0  loss_mask_2: 0.8108  loss_dice_2: 0.9106  loss_ce_3: 0.2271  loss_cate_3: 0  loss_mask_3: 0.8016  loss_dice_3: 0.887  loss_ce_4: 0.2376  loss_cate_4: 0  loss_mask_4: 0.8058  loss_dice_4: 0.8577  loss_ce_5: 0.2308  loss_cate_5: 0  loss_mask_5: 0.8103  loss_dice_5: 0.8789  loss_ce_6: 0.2371  loss_cate_6: 0  loss_mask_6: 0.7821  loss_dice_6: 0.8954  loss_ce_7: 0.2384  loss_cate_7: 0  loss_mask_7: 0.789  loss_dice_7: 0.9106  loss_ce_8: 0.2214  loss_cate_8: 0  loss_mask_8: 0.806  loss_dice_8: 0.898  time: 1.1024  data_time: 0.0130  lr: 9.2136e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:26:45 d2.utils.events]: \u001b[0m eta: 22:17:23  iter: 6979  total_loss: 20.66  loss_ce: 0.2408  loss_cate: 0.163  loss_mask: 0.6939  loss_dice: 0.9114  loss_ce_0: 0.3952  loss_cate_0: 0  loss_mask_0: 0.7218  loss_dice_0: 1.036  loss_ce_1: 0.2881  loss_cate_1: 0  loss_mask_1: 0.7494  loss_dice_1: 0.9856  loss_ce_2: 0.2474  loss_cate_2: 0  loss_mask_2: 0.6867  loss_dice_2: 0.9957  loss_ce_3: 0.2674  loss_cate_3: 0  loss_mask_3: 0.7294  loss_dice_3: 0.9656  loss_ce_4: 0.269  loss_cate_4: 0  loss_mask_4: 0.7259  loss_dice_4: 0.977  loss_ce_5: 0.2511  loss_cate_5: 0  loss_mask_5: 0.6877  loss_dice_5: 0.9645  loss_ce_6: 0.2446  loss_cate_6: 0  loss_mask_6: 0.7487  loss_dice_6: 0.9729  loss_ce_7: 0.2311  loss_cate_7: 0  loss_mask_7: 0.691  loss_dice_7: 0.9432  loss_ce_8: 0.2325  loss_cate_8: 0  loss_mask_8: 0.721  loss_dice_8: 0.9524  time: 1.1025  data_time: 0.0128  lr: 9.2113e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:29:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 14:29:50 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 14:29:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 14:32:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 14:32:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0015 s/iter. Inference: 0.1616 s/iter. Eval: 0.0582 s/iter. Total: 0.2214 s/iter. ETA=0:07:23\n",
      "\u001b[32m[10/12 14:32:30 d2.evaluation.evaluator]: \u001b[0mInference done 36/2016. Dataloading: 0.0023 s/iter. Inference: 0.1617 s/iter. Eval: 0.0427 s/iter. Total: 0.2070 s/iter. ETA=0:06:49\n",
      "\u001b[32m[10/12 14:32:36 d2.evaluation.evaluator]: \u001b[0mInference done 61/2016. Dataloading: 0.0025 s/iter. Inference: 0.1623 s/iter. Eval: 0.0388 s/iter. Total: 0.2054 s/iter. ETA=0:06:41\n",
      "\u001b[32m[10/12 14:32:41 d2.evaluation.evaluator]: \u001b[0mInference done 85/2016. Dataloading: 0.0026 s/iter. Inference: 0.1627 s/iter. Eval: 0.0405 s/iter. Total: 0.2070 s/iter. ETA=0:06:39\n",
      "\u001b[32m[10/12 14:32:46 d2.evaluation.evaluator]: \u001b[0mInference done 110/2016. Dataloading: 0.0026 s/iter. Inference: 0.1625 s/iter. Eval: 0.0395 s/iter. Total: 0.2056 s/iter. ETA=0:06:31\n",
      "\u001b[32m[10/12 14:32:51 d2.evaluation.evaluator]: \u001b[0mInference done 132/2016. Dataloading: 0.0026 s/iter. Inference: 0.1626 s/iter. Eval: 0.0444 s/iter. Total: 0.2104 s/iter. ETA=0:06:36\n",
      "\u001b[32m[10/12 14:32:56 d2.evaluation.evaluator]: \u001b[0mInference done 153/2016. Dataloading: 0.0026 s/iter. Inference: 0.1628 s/iter. Eval: 0.0484 s/iter. Total: 0.2146 s/iter. ETA=0:06:39\n",
      "\u001b[32m[10/12 14:33:01 d2.evaluation.evaluator]: \u001b[0mInference done 178/2016. Dataloading: 0.0025 s/iter. Inference: 0.1627 s/iter. Eval: 0.0465 s/iter. Total: 0.2127 s/iter. ETA=0:06:30\n",
      "\u001b[32m[10/12 14:33:06 d2.evaluation.evaluator]: \u001b[0mInference done 203/2016. Dataloading: 0.0025 s/iter. Inference: 0.1628 s/iter. Eval: 0.0456 s/iter. Total: 0.2117 s/iter. ETA=0:06:23\n",
      "\u001b[32m[10/12 14:33:11 d2.evaluation.evaluator]: \u001b[0mInference done 229/2016. Dataloading: 0.0025 s/iter. Inference: 0.1628 s/iter. Eval: 0.0435 s/iter. Total: 0.2095 s/iter. ETA=0:06:14\n",
      "\u001b[32m[10/12 14:33:16 d2.evaluation.evaluator]: \u001b[0mInference done 250/2016. Dataloading: 0.0025 s/iter. Inference: 0.1628 s/iter. Eval: 0.0465 s/iter. Total: 0.2126 s/iter. ETA=0:06:15\n",
      "\u001b[32m[10/12 14:33:21 d2.evaluation.evaluator]: \u001b[0mInference done 276/2016. Dataloading: 0.0025 s/iter. Inference: 0.1628 s/iter. Eval: 0.0448 s/iter. Total: 0.2108 s/iter. ETA=0:06:06\n",
      "\u001b[32m[10/12 14:33:26 d2.evaluation.evaluator]: \u001b[0mInference done 300/2016. Dataloading: 0.0025 s/iter. Inference: 0.1630 s/iter. Eval: 0.0448 s/iter. Total: 0.2111 s/iter. ETA=0:06:02\n",
      "\u001b[32m[10/12 14:33:31 d2.evaluation.evaluator]: \u001b[0mInference done 326/2016. Dataloading: 0.0025 s/iter. Inference: 0.1630 s/iter. Eval: 0.0438 s/iter. Total: 0.2100 s/iter. ETA=0:05:54\n",
      "\u001b[32m[10/12 14:33:37 d2.evaluation.evaluator]: \u001b[0mInference done 352/2016. Dataloading: 0.0024 s/iter. Inference: 0.1630 s/iter. Eval: 0.0428 s/iter. Total: 0.2090 s/iter. ETA=0:05:47\n",
      "\u001b[32m[10/12 14:33:42 d2.evaluation.evaluator]: \u001b[0mInference done 375/2016. Dataloading: 0.0025 s/iter. Inference: 0.1630 s/iter. Eval: 0.0434 s/iter. Total: 0.2096 s/iter. ETA=0:05:43\n",
      "\u001b[32m[10/12 14:33:47 d2.evaluation.evaluator]: \u001b[0mInference done 401/2016. Dataloading: 0.0025 s/iter. Inference: 0.1631 s/iter. Eval: 0.0424 s/iter. Total: 0.2086 s/iter. ETA=0:05:36\n",
      "\u001b[32m[10/12 14:33:52 d2.evaluation.evaluator]: \u001b[0mInference done 424/2016. Dataloading: 0.0025 s/iter. Inference: 0.1632 s/iter. Eval: 0.0429 s/iter. Total: 0.2092 s/iter. ETA=0:05:32\n",
      "\u001b[32m[10/12 14:33:57 d2.evaluation.evaluator]: \u001b[0mInference done 450/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0422 s/iter. Total: 0.2084 s/iter. ETA=0:05:26\n",
      "\u001b[32m[10/12 14:34:02 d2.evaluation.evaluator]: \u001b[0mInference done 475/2016. Dataloading: 0.0025 s/iter. Inference: 0.1632 s/iter. Eval: 0.0418 s/iter. Total: 0.2080 s/iter. ETA=0:05:20\n",
      "\u001b[32m[10/12 14:34:07 d2.evaluation.evaluator]: \u001b[0mInference done 501/2016. Dataloading: 0.0025 s/iter. Inference: 0.1631 s/iter. Eval: 0.0416 s/iter. Total: 0.2078 s/iter. ETA=0:05:14\n",
      "\u001b[32m[10/12 14:34:12 d2.evaluation.evaluator]: \u001b[0mInference done 526/2016. Dataloading: 0.0025 s/iter. Inference: 0.1631 s/iter. Eval: 0.0414 s/iter. Total: 0.2077 s/iter. ETA=0:05:09\n",
      "\u001b[32m[10/12 14:34:17 d2.evaluation.evaluator]: \u001b[0mInference done 552/2016. Dataloading: 0.0024 s/iter. Inference: 0.1631 s/iter. Eval: 0.0410 s/iter. Total: 0.2073 s/iter. ETA=0:05:03\n",
      "\u001b[32m[10/12 14:34:23 d2.evaluation.evaluator]: \u001b[0mInference done 577/2016. Dataloading: 0.0025 s/iter. Inference: 0.1632 s/iter. Eval: 0.0408 s/iter. Total: 0.2071 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 14:34:28 d2.evaluation.evaluator]: \u001b[0mInference done 602/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0406 s/iter. Total: 0.2070 s/iter. ETA=0:04:52\n",
      "\u001b[32m[10/12 14:34:33 d2.evaluation.evaluator]: \u001b[0mInference done 627/2016. Dataloading: 0.0025 s/iter. Inference: 0.1633 s/iter. Eval: 0.0403 s/iter. Total: 0.2069 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 14:34:38 d2.evaluation.evaluator]: \u001b[0mInference done 652/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0402 s/iter. Total: 0.2067 s/iter. ETA=0:04:41\n",
      "\u001b[32m[10/12 14:34:43 d2.evaluation.evaluator]: \u001b[0mInference done 678/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0397 s/iter. Total: 0.2062 s/iter. ETA=0:04:35\n",
      "\u001b[32m[10/12 14:34:48 d2.evaluation.evaluator]: \u001b[0mInference done 703/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0396 s/iter. Total: 0.2062 s/iter. ETA=0:04:30\n",
      "\u001b[32m[10/12 14:34:53 d2.evaluation.evaluator]: \u001b[0mInference done 727/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0397 s/iter. Total: 0.2063 s/iter. ETA=0:04:25\n",
      "\u001b[32m[10/12 14:34:58 d2.evaluation.evaluator]: \u001b[0mInference done 751/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0400 s/iter. Total: 0.2067 s/iter. ETA=0:04:21\n",
      "\u001b[32m[10/12 14:35:03 d2.evaluation.evaluator]: \u001b[0mInference done 768/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0423 s/iter. Total: 0.2089 s/iter. ETA=0:04:20\n",
      "\u001b[32m[10/12 14:35:09 d2.evaluation.evaluator]: \u001b[0mInference done 793/2016. Dataloading: 0.0025 s/iter. Inference: 0.1636 s/iter. Eval: 0.0420 s/iter. Total: 0.2088 s/iter. ETA=0:04:15\n",
      "\u001b[32m[10/12 14:35:14 d2.evaluation.evaluator]: \u001b[0mInference done 817/2016. Dataloading: 0.0025 s/iter. Inference: 0.1636 s/iter. Eval: 0.0421 s/iter. Total: 0.2090 s/iter. ETA=0:04:10\n",
      "\u001b[32m[10/12 14:35:19 d2.evaluation.evaluator]: \u001b[0mInference done 843/2016. Dataloading: 0.0025 s/iter. Inference: 0.1637 s/iter. Eval: 0.0418 s/iter. Total: 0.2087 s/iter. ETA=0:04:04\n",
      "\u001b[32m[10/12 14:35:24 d2.evaluation.evaluator]: \u001b[0mInference done 868/2016. Dataloading: 0.0025 s/iter. Inference: 0.1637 s/iter. Eval: 0.0415 s/iter. Total: 0.2085 s/iter. ETA=0:03:59\n",
      "\u001b[32m[10/12 14:35:29 d2.evaluation.evaluator]: \u001b[0mInference done 894/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0412 s/iter. Total: 0.2082 s/iter. ETA=0:03:53\n",
      "\u001b[32m[10/12 14:35:34 d2.evaluation.evaluator]: \u001b[0mInference done 920/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0410 s/iter. Total: 0.2080 s/iter. ETA=0:03:47\n",
      "\u001b[32m[10/12 14:35:39 d2.evaluation.evaluator]: \u001b[0mInference done 945/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0409 s/iter. Total: 0.2079 s/iter. ETA=0:03:42\n",
      "\u001b[32m[10/12 14:35:45 d2.evaluation.evaluator]: \u001b[0mInference done 971/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0406 s/iter. Total: 0.2076 s/iter. ETA=0:03:36\n",
      "\u001b[32m[10/12 14:35:50 d2.evaluation.evaluator]: \u001b[0mInference done 996/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0405 s/iter. Total: 0.2076 s/iter. ETA=0:03:31\n",
      "\u001b[32m[10/12 14:35:55 d2.evaluation.evaluator]: \u001b[0mInference done 1021/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0404 s/iter. Total: 0.2075 s/iter. ETA=0:03:26\n",
      "\u001b[32m[10/12 14:36:00 d2.evaluation.evaluator]: \u001b[0mInference done 1047/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0402 s/iter. Total: 0.2072 s/iter. ETA=0:03:20\n",
      "\u001b[32m[10/12 14:36:05 d2.evaluation.evaluator]: \u001b[0mInference done 1072/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0401 s/iter. Total: 0.2071 s/iter. ETA=0:03:15\n",
      "\u001b[32m[10/12 14:36:10 d2.evaluation.evaluator]: \u001b[0mInference done 1097/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0399 s/iter. Total: 0.2070 s/iter. ETA=0:03:10\n",
      "\u001b[32m[10/12 14:36:15 d2.evaluation.evaluator]: \u001b[0mInference done 1122/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0398 s/iter. Total: 0.2069 s/iter. ETA=0:03:04\n",
      "\u001b[32m[10/12 14:36:20 d2.evaluation.evaluator]: \u001b[0mInference done 1147/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0398 s/iter. Total: 0.2068 s/iter. ETA=0:02:59\n",
      "\u001b[32m[10/12 14:36:25 d2.evaluation.evaluator]: \u001b[0mInference done 1173/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0396 s/iter. Total: 0.2066 s/iter. ETA=0:02:54\n",
      "\u001b[32m[10/12 14:36:31 d2.evaluation.evaluator]: \u001b[0mInference done 1198/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0396 s/iter. Total: 0.2066 s/iter. ETA=0:02:49\n",
      "\u001b[32m[10/12 14:36:36 d2.evaluation.evaluator]: \u001b[0mInference done 1225/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0393 s/iter. Total: 0.2063 s/iter. ETA=0:02:43\n",
      "\u001b[32m[10/12 14:36:41 d2.evaluation.evaluator]: \u001b[0mInference done 1251/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0391 s/iter. Total: 0.2061 s/iter. ETA=0:02:37\n",
      "\u001b[32m[10/12 14:36:46 d2.evaluation.evaluator]: \u001b[0mInference done 1277/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0390 s/iter. Total: 0.2059 s/iter. ETA=0:02:32\n",
      "\u001b[32m[10/12 14:36:51 d2.evaluation.evaluator]: \u001b[0mInference done 1302/2016. Dataloading: 0.0025 s/iter. Inference: 0.1638 s/iter. Eval: 0.0390 s/iter. Total: 0.2059 s/iter. ETA=0:02:27\n",
      "\u001b[32m[10/12 14:36:56 d2.evaluation.evaluator]: \u001b[0mInference done 1327/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0388 s/iter. Total: 0.2058 s/iter. ETA=0:02:21\n",
      "\u001b[32m[10/12 14:37:01 d2.evaluation.evaluator]: \u001b[0mInference done 1352/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0387 s/iter. Total: 0.2057 s/iter. ETA=0:02:16\n",
      "\u001b[32m[10/12 14:37:06 d2.evaluation.evaluator]: \u001b[0mInference done 1377/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0388 s/iter. Total: 0.2058 s/iter. ETA=0:02:11\n",
      "\u001b[32m[10/12 14:37:11 d2.evaluation.evaluator]: \u001b[0mInference done 1403/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0386 s/iter. Total: 0.2056 s/iter. ETA=0:02:06\n",
      "\u001b[32m[10/12 14:37:17 d2.evaluation.evaluator]: \u001b[0mInference done 1428/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2055 s/iter. ETA=0:02:00\n",
      "\u001b[32m[10/12 14:37:22 d2.evaluation.evaluator]: \u001b[0mInference done 1453/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2058 s/iter. ETA=0:01:55\n",
      "\u001b[32m[10/12 14:37:27 d2.evaluation.evaluator]: \u001b[0mInference done 1477/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0386 s/iter. Total: 0.2059 s/iter. ETA=0:01:50\n",
      "\u001b[32m[10/12 14:37:32 d2.evaluation.evaluator]: \u001b[0mInference done 1503/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0384 s/iter. Total: 0.2057 s/iter. ETA=0:01:45\n",
      "\u001b[32m[10/12 14:37:37 d2.evaluation.evaluator]: \u001b[0mInference done 1527/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2058 s/iter. ETA=0:01:40\n",
      "\u001b[32m[10/12 14:37:42 d2.evaluation.evaluator]: \u001b[0mInference done 1552/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2058 s/iter. ETA=0:01:35\n",
      "\u001b[32m[10/12 14:37:48 d2.evaluation.evaluator]: \u001b[0mInference done 1576/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0386 s/iter. Total: 0.2059 s/iter. ETA=0:01:30\n",
      "\u001b[32m[10/12 14:37:53 d2.evaluation.evaluator]: \u001b[0mInference done 1601/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2059 s/iter. ETA=0:01:25\n",
      "\u001b[32m[10/12 14:37:58 d2.evaluation.evaluator]: \u001b[0mInference done 1626/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2059 s/iter. ETA=0:01:20\n",
      "\u001b[32m[10/12 14:38:03 d2.evaluation.evaluator]: \u001b[0mInference done 1651/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0385 s/iter. Total: 0.2058 s/iter. ETA=0:01:15\n",
      "\u001b[32m[10/12 14:38:08 d2.evaluation.evaluator]: \u001b[0mInference done 1677/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0384 s/iter. Total: 0.2057 s/iter. ETA=0:01:09\n",
      "\u001b[32m[10/12 14:38:13 d2.evaluation.evaluator]: \u001b[0mInference done 1702/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0384 s/iter. Total: 0.2057 s/iter. ETA=0:01:04\n",
      "\u001b[32m[10/12 14:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 1727/2016. Dataloading: 0.0026 s/iter. Inference: 0.1639 s/iter. Eval: 0.0383 s/iter. Total: 0.2056 s/iter. ETA=0:00:59\n",
      "\u001b[32m[10/12 14:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 1751/2016. Dataloading: 0.0026 s/iter. Inference: 0.1639 s/iter. Eval: 0.0384 s/iter. Total: 0.2057 s/iter. ETA=0:00:54\n",
      "\u001b[32m[10/12 14:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 1777/2016. Dataloading: 0.0026 s/iter. Inference: 0.1639 s/iter. Eval: 0.0383 s/iter. Total: 0.2056 s/iter. ETA=0:00:49\n",
      "\u001b[32m[10/12 14:38:33 d2.evaluation.evaluator]: \u001b[0mInference done 1802/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0382 s/iter. Total: 0.2056 s/iter. ETA=0:00:43\n",
      "\u001b[32m[10/12 14:38:38 d2.evaluation.evaluator]: \u001b[0mInference done 1827/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0382 s/iter. Total: 0.2055 s/iter. ETA=0:00:38\n",
      "\u001b[32m[10/12 14:38:43 d2.evaluation.evaluator]: \u001b[0mInference done 1852/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0382 s/iter. Total: 0.2054 s/iter. ETA=0:00:33\n",
      "\u001b[32m[10/12 14:38:48 d2.evaluation.evaluator]: \u001b[0mInference done 1875/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0383 s/iter. Total: 0.2056 s/iter. ETA=0:00:28\n",
      "\u001b[32m[10/12 14:38:54 d2.evaluation.evaluator]: \u001b[0mInference done 1901/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0382 s/iter. Total: 0.2054 s/iter. ETA=0:00:23\n",
      "\u001b[32m[10/12 14:38:59 d2.evaluation.evaluator]: \u001b[0mInference done 1927/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0381 s/iter. Total: 0.2053 s/iter. ETA=0:00:18\n",
      "\u001b[32m[10/12 14:39:04 d2.evaluation.evaluator]: \u001b[0mInference done 1953/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0380 s/iter. Total: 0.2052 s/iter. ETA=0:00:12\n",
      "\u001b[32m[10/12 14:39:09 d2.evaluation.evaluator]: \u001b[0mInference done 1979/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0379 s/iter. Total: 0.2050 s/iter. ETA=0:00:07\n",
      "\u001b[32m[10/12 14:39:14 d2.evaluation.evaluator]: \u001b[0mInference done 2005/2016. Dataloading: 0.0025 s/iter. Inference: 0.1639 s/iter. Eval: 0.0378 s/iter. Total: 0.2049 s/iter. ETA=0:00:02\n",
      "\u001b[32m[10/12 14:39:16 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:51.978564 (0.204863 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 14:39:16 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:29 (0.163856 s / iter per device, on 1 devices)\n",
      "miou = 74.60243459897035\n",
      "OA = 88.39608714694069\n",
      "Kappa = 84.86224805634541\n",
      "F1_score = 71.08441557787671\n",
      "\u001b[32m[10/12 14:39:17 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 74.60243459897035, 'fwIoU': 79.85202535951534, 'IoU-Background': 40.57613131487837, 'IoU-Surfaces': 83.06229478017364, 'IoU-Building': 90.99128278873394, 'IoU-Low vegetation': 74.75998023827046, 'IoU-tree': 74.90014331632561, 'IoU-Car': 83.32477515544007, 'mACC': 83.8480211269859, 'pACC': 88.39608714694069, 'ACC-Background': 57.607308242842926, 'ACC-Surfaces': 92.15992141943524, 'ACC-Building': 96.46074004635487, 'ACC-Low vegetation': 85.88178558809675, 'ACC-tree': 81.54724350218456, 'ACC-Car': 89.43112796300098})])\n",
      "\u001b[32m[10/12 14:39:17 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 14:39:17 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 14:39:17 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 14:39:17 d2.evaluation.testing]: \u001b[0mcopypaste: 74.6024,79.8520,83.8480,88.3961\n",
      "\u001b[32m[10/12 14:39:17 d2.utils.events]: \u001b[0m eta: 22:17:19  iter: 6999  total_loss: 20.64  loss_ce: 0.2452  loss_cate: 0.1493  loss_mask: 0.7926  loss_dice: 0.9063  loss_ce_0: 0.4008  loss_cate_0: 0  loss_mask_0: 0.816  loss_dice_0: 1.024  loss_ce_1: 0.2444  loss_cate_1: 0  loss_mask_1: 0.8018  loss_dice_1: 0.9997  loss_ce_2: 0.3051  loss_cate_2: 0  loss_mask_2: 0.7868  loss_dice_2: 0.9473  loss_ce_3: 0.2328  loss_cate_3: 0  loss_mask_3: 0.7944  loss_dice_3: 0.9404  loss_ce_4: 0.3063  loss_cate_4: 0  loss_mask_4: 0.783  loss_dice_4: 0.9695  loss_ce_5: 0.272  loss_cate_5: 0  loss_mask_5: 0.7914  loss_dice_5: 0.9538  loss_ce_6: 0.2146  loss_cate_6: 0  loss_mask_6: 0.7839  loss_dice_6: 0.9277  loss_ce_7: 0.2205  loss_cate_7: 0  loss_mask_7: 0.815  loss_dice_7: 0.9512  loss_ce_8: 0.1698  loss_cate_8: 0  loss_mask_8: 0.8163  loss_dice_8: 0.9987  time: 1.1025  data_time: 0.0134  lr: 9.2091e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:39:45 d2.utils.events]: \u001b[0m eta: 22:16:34  iter: 7019  total_loss: 19.1  loss_ce: 0.1916  loss_cate: 0.1427  loss_mask: 0.7491  loss_dice: 1  loss_ce_0: 0.341  loss_cate_0: 0  loss_mask_0: 0.7608  loss_dice_0: 1.005  loss_ce_1: 0.2071  loss_cate_1: 0  loss_mask_1: 0.7688  loss_dice_1: 0.9699  loss_ce_2: 0.2568  loss_cate_2: 0  loss_mask_2: 0.7228  loss_dice_2: 0.9779  loss_ce_3: 0.1946  loss_cate_3: 0  loss_mask_3: 0.7311  loss_dice_3: 0.9602  loss_ce_4: 0.1889  loss_cate_4: 0  loss_mask_4: 0.7278  loss_dice_4: 0.9682  loss_ce_5: 0.1845  loss_cate_5: 0  loss_mask_5: 0.7394  loss_dice_5: 0.9869  loss_ce_6: 0.1876  loss_cate_6: 0  loss_mask_6: 0.7394  loss_dice_6: 0.9674  loss_ce_7: 0.1699  loss_cate_7: 0  loss_mask_7: 0.7459  loss_dice_7: 0.9718  loss_ce_8: 0.1759  loss_cate_8: 0  loss_mask_8: 0.7329  loss_dice_8: 1  time: 1.1024  data_time: 0.0128  lr: 9.2068e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:40:08 d2.utils.events]: \u001b[0m eta: 22:16:28  iter: 7039  total_loss: 20.89  loss_ce: 0.09809  loss_cate: 0.1426  loss_mask: 0.8085  loss_dice: 1.01  loss_ce_0: 0.2822  loss_cate_0: 0  loss_mask_0: 0.8148  loss_dice_0: 1.045  loss_ce_1: 0.1618  loss_cate_1: 0  loss_mask_1: 0.8069  loss_dice_1: 1.004  loss_ce_2: 0.2231  loss_cate_2: 0  loss_mask_2: 0.8093  loss_dice_2: 0.995  loss_ce_3: 0.2316  loss_cate_3: 0  loss_mask_3: 0.8095  loss_dice_3: 0.9907  loss_ce_4: 0.1331  loss_cate_4: 0  loss_mask_4: 0.8021  loss_dice_4: 0.9826  loss_ce_5: 0.1331  loss_cate_5: 0  loss_mask_5: 0.7993  loss_dice_5: 0.9928  loss_ce_6: 0.1215  loss_cate_6: 0  loss_mask_6: 0.7639  loss_dice_6: 0.9705  loss_ce_7: 0.1174  loss_cate_7: 0  loss_mask_7: 0.7663  loss_dice_7: 0.9618  loss_ce_8: 0.1134  loss_cate_8: 0  loss_mask_8: 0.8187  loss_dice_8: 0.9771  time: 1.1024  data_time: 0.0147  lr: 9.2045e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:40:30 d2.utils.events]: \u001b[0m eta: 22:16:21  iter: 7059  total_loss: 20.8  loss_ce: 0.1793  loss_cate: 0.1529  loss_mask: 0.768  loss_dice: 0.9941  loss_ce_0: 0.2853  loss_cate_0: 0  loss_mask_0: 0.7578  loss_dice_0: 1.067  loss_ce_1: 0.1602  loss_cate_1: 0  loss_mask_1: 0.7841  loss_dice_1: 1.043  loss_ce_2: 0.2129  loss_cate_2: 0  loss_mask_2: 0.7615  loss_dice_2: 1.027  loss_ce_3: 0.2131  loss_cate_3: 0  loss_mask_3: 0.752  loss_dice_3: 1.001  loss_ce_4: 0.2236  loss_cate_4: 0  loss_mask_4: 0.7883  loss_dice_4: 0.9953  loss_ce_5: 0.2246  loss_cate_5: 0  loss_mask_5: 0.7586  loss_dice_5: 0.9889  loss_ce_6: 0.1775  loss_cate_6: 0  loss_mask_6: 0.8013  loss_dice_6: 1.007  loss_ce_7: 0.1234  loss_cate_7: 0  loss_mask_7: 0.8099  loss_dice_7: 1.01  loss_ce_8: 0.1251  loss_cate_8: 0  loss_mask_8: 0.7941  loss_dice_8: 0.9904  time: 1.1024  data_time: 0.0145  lr: 9.2022e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:40:53 d2.utils.events]: \u001b[0m eta: 22:16:09  iter: 7079  total_loss: 22.86  loss_ce: 0.2358  loss_cate: 0.1727  loss_mask: 0.9327  loss_dice: 1.093  loss_ce_0: 0.3446  loss_cate_0: 0  loss_mask_0: 0.9492  loss_dice_0: 1.151  loss_ce_1: 0.2763  loss_cate_1: 0  loss_mask_1: 0.9357  loss_dice_1: 1.111  loss_ce_2: 0.2505  loss_cate_2: 0  loss_mask_2: 0.9701  loss_dice_2: 1.088  loss_ce_3: 0.2458  loss_cate_3: 0  loss_mask_3: 0.9476  loss_dice_3: 1.093  loss_ce_4: 0.3016  loss_cate_4: 0  loss_mask_4: 0.956  loss_dice_4: 1.083  loss_ce_5: 0.2606  loss_cate_5: 0  loss_mask_5: 0.9491  loss_dice_5: 1.063  loss_ce_6: 0.2606  loss_cate_6: 0  loss_mask_6: 0.9371  loss_dice_6: 1.058  loss_ce_7: 0.2326  loss_cate_7: 0  loss_mask_7: 0.99  loss_dice_7: 1.059  loss_ce_8: 0.2409  loss_cate_8: 0  loss_mask_8: 0.9842  loss_dice_8: 1.066  time: 1.1024  data_time: 0.0138  lr: 9.2e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:41:16 d2.utils.events]: \u001b[0m eta: 22:16:04  iter: 7099  total_loss: 21.25  loss_ce: 0.2561  loss_cate: 0.163  loss_mask: 0.9364  loss_dice: 1.004  loss_ce_0: 0.3213  loss_cate_0: 0  loss_mask_0: 0.9402  loss_dice_0: 1.04  loss_ce_1: 0.2294  loss_cate_1: 0  loss_mask_1: 0.9447  loss_dice_1: 1.016  loss_ce_2: 0.2645  loss_cate_2: 0  loss_mask_2: 0.9533  loss_dice_2: 0.9753  loss_ce_3: 0.1852  loss_cate_3: 0  loss_mask_3: 0.9536  loss_dice_3: 0.9853  loss_ce_4: 0.1576  loss_cate_4: 0  loss_mask_4: 0.9267  loss_dice_4: 1.008  loss_ce_5: 0.2384  loss_cate_5: 0  loss_mask_5: 0.8583  loss_dice_5: 0.9848  loss_ce_6: 0.2286  loss_cate_6: 0  loss_mask_6: 0.9211  loss_dice_6: 0.9682  loss_ce_7: 0.2213  loss_cate_7: 0  loss_mask_7: 0.9255  loss_dice_7: 0.9868  loss_ce_8: 0.1886  loss_cate_8: 0  loss_mask_8: 0.932  loss_dice_8: 1.046  time: 1.1024  data_time: 0.0131  lr: 9.1977e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:41:39 d2.utils.events]: \u001b[0m eta: 22:15:51  iter: 7119  total_loss: 21.42  loss_ce: 0.3332  loss_cate: 0.1643  loss_mask: 0.9072  loss_dice: 1.003  loss_ce_0: 0.4217  loss_cate_0: 0  loss_mask_0: 0.8841  loss_dice_0: 0.9742  loss_ce_1: 0.3107  loss_cate_1: 0  loss_mask_1: 0.9093  loss_dice_1: 0.9684  loss_ce_2: 0.2676  loss_cate_2: 0  loss_mask_2: 0.9311  loss_dice_2: 1.015  loss_ce_3: 0.2569  loss_cate_3: 0  loss_mask_3: 0.9415  loss_dice_3: 0.996  loss_ce_4: 0.2681  loss_cate_4: 0  loss_mask_4: 0.9402  loss_dice_4: 1.003  loss_ce_5: 0.2744  loss_cate_5: 0  loss_mask_5: 0.9431  loss_dice_5: 0.9419  loss_ce_6: 0.2978  loss_cate_6: 0  loss_mask_6: 0.9208  loss_dice_6: 0.9706  loss_ce_7: 0.2686  loss_cate_7: 0  loss_mask_7: 0.9287  loss_dice_7: 0.9807  loss_ce_8: 0.3159  loss_cate_8: 0  loss_mask_8: 0.9201  loss_dice_8: 1.035  time: 1.1024  data_time: 0.0136  lr: 9.1954e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:42:05 d2.utils.events]: \u001b[0m eta: 22:15:21  iter: 7139  total_loss: 21.87  loss_ce: 0.2288  loss_cate: 0.1934  loss_mask: 0.7675  loss_dice: 1.076  loss_ce_0: 0.3575  loss_cate_0: 0  loss_mask_0: 0.7547  loss_dice_0: 1.199  loss_ce_1: 0.318  loss_cate_1: 0  loss_mask_1: 0.745  loss_dice_1: 1.112  loss_ce_2: 0.2605  loss_cate_2: 0  loss_mask_2: 0.7799  loss_dice_2: 1.137  loss_ce_3: 0.2675  loss_cate_3: 0  loss_mask_3: 0.7725  loss_dice_3: 1.014  loss_ce_4: 0.2159  loss_cate_4: 0  loss_mask_4: 0.7393  loss_dice_4: 1.05  loss_ce_5: 0.2107  loss_cate_5: 0  loss_mask_5: 0.7502  loss_dice_5: 1.094  loss_ce_6: 0.2404  loss_cate_6: 0  loss_mask_6: 0.7459  loss_dice_6: 1.08  loss_ce_7: 0.2352  loss_cate_7: 0  loss_mask_7: 0.7581  loss_dice_7: 1.066  loss_ce_8: 0.2509  loss_cate_8: 0  loss_mask_8: 0.7742  loss_dice_8: 1.091  time: 1.1025  data_time: 0.0428  lr: 9.1932e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:42:30 d2.utils.events]: \u001b[0m eta: 22:15:10  iter: 7159  total_loss: 21.06  loss_ce: 0.2339  loss_cate: 0.1755  loss_mask: 0.8992  loss_dice: 0.8908  loss_ce_0: 0.4195  loss_cate_0: 0  loss_mask_0: 0.8714  loss_dice_0: 1.054  loss_ce_1: 0.3724  loss_cate_1: 0  loss_mask_1: 0.8787  loss_dice_1: 1.005  loss_ce_2: 0.2789  loss_cate_2: 0  loss_mask_2: 0.9052  loss_dice_2: 0.9581  loss_ce_3: 0.2325  loss_cate_3: 0  loss_mask_3: 0.9154  loss_dice_3: 0.9261  loss_ce_4: 0.2153  loss_cate_4: 0  loss_mask_4: 0.9078  loss_dice_4: 0.9483  loss_ce_5: 0.2077  loss_cate_5: 0  loss_mask_5: 0.9065  loss_dice_5: 0.9308  loss_ce_6: 0.243  loss_cate_6: 0  loss_mask_6: 0.8681  loss_dice_6: 0.875  loss_ce_7: 0.2354  loss_cate_7: 0  loss_mask_7: 0.8799  loss_dice_7: 0.9092  loss_ce_8: 0.2246  loss_cate_8: 0  loss_mask_8: 0.8904  loss_dice_8: 0.9023  time: 1.1029  data_time: 0.1724  lr: 9.1909e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:42:56 d2.utils.events]: \u001b[0m eta: 22:14:59  iter: 7179  total_loss: 22.43  loss_ce: 0.2664  loss_cate: 0.1254  loss_mask: 0.7679  loss_dice: 1.087  loss_ce_0: 0.3733  loss_cate_0: 0  loss_mask_0: 0.7832  loss_dice_0: 1.106  loss_ce_1: 0.2588  loss_cate_1: 0  loss_mask_1: 0.806  loss_dice_1: 1.142  loss_ce_2: 0.2059  loss_cate_2: 0  loss_mask_2: 0.8028  loss_dice_2: 1.115  loss_ce_3: 0.1951  loss_cate_3: 0  loss_mask_3: 0.7821  loss_dice_3: 1.101  loss_ce_4: 0.2342  loss_cate_4: 0  loss_mask_4: 0.8116  loss_dice_4: 1.082  loss_ce_5: 0.1957  loss_cate_5: 0  loss_mask_5: 0.7799  loss_dice_5: 1.137  loss_ce_6: 0.2313  loss_cate_6: 0  loss_mask_6: 0.775  loss_dice_6: 1.092  loss_ce_7: 0.2161  loss_cate_7: 0  loss_mask_7: 0.7529  loss_dice_7: 1.091  loss_ce_8: 0.2391  loss_cate_8: 0  loss_mask_8: 0.7774  loss_dice_8: 1.086  time: 1.1033  data_time: 0.1571  lr: 9.1886e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:43:18 d2.utils.events]: \u001b[0m eta: 22:14:37  iter: 7199  total_loss: 21.81  loss_ce: 0.2468  loss_cate: 0.172  loss_mask: 0.969  loss_dice: 1.014  loss_ce_0: 0.4811  loss_cate_0: 0  loss_mask_0: 0.9081  loss_dice_0: 1.119  loss_ce_1: 0.2766  loss_cate_1: 0  loss_mask_1: 0.9138  loss_dice_1: 1.047  loss_ce_2: 0.2092  loss_cate_2: 0  loss_mask_2: 0.9048  loss_dice_2: 1.038  loss_ce_3: 0.244  loss_cate_3: 0  loss_mask_3: 0.893  loss_dice_3: 0.976  loss_ce_4: 0.219  loss_cate_4: 0  loss_mask_4: 0.8959  loss_dice_4: 1.033  loss_ce_5: 0.2361  loss_cate_5: 0  loss_mask_5: 0.9071  loss_dice_5: 1.007  loss_ce_6: 0.2477  loss_cate_6: 0  loss_mask_6: 0.9275  loss_dice_6: 1.028  loss_ce_7: 0.2541  loss_cate_7: 0  loss_mask_7: 0.9014  loss_dice_7: 0.9873  loss_ce_8: 0.2227  loss_cate_8: 0  loss_mask_8: 0.9274  loss_dice_8: 1.023  time: 1.1033  data_time: 0.0127  lr: 9.1863e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:43:41 d2.utils.events]: \u001b[0m eta: 22:14:15  iter: 7219  total_loss: 22.92  loss_ce: 0.2771  loss_cate: 0.1738  loss_mask: 0.8365  loss_dice: 1.119  loss_ce_0: 0.4845  loss_cate_0: 0  loss_mask_0: 0.8835  loss_dice_0: 1.161  loss_ce_1: 0.3777  loss_cate_1: 0  loss_mask_1: 0.874  loss_dice_1: 1.185  loss_ce_2: 0.3004  loss_cate_2: 0  loss_mask_2: 0.843  loss_dice_2: 1.126  loss_ce_3: 0.3334  loss_cate_3: 0  loss_mask_3: 0.8623  loss_dice_3: 1.12  loss_ce_4: 0.3446  loss_cate_4: 0  loss_mask_4: 0.8473  loss_dice_4: 1.052  loss_ce_5: 0.2994  loss_cate_5: 0  loss_mask_5: 0.8444  loss_dice_5: 1.095  loss_ce_6: 0.261  loss_cate_6: 0  loss_mask_6: 0.8224  loss_dice_6: 1.088  loss_ce_7: 0.2734  loss_cate_7: 0  loss_mask_7: 0.8166  loss_dice_7: 1.144  loss_ce_8: 0.2148  loss_cate_8: 0  loss_mask_8: 0.8243  loss_dice_8: 1.203  time: 1.1033  data_time: 0.0133  lr: 9.1841e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:44:04 d2.utils.events]: \u001b[0m eta: 22:13:53  iter: 7239  total_loss: 21.31  loss_ce: 0.189  loss_cate: 0.1579  loss_mask: 0.8768  loss_dice: 1.007  loss_ce_0: 0.36  loss_cate_0: 0  loss_mask_0: 0.9074  loss_dice_0: 1.009  loss_ce_1: 0.2831  loss_cate_1: 0  loss_mask_1: 0.9154  loss_dice_1: 1.045  loss_ce_2: 0.2644  loss_cate_2: 0  loss_mask_2: 0.8471  loss_dice_2: 0.9951  loss_ce_3: 0.2726  loss_cate_3: 0  loss_mask_3: 0.8249  loss_dice_3: 0.9435  loss_ce_4: 0.2339  loss_cate_4: 0  loss_mask_4: 0.8534  loss_dice_4: 0.9801  loss_ce_5: 0.2792  loss_cate_5: 0  loss_mask_5: 0.8548  loss_dice_5: 1.024  loss_ce_6: 0.2236  loss_cate_6: 0  loss_mask_6: 0.8766  loss_dice_6: 1.014  loss_ce_7: 0.1658  loss_cate_7: 0  loss_mask_7: 0.8751  loss_dice_7: 1.041  loss_ce_8: 0.1786  loss_cate_8: 0  loss_mask_8: 0.8625  loss_dice_8: 0.9991  time: 1.1033  data_time: 0.0426  lr: 9.1818e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:44:27 d2.utils.events]: \u001b[0m eta: 22:13:31  iter: 7259  total_loss: 25.11  loss_ce: 0.3018  loss_cate: 0.1638  loss_mask: 0.9263  loss_dice: 1.131  loss_ce_0: 0.3622  loss_cate_0: 0  loss_mask_0: 0.9345  loss_dice_0: 1.127  loss_ce_1: 0.305  loss_cate_1: 0  loss_mask_1: 0.9299  loss_dice_1: 1.197  loss_ce_2: 0.2644  loss_cate_2: 0  loss_mask_2: 0.9021  loss_dice_2: 1.072  loss_ce_3: 0.2734  loss_cate_3: 0  loss_mask_3: 0.868  loss_dice_3: 1.14  loss_ce_4: 0.3314  loss_cate_4: 0  loss_mask_4: 0.893  loss_dice_4: 1.11  loss_ce_5: 0.315  loss_cate_5: 0  loss_mask_5: 0.9207  loss_dice_5: 1.124  loss_ce_6: 0.2775  loss_cate_6: 0  loss_mask_6: 0.9114  loss_dice_6: 1.075  loss_ce_7: 0.361  loss_cate_7: 0  loss_mask_7: 0.9208  loss_dice_7: 1.136  loss_ce_8: 0.3537  loss_cate_8: 0  loss_mask_8: 0.8756  loss_dice_8: 1.131  time: 1.1033  data_time: 0.0131  lr: 9.1795e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:44:49 d2.utils.events]: \u001b[0m eta: 22:13:06  iter: 7279  total_loss: 20.07  loss_ce: 0.2382  loss_cate: 0.1425  loss_mask: 0.7601  loss_dice: 0.94  loss_ce_0: 0.3912  loss_cate_0: 0  loss_mask_0: 0.7853  loss_dice_0: 0.963  loss_ce_1: 0.2733  loss_cate_1: 0  loss_mask_1: 0.8186  loss_dice_1: 0.9636  loss_ce_2: 0.272  loss_cate_2: 0  loss_mask_2: 0.783  loss_dice_2: 0.9273  loss_ce_3: 0.2636  loss_cate_3: 0  loss_mask_3: 0.7702  loss_dice_3: 1.006  loss_ce_4: 0.2473  loss_cate_4: 0  loss_mask_4: 0.7981  loss_dice_4: 0.9983  loss_ce_5: 0.3004  loss_cate_5: 0  loss_mask_5: 0.8034  loss_dice_5: 0.9948  loss_ce_6: 0.2383  loss_cate_6: 0  loss_mask_6: 0.7854  loss_dice_6: 0.9376  loss_ce_7: 0.24  loss_cate_7: 0  loss_mask_7: 0.7868  loss_dice_7: 0.9647  loss_ce_8: 0.219  loss_cate_8: 0  loss_mask_8: 0.7817  loss_dice_8: 0.9656  time: 1.1033  data_time: 0.0125  lr: 9.1773e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:45:16 d2.utils.events]: \u001b[0m eta: 22:12:46  iter: 7299  total_loss: 21.58  loss_ce: 0.2311  loss_cate: 0.1824  loss_mask: 0.8603  loss_dice: 1.03  loss_ce_0: 0.4338  loss_cate_0: 0  loss_mask_0: 0.8479  loss_dice_0: 1.14  loss_ce_1: 0.2552  loss_cate_1: 0  loss_mask_1: 0.8386  loss_dice_1: 1.079  loss_ce_2: 0.2484  loss_cate_2: 0  loss_mask_2: 0.8861  loss_dice_2: 1.054  loss_ce_3: 0.2124  loss_cate_3: 0  loss_mask_3: 0.8844  loss_dice_3: 1.084  loss_ce_4: 0.2341  loss_cate_4: 0  loss_mask_4: 0.87  loss_dice_4: 1.06  loss_ce_5: 0.2349  loss_cate_5: 0  loss_mask_5: 0.8667  loss_dice_5: 1.082  loss_ce_6: 0.2498  loss_cate_6: 0  loss_mask_6: 0.8114  loss_dice_6: 1.018  loss_ce_7: 0.2714  loss_cate_7: 0  loss_mask_7: 0.8018  loss_dice_7: 1.053  loss_ce_8: 0.2406  loss_cate_8: 0  loss_mask_8: 0.8247  loss_dice_8: 1.068  time: 1.1033  data_time: 0.0136  lr: 9.175e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:45:39 d2.utils.events]: \u001b[0m eta: 22:12:27  iter: 7319  total_loss: 20.21  loss_ce: 0.2167  loss_cate: 0.1736  loss_mask: 0.7914  loss_dice: 1.011  loss_ce_0: 0.3059  loss_cate_0: 0  loss_mask_0: 0.806  loss_dice_0: 1.002  loss_ce_1: 0.2152  loss_cate_1: 0  loss_mask_1: 0.8355  loss_dice_1: 0.9968  loss_ce_2: 0.2071  loss_cate_2: 0  loss_mask_2: 0.8248  loss_dice_2: 0.9493  loss_ce_3: 0.1951  loss_cate_3: 0  loss_mask_3: 0.7916  loss_dice_3: 0.9546  loss_ce_4: 0.1852  loss_cate_4: 0  loss_mask_4: 0.8083  loss_dice_4: 0.9471  loss_ce_5: 0.2383  loss_cate_5: 0  loss_mask_5: 0.8147  loss_dice_5: 0.9829  loss_ce_6: 0.2718  loss_cate_6: 0  loss_mask_6: 0.8271  loss_dice_6: 0.9383  loss_ce_7: 0.2478  loss_cate_7: 0  loss_mask_7: 0.7917  loss_dice_7: 0.956  loss_ce_8: 0.2481  loss_cate_8: 0  loss_mask_8: 0.7883  loss_dice_8: 0.9797  time: 1.1033  data_time: 0.0135  lr: 9.1727e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:46:02 d2.utils.events]: \u001b[0m eta: 22:11:57  iter: 7339  total_loss: 21.35  loss_ce: 0.2392  loss_cate: 0.1365  loss_mask: 0.812  loss_dice: 0.9849  loss_ce_0: 0.3942  loss_cate_0: 0  loss_mask_0: 0.8586  loss_dice_0: 1.048  loss_ce_1: 0.2447  loss_cate_1: 0  loss_mask_1: 0.8192  loss_dice_1: 1.051  loss_ce_2: 0.2782  loss_cate_2: 0  loss_mask_2: 0.8161  loss_dice_2: 1.037  loss_ce_3: 0.2441  loss_cate_3: 0  loss_mask_3: 0.8394  loss_dice_3: 1.025  loss_ce_4: 0.238  loss_cate_4: 0  loss_mask_4: 0.8104  loss_dice_4: 1.022  loss_ce_5: 0.226  loss_cate_5: 0  loss_mask_5: 0.8165  loss_dice_5: 1.005  loss_ce_6: 0.2513  loss_cate_6: 0  loss_mask_6: 0.785  loss_dice_6: 0.9988  loss_ce_7: 0.2454  loss_cate_7: 0  loss_mask_7: 0.8119  loss_dice_7: 1.017  loss_ce_8: 0.2462  loss_cate_8: 0  loss_mask_8: 0.8177  loss_dice_8: 1.007  time: 1.1033  data_time: 0.0131  lr: 9.1704e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:46:24 d2.utils.events]: \u001b[0m eta: 22:11:36  iter: 7359  total_loss: 20.67  loss_ce: 0.2738  loss_cate: 0.1485  loss_mask: 0.8317  loss_dice: 0.9451  loss_ce_0: 0.3731  loss_cate_0: 0  loss_mask_0: 0.8611  loss_dice_0: 1.004  loss_ce_1: 0.2252  loss_cate_1: 0  loss_mask_1: 0.8319  loss_dice_1: 1.009  loss_ce_2: 0.2456  loss_cate_2: 0  loss_mask_2: 0.8326  loss_dice_2: 0.9989  loss_ce_3: 0.2436  loss_cate_3: 0  loss_mask_3: 0.8444  loss_dice_3: 0.932  loss_ce_4: 0.2559  loss_cate_4: 0  loss_mask_4: 0.8293  loss_dice_4: 0.9536  loss_ce_5: 0.216  loss_cate_5: 0  loss_mask_5: 0.8441  loss_dice_5: 0.9435  loss_ce_6: 0.2252  loss_cate_6: 0  loss_mask_6: 0.8349  loss_dice_6: 0.9419  loss_ce_7: 0.2731  loss_cate_7: 0  loss_mask_7: 0.8469  loss_dice_7: 0.9502  loss_ce_8: 0.2835  loss_cate_8: 0  loss_mask_8: 0.846  loss_dice_8: 0.9761  time: 1.1033  data_time: 0.0133  lr: 9.1682e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:46:47 d2.utils.events]: \u001b[0m eta: 22:11:16  iter: 7379  total_loss: 22.81  loss_ce: 0.2312  loss_cate: 0.1841  loss_mask: 0.9438  loss_dice: 1.002  loss_ce_0: 0.3551  loss_cate_0: 0  loss_mask_0: 0.948  loss_dice_0: 1.062  loss_ce_1: 0.2332  loss_cate_1: 0  loss_mask_1: 0.9254  loss_dice_1: 0.9855  loss_ce_2: 0.2455  loss_cate_2: 0  loss_mask_2: 0.9629  loss_dice_2: 0.9871  loss_ce_3: 0.221  loss_cate_3: 0  loss_mask_3: 0.9436  loss_dice_3: 1.043  loss_ce_4: 0.2037  loss_cate_4: 0  loss_mask_4: 0.9965  loss_dice_4: 1.046  loss_ce_5: 0.2185  loss_cate_5: 0  loss_mask_5: 0.9168  loss_dice_5: 1.043  loss_ce_6: 0.2044  loss_cate_6: 0  loss_mask_6: 0.9188  loss_dice_6: 1.042  loss_ce_7: 0.2615  loss_cate_7: 0  loss_mask_7: 0.9228  loss_dice_7: 1.03  loss_ce_8: 0.2352  loss_cate_8: 0  loss_mask_8: 0.9359  loss_dice_8: 1.029  time: 1.1033  data_time: 0.0134  lr: 9.1659e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:47:10 d2.utils.events]: \u001b[0m eta: 22:10:55  iter: 7399  total_loss: 22.49  loss_ce: 0.1848  loss_cate: 0.1497  loss_mask: 0.9857  loss_dice: 1.066  loss_ce_0: 0.2858  loss_cate_0: 0  loss_mask_0: 1.064  loss_dice_0: 1.132  loss_ce_1: 0.2243  loss_cate_1: 0  loss_mask_1: 1.039  loss_dice_1: 1.082  loss_ce_2: 0.259  loss_cate_2: 0  loss_mask_2: 0.9226  loss_dice_2: 1.096  loss_ce_3: 0.2597  loss_cate_3: 0  loss_mask_3: 0.9729  loss_dice_3: 1.073  loss_ce_4: 0.2261  loss_cate_4: 0  loss_mask_4: 0.9042  loss_dice_4: 1.024  loss_ce_5: 0.1894  loss_cate_5: 0  loss_mask_5: 0.9832  loss_dice_5: 1.08  loss_ce_6: 0.1865  loss_cate_6: 0  loss_mask_6: 0.9842  loss_dice_6: 1.085  loss_ce_7: 0.1895  loss_cate_7: 0  loss_mask_7: 0.8916  loss_dice_7: 1.086  loss_ce_8: 0.2205  loss_cate_8: 0  loss_mask_8: 0.8869  loss_dice_8: 1.031  time: 1.1033  data_time: 0.0122  lr: 9.1636e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:47:32 d2.utils.events]: \u001b[0m eta: 22:10:23  iter: 7419  total_loss: 21.55  loss_ce: 0.2193  loss_cate: 0.1941  loss_mask: 0.7665  loss_dice: 1.008  loss_ce_0: 0.3098  loss_cate_0: 0  loss_mask_0: 0.9887  loss_dice_0: 1.137  loss_ce_1: 0.2363  loss_cate_1: 0  loss_mask_1: 0.8315  loss_dice_1: 1.067  loss_ce_2: 0.2379  loss_cate_2: 0  loss_mask_2: 0.8471  loss_dice_2: 1.027  loss_ce_3: 0.2871  loss_cate_3: 0  loss_mask_3: 0.8601  loss_dice_3: 1.015  loss_ce_4: 0.2255  loss_cate_4: 0  loss_mask_4: 0.8233  loss_dice_4: 0.9985  loss_ce_5: 0.2185  loss_cate_5: 0  loss_mask_5: 0.7974  loss_dice_5: 1.01  loss_ce_6: 0.2758  loss_cate_6: 0  loss_mask_6: 0.8309  loss_dice_6: 1.013  loss_ce_7: 0.2675  loss_cate_7: 0  loss_mask_7: 0.8192  loss_dice_7: 0.974  loss_ce_8: 0.2216  loss_cate_8: 0  loss_mask_8: 0.7789  loss_dice_8: 1.016  time: 1.1033  data_time: 0.0125  lr: 9.1614e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:47:55 d2.utils.events]: \u001b[0m eta: 22:09:41  iter: 7439  total_loss: 22.66  loss_ce: 0.3017  loss_cate: 0.173  loss_mask: 0.865  loss_dice: 1.025  loss_ce_0: 0.408  loss_cate_0: 0  loss_mask_0: 0.8623  loss_dice_0: 1.095  loss_ce_1: 0.362  loss_cate_1: 0  loss_mask_1: 0.8588  loss_dice_1: 1.088  loss_ce_2: 0.3003  loss_cate_2: 0  loss_mask_2: 0.8665  loss_dice_2: 1.062  loss_ce_3: 0.3167  loss_cate_3: 0  loss_mask_3: 0.8682  loss_dice_3: 1.078  loss_ce_4: 0.3372  loss_cate_4: 0  loss_mask_4: 0.8664  loss_dice_4: 1.068  loss_ce_5: 0.3728  loss_cate_5: 0  loss_mask_5: 0.8694  loss_dice_5: 1.048  loss_ce_6: 0.348  loss_cate_6: 0  loss_mask_6: 0.8573  loss_dice_6: 1.043  loss_ce_7: 0.3633  loss_cate_7: 0  loss_mask_7: 0.8662  loss_dice_7: 1.07  loss_ce_8: 0.3296  loss_cate_8: 0  loss_mask_8: 0.8622  loss_dice_8: 1.056  time: 1.1032  data_time: 0.0125  lr: 9.1591e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:48:18 d2.utils.events]: \u001b[0m eta: 22:09:31  iter: 7459  total_loss: 23.89  loss_ce: 0.2166  loss_cate: 0.1675  loss_mask: 0.9149  loss_dice: 1.053  loss_ce_0: 0.3264  loss_cate_0: 0  loss_mask_0: 0.8763  loss_dice_0: 1.204  loss_ce_1: 0.2607  loss_cate_1: 0  loss_mask_1: 0.9392  loss_dice_1: 1.177  loss_ce_2: 0.2738  loss_cate_2: 0  loss_mask_2: 0.922  loss_dice_2: 1.131  loss_ce_3: 0.268  loss_cate_3: 0  loss_mask_3: 0.9295  loss_dice_3: 1.111  loss_ce_4: 0.253  loss_cate_4: 0  loss_mask_4: 0.9557  loss_dice_4: 1.081  loss_ce_5: 0.1761  loss_cate_5: 0  loss_mask_5: 0.9565  loss_dice_5: 1.129  loss_ce_6: 0.2535  loss_cate_6: 0  loss_mask_6: 0.9485  loss_dice_6: 1.158  loss_ce_7: 0.2222  loss_cate_7: 0  loss_mask_7: 0.9769  loss_dice_7: 1.143  loss_ce_8: 0.2133  loss_cate_8: 0  loss_mask_8: 0.9642  loss_dice_8: 1.124  time: 1.1032  data_time: 0.0134  lr: 9.1568e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:48:40 d2.utils.events]: \u001b[0m eta: 22:08:54  iter: 7479  total_loss: 23.36  loss_ce: 0.3389  loss_cate: 0.2049  loss_mask: 0.8892  loss_dice: 1.078  loss_ce_0: 0.3884  loss_cate_0: 0  loss_mask_0: 0.8561  loss_dice_0: 1.212  loss_ce_1: 0.284  loss_cate_1: 0  loss_mask_1: 0.8937  loss_dice_1: 1.069  loss_ce_2: 0.2823  loss_cate_2: 0  loss_mask_2: 0.8603  loss_dice_2: 1.106  loss_ce_3: 0.2876  loss_cate_3: 0  loss_mask_3: 0.8973  loss_dice_3: 1.053  loss_ce_4: 0.296  loss_cate_4: 0  loss_mask_4: 0.8556  loss_dice_4: 1.105  loss_ce_5: 0.2828  loss_cate_5: 0  loss_mask_5: 0.88  loss_dice_5: 1.117  loss_ce_6: 0.3245  loss_cate_6: 0  loss_mask_6: 0.8433  loss_dice_6: 1.092  loss_ce_7: 0.3519  loss_cate_7: 0  loss_mask_7: 0.8614  loss_dice_7: 1.114  loss_ce_8: 0.3213  loss_cate_8: 0  loss_mask_8: 0.8533  loss_dice_8: 1.086  time: 1.1032  data_time: 0.0131  lr: 9.1545e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:49:04 d2.utils.events]: \u001b[0m eta: 22:08:25  iter: 7499  total_loss: 21.89  loss_ce: 0.2497  loss_cate: 0.1665  loss_mask: 0.7764  loss_dice: 0.9879  loss_ce_0: 0.356  loss_cate_0: 0  loss_mask_0: 0.7808  loss_dice_0: 1.058  loss_ce_1: 0.2227  loss_cate_1: 0  loss_mask_1: 0.8241  loss_dice_1: 0.9949  loss_ce_2: 0.2003  loss_cate_2: 0  loss_mask_2: 0.7933  loss_dice_2: 0.9919  loss_ce_3: 0.2608  loss_cate_3: 0  loss_mask_3: 0.7897  loss_dice_3: 0.9945  loss_ce_4: 0.2605  loss_cate_4: 0  loss_mask_4: 0.7997  loss_dice_4: 0.9839  loss_ce_5: 0.2499  loss_cate_5: 0  loss_mask_5: 0.7843  loss_dice_5: 1.008  loss_ce_6: 0.2587  loss_cate_6: 0  loss_mask_6: 0.8011  loss_dice_6: 1.008  loss_ce_7: 0.2425  loss_cate_7: 0  loss_mask_7: 0.7978  loss_dice_7: 1.009  loss_ce_8: 0.2551  loss_cate_8: 0  loss_mask_8: 0.7974  loss_dice_8: 1.005  time: 1.1033  data_time: 0.0322  lr: 9.1523e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:49:26 d2.utils.events]: \u001b[0m eta: 22:07:50  iter: 7519  total_loss: 19.15  loss_ce: 0.1902  loss_cate: 0.1768  loss_mask: 0.7255  loss_dice: 0.8878  loss_ce_0: 0.3164  loss_cate_0: 0  loss_mask_0: 0.7534  loss_dice_0: 0.9709  loss_ce_1: 0.2032  loss_cate_1: 0  loss_mask_1: 0.7582  loss_dice_1: 0.9253  loss_ce_2: 0.1833  loss_cate_2: 0  loss_mask_2: 0.7388  loss_dice_2: 0.8703  loss_ce_3: 0.1357  loss_cate_3: 0  loss_mask_3: 0.714  loss_dice_3: 0.8667  loss_ce_4: 0.1246  loss_cate_4: 0  loss_mask_4: 0.7227  loss_dice_4: 0.8944  loss_ce_5: 0.1259  loss_cate_5: 0  loss_mask_5: 0.7108  loss_dice_5: 0.8636  loss_ce_6: 0.1671  loss_cate_6: 0  loss_mask_6: 0.6903  loss_dice_6: 0.8824  loss_ce_7: 0.1994  loss_cate_7: 0  loss_mask_7: 0.6926  loss_dice_7: 0.879  loss_ce_8: 0.2136  loss_cate_8: 0  loss_mask_8: 0.7065  loss_dice_8: 0.9082  time: 1.1033  data_time: 0.0125  lr: 9.15e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:49:48 d2.utils.events]: \u001b[0m eta: 22:07:18  iter: 7539  total_loss: 20.28  loss_ce: 0.18  loss_cate: 0.1625  loss_mask: 0.7894  loss_dice: 1.009  loss_ce_0: 0.3659  loss_cate_0: 0  loss_mask_0: 0.7815  loss_dice_0: 1.014  loss_ce_1: 0.195  loss_cate_1: 0  loss_mask_1: 0.7971  loss_dice_1: 0.9952  loss_ce_2: 0.1802  loss_cate_2: 0  loss_mask_2: 0.7684  loss_dice_2: 0.9728  loss_ce_3: 0.1921  loss_cate_3: 0  loss_mask_3: 0.7643  loss_dice_3: 0.9733  loss_ce_4: 0.1745  loss_cate_4: 0  loss_mask_4: 0.7747  loss_dice_4: 0.9537  loss_ce_5: 0.156  loss_cate_5: 0  loss_mask_5: 0.7865  loss_dice_5: 0.9445  loss_ce_6: 0.2013  loss_cate_6: 0  loss_mask_6: 0.8188  loss_dice_6: 1.004  loss_ce_7: 0.1515  loss_cate_7: 0  loss_mask_7: 0.8167  loss_dice_7: 1.028  loss_ce_8: 0.17  loss_cate_8: 0  loss_mask_8: 0.7872  loss_dice_8: 0.9844  time: 1.1032  data_time: 0.0137  lr: 9.1477e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:50:11 d2.utils.events]: \u001b[0m eta: 22:06:47  iter: 7559  total_loss: 22.8  loss_ce: 0.2482  loss_cate: 0.1846  loss_mask: 0.9098  loss_dice: 1.07  loss_ce_0: 0.4304  loss_cate_0: 0  loss_mask_0: 0.966  loss_dice_0: 1.052  loss_ce_1: 0.2489  loss_cate_1: 0  loss_mask_1: 0.8975  loss_dice_1: 1.078  loss_ce_2: 0.2594  loss_cate_2: 0  loss_mask_2: 0.9383  loss_dice_2: 1.075  loss_ce_3: 0.2422  loss_cate_3: 0  loss_mask_3: 0.9568  loss_dice_3: 1.115  loss_ce_4: 0.2197  loss_cate_4: 0  loss_mask_4: 0.9355  loss_dice_4: 1.082  loss_ce_5: 0.2337  loss_cate_5: 0  loss_mask_5: 0.9326  loss_dice_5: 1.104  loss_ce_6: 0.2069  loss_cate_6: 0  loss_mask_6: 0.9305  loss_dice_6: 1.079  loss_ce_7: 0.245  loss_cate_7: 0  loss_mask_7: 0.9331  loss_dice_7: 1.07  loss_ce_8: 0.2214  loss_cate_8: 0  loss_mask_8: 0.9596  loss_dice_8: 1.074  time: 1.1032  data_time: 0.0135  lr: 9.1454e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:50:34 d2.utils.events]: \u001b[0m eta: 22:06:20  iter: 7579  total_loss: 20.64  loss_ce: 0.2399  loss_cate: 0.1695  loss_mask: 0.759  loss_dice: 1.093  loss_ce_0: 0.3195  loss_cate_0: 0  loss_mask_0: 0.8142  loss_dice_0: 1.121  loss_ce_1: 0.2561  loss_cate_1: 0  loss_mask_1: 0.7545  loss_dice_1: 1.121  loss_ce_2: 0.253  loss_cate_2: 0  loss_mask_2: 0.8411  loss_dice_2: 1.139  loss_ce_3: 0.2938  loss_cate_3: 0  loss_mask_3: 0.7876  loss_dice_3: 1.068  loss_ce_4: 0.2855  loss_cate_4: 0  loss_mask_4: 0.785  loss_dice_4: 1.092  loss_ce_5: 0.266  loss_cate_5: 0  loss_mask_5: 0.7902  loss_dice_5: 1.069  loss_ce_6: 0.2437  loss_cate_6: 0  loss_mask_6: 0.7713  loss_dice_6: 1.112  loss_ce_7: 0.2292  loss_cate_7: 0  loss_mask_7: 0.7757  loss_dice_7: 1.066  loss_ce_8: 0.2525  loss_cate_8: 0  loss_mask_8: 0.7753  loss_dice_8: 1.086  time: 1.1032  data_time: 0.0129  lr: 9.1432e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:50:56 d2.utils.events]: \u001b[0m eta: 22:06:06  iter: 7599  total_loss: 19.96  loss_ce: 0.2532  loss_cate: 0.1645  loss_mask: 0.7473  loss_dice: 0.9895  loss_ce_0: 0.3608  loss_cate_0: 0  loss_mask_0: 0.8031  loss_dice_0: 1.017  loss_ce_1: 0.2258  loss_cate_1: 0  loss_mask_1: 0.8602  loss_dice_1: 0.995  loss_ce_2: 0.2362  loss_cate_2: 0  loss_mask_2: 0.8284  loss_dice_2: 0.9978  loss_ce_3: 0.1773  loss_cate_3: 0  loss_mask_3: 0.8371  loss_dice_3: 0.975  loss_ce_4: 0.2033  loss_cate_4: 0  loss_mask_4: 0.8561  loss_dice_4: 1.039  loss_ce_5: 0.2206  loss_cate_5: 0  loss_mask_5: 0.8621  loss_dice_5: 0.9981  loss_ce_6: 0.2473  loss_cate_6: 0  loss_mask_6: 0.8234  loss_dice_6: 0.9843  loss_ce_7: 0.2373  loss_cate_7: 0  loss_mask_7: 0.7792  loss_dice_7: 0.9797  loss_ce_8: 0.2291  loss_cate_8: 0  loss_mask_8: 0.7688  loss_dice_8: 0.9751  time: 1.1032  data_time: 0.0129  lr: 9.1409e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:51:19 d2.utils.events]: \u001b[0m eta: 22:05:41  iter: 7619  total_loss: 20.03  loss_ce: 0.2428  loss_cate: 0.1476  loss_mask: 0.7574  loss_dice: 0.9403  loss_ce_0: 0.3238  loss_cate_0: 0  loss_mask_0: 0.8024  loss_dice_0: 1.039  loss_ce_1: 0.2634  loss_cate_1: 0  loss_mask_1: 0.8031  loss_dice_1: 0.9027  loss_ce_2: 0.2634  loss_cate_2: 0  loss_mask_2: 0.7829  loss_dice_2: 0.9623  loss_ce_3: 0.2326  loss_cate_3: 0  loss_mask_3: 0.7676  loss_dice_3: 0.9589  loss_ce_4: 0.2359  loss_cate_4: 0  loss_mask_4: 0.7688  loss_dice_4: 0.9517  loss_ce_5: 0.2421  loss_cate_5: 0  loss_mask_5: 0.7585  loss_dice_5: 0.9598  loss_ce_6: 0.2583  loss_cate_6: 0  loss_mask_6: 0.7731  loss_dice_6: 1.012  loss_ce_7: 0.254  loss_cate_7: 0  loss_mask_7: 0.7653  loss_dice_7: 0.9799  loss_ce_8: 0.2179  loss_cate_8: 0  loss_mask_8: 0.7539  loss_dice_8: 0.9905  time: 1.1032  data_time: 0.0126  lr: 9.1386e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:51:42 d2.utils.events]: \u001b[0m eta: 22:05:08  iter: 7639  total_loss: 25.3  loss_ce: 0.2994  loss_cate: 0.1865  loss_mask: 0.9552  loss_dice: 1.194  loss_ce_0: 0.4529  loss_cate_0: 0  loss_mask_0: 0.8759  loss_dice_0: 1.237  loss_ce_1: 0.329  loss_cate_1: 0  loss_mask_1: 0.9451  loss_dice_1: 1.247  loss_ce_2: 0.3023  loss_cate_2: 0  loss_mask_2: 0.9544  loss_dice_2: 1.207  loss_ce_3: 0.2612  loss_cate_3: 0  loss_mask_3: 0.9535  loss_dice_3: 1.159  loss_ce_4: 0.2841  loss_cate_4: 0  loss_mask_4: 0.9584  loss_dice_4: 1.193  loss_ce_5: 0.2799  loss_cate_5: 0  loss_mask_5: 0.9788  loss_dice_5: 1.214  loss_ce_6: 0.322  loss_cate_6: 0  loss_mask_6: 0.9727  loss_dice_6: 1.205  loss_ce_7: 0.2632  loss_cate_7: 0  loss_mask_7: 0.997  loss_dice_7: 1.193  loss_ce_8: 0.2856  loss_cate_8: 0  loss_mask_8: 0.9602  loss_dice_8: 1.189  time: 1.1032  data_time: 0.0132  lr: 9.1364e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:52:04 d2.utils.events]: \u001b[0m eta: 22:04:48  iter: 7659  total_loss: 21.35  loss_ce: 0.2403  loss_cate: 0.1433  loss_mask: 0.7503  loss_dice: 1.09  loss_ce_0: 0.3693  loss_cate_0: 0  loss_mask_0: 0.8452  loss_dice_0: 1.083  loss_ce_1: 0.2807  loss_cate_1: 0  loss_mask_1: 0.8644  loss_dice_1: 1.057  loss_ce_2: 0.2754  loss_cate_2: 0  loss_mask_2: 0.836  loss_dice_2: 1.059  loss_ce_3: 0.2871  loss_cate_3: 0  loss_mask_3: 0.8163  loss_dice_3: 1.039  loss_ce_4: 0.2348  loss_cate_4: 0  loss_mask_4: 0.8219  loss_dice_4: 1.081  loss_ce_5: 0.2132  loss_cate_5: 0  loss_mask_5: 0.8  loss_dice_5: 1.081  loss_ce_6: 0.2582  loss_cate_6: 0  loss_mask_6: 0.7929  loss_dice_6: 1.093  loss_ce_7: 0.2578  loss_cate_7: 0  loss_mask_7: 0.8017  loss_dice_7: 1.076  loss_ce_8: 0.2606  loss_cate_8: 0  loss_mask_8: 0.8047  loss_dice_8: 1.131  time: 1.1032  data_time: 0.0145  lr: 9.1341e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:52:27 d2.utils.events]: \u001b[0m eta: 22:04:22  iter: 7679  total_loss: 21.8  loss_ce: 0.2291  loss_cate: 0.1458  loss_mask: 0.8797  loss_dice: 1.029  loss_ce_0: 0.3697  loss_cate_0: 0  loss_mask_0: 0.8738  loss_dice_0: 1.093  loss_ce_1: 0.2606  loss_cate_1: 0  loss_mask_1: 0.8394  loss_dice_1: 1.05  loss_ce_2: 0.2639  loss_cate_2: 0  loss_mask_2: 0.8549  loss_dice_2: 1.028  loss_ce_3: 0.2347  loss_cate_3: 0  loss_mask_3: 0.9003  loss_dice_3: 1.028  loss_ce_4: 0.2438  loss_cate_4: 0  loss_mask_4: 0.8307  loss_dice_4: 1.004  loss_ce_5: 0.2349  loss_cate_5: 0  loss_mask_5: 0.8779  loss_dice_5: 1.012  loss_ce_6: 0.2397  loss_cate_6: 0  loss_mask_6: 0.8716  loss_dice_6: 1.015  loss_ce_7: 0.2529  loss_cate_7: 0  loss_mask_7: 0.8676  loss_dice_7: 0.9775  loss_ce_8: 0.2235  loss_cate_8: 0  loss_mask_8: 0.8659  loss_dice_8: 1.012  time: 1.1032  data_time: 0.0129  lr: 9.1318e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:52:51 d2.utils.events]: \u001b[0m eta: 22:04:04  iter: 7699  total_loss: 20.8  loss_ce: 0.2059  loss_cate: 0.1613  loss_mask: 0.8663  loss_dice: 0.8712  loss_ce_0: 0.2848  loss_cate_0: 0  loss_mask_0: 0.8965  loss_dice_0: 0.9758  loss_ce_1: 0.2178  loss_cate_1: 0  loss_mask_1: 0.8515  loss_dice_1: 0.938  loss_ce_2: 0.1719  loss_cate_2: 0  loss_mask_2: 0.838  loss_dice_2: 0.9127  loss_ce_3: 0.2093  loss_cate_3: 0  loss_mask_3: 0.8305  loss_dice_3: 0.9175  loss_ce_4: 0.1435  loss_cate_4: 0  loss_mask_4: 0.8981  loss_dice_4: 0.8921  loss_ce_5: 0.2074  loss_cate_5: 0  loss_mask_5: 0.8725  loss_dice_5: 0.8956  loss_ce_6: 0.17  loss_cate_6: 0  loss_mask_6: 0.845  loss_dice_6: 0.8745  loss_ce_7: 0.201  loss_cate_7: 0  loss_mask_7: 0.8485  loss_dice_7: 0.8733  loss_ce_8: 0.1575  loss_cate_8: 0  loss_mask_8: 0.8602  loss_dice_8: 0.914  time: 1.1032  data_time: 0.0139  lr: 9.1295e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:53:13 d2.utils.events]: \u001b[0m eta: 22:03:40  iter: 7719  total_loss: 22.82  loss_ce: 0.2464  loss_cate: 0.1654  loss_mask: 0.8138  loss_dice: 0.9711  loss_ce_0: 0.4479  loss_cate_0: 0  loss_mask_0: 0.9046  loss_dice_0: 1.039  loss_ce_1: 0.288  loss_cate_1: 0  loss_mask_1: 0.8285  loss_dice_1: 1.01  loss_ce_2: 0.2227  loss_cate_2: 0  loss_mask_2: 0.8099  loss_dice_2: 0.9647  loss_ce_3: 0.1684  loss_cate_3: 0  loss_mask_3: 0.8138  loss_dice_3: 0.9877  loss_ce_4: 0.1518  loss_cate_4: 0  loss_mask_4: 0.806  loss_dice_4: 0.9918  loss_ce_5: 0.161  loss_cate_5: 0  loss_mask_5: 0.818  loss_dice_5: 0.9687  loss_ce_6: 0.1776  loss_cate_6: 0  loss_mask_6: 0.8097  loss_dice_6: 0.9684  loss_ce_7: 0.1644  loss_cate_7: 0  loss_mask_7: 0.8185  loss_dice_7: 0.9725  loss_ce_8: 0.2851  loss_cate_8: 0  loss_mask_8: 0.8072  loss_dice_8: 0.9996  time: 1.1032  data_time: 0.0137  lr: 9.1273e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:53:37 d2.utils.events]: \u001b[0m eta: 22:03:15  iter: 7739  total_loss: 23.7  loss_ce: 0.2564  loss_cate: 0.1697  loss_mask: 0.9103  loss_dice: 1.038  loss_ce_0: 0.3149  loss_cate_0: 0  loss_mask_0: 0.9126  loss_dice_0: 1.204  loss_ce_1: 0.3569  loss_cate_1: 0  loss_mask_1: 0.8961  loss_dice_1: 1.052  loss_ce_2: 0.2555  loss_cate_2: 0  loss_mask_2: 0.8997  loss_dice_2: 1.037  loss_ce_3: 0.256  loss_cate_3: 0  loss_mask_3: 0.9304  loss_dice_3: 1.082  loss_ce_4: 0.201  loss_cate_4: 0  loss_mask_4: 0.9315  loss_dice_4: 1.036  loss_ce_5: 0.2175  loss_cate_5: 0  loss_mask_5: 0.9241  loss_dice_5: 1.097  loss_ce_6: 0.2424  loss_cate_6: 0  loss_mask_6: 0.9155  loss_dice_6: 1.073  loss_ce_7: 0.2573  loss_cate_7: 0  loss_mask_7: 0.8999  loss_dice_7: 1.086  loss_ce_8: 0.2544  loss_cate_8: 0  loss_mask_8: 0.8997  loss_dice_8: 1.065  time: 1.1032  data_time: 0.0129  lr: 9.125e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:54:00 d2.utils.events]: \u001b[0m eta: 22:02:47  iter: 7759  total_loss: 19.06  loss_ce: 0.3233  loss_cate: 0.1476  loss_mask: 0.7481  loss_dice: 0.8728  loss_ce_0: 0.3717  loss_cate_0: 0  loss_mask_0: 0.76  loss_dice_0: 0.946  loss_ce_1: 0.2995  loss_cate_1: 0  loss_mask_1: 0.7238  loss_dice_1: 0.9259  loss_ce_2: 0.2569  loss_cate_2: 0  loss_mask_2: 0.7575  loss_dice_2: 0.8724  loss_ce_3: 0.2656  loss_cate_3: 0  loss_mask_3: 0.6917  loss_dice_3: 0.8746  loss_ce_4: 0.2575  loss_cate_4: 0  loss_mask_4: 0.7313  loss_dice_4: 0.8736  loss_ce_5: 0.1937  loss_cate_5: 0  loss_mask_5: 0.7342  loss_dice_5: 0.8698  loss_ce_6: 0.2795  loss_cate_6: 0  loss_mask_6: 0.7551  loss_dice_6: 0.8874  loss_ce_7: 0.2904  loss_cate_7: 0  loss_mask_7: 0.749  loss_dice_7: 0.8905  loss_ce_8: 0.2861  loss_cate_8: 0  loss_mask_8: 0.755  loss_dice_8: 0.8731  time: 1.1032  data_time: 0.0127  lr: 9.1227e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:54:23 d2.utils.events]: \u001b[0m eta: 22:02:22  iter: 7779  total_loss: 21.25  loss_ce: 0.1628  loss_cate: 0.1566  loss_mask: 0.8415  loss_dice: 0.9349  loss_ce_0: 0.2949  loss_cate_0: 0  loss_mask_0: 0.8801  loss_dice_0: 0.9548  loss_ce_1: 0.1916  loss_cate_1: 0  loss_mask_1: 0.9076  loss_dice_1: 1.015  loss_ce_2: 0.1864  loss_cate_2: 0  loss_mask_2: 0.8972  loss_dice_2: 0.9983  loss_ce_3: 0.1848  loss_cate_3: 0  loss_mask_3: 0.8801  loss_dice_3: 0.957  loss_ce_4: 0.1803  loss_cate_4: 0  loss_mask_4: 0.8826  loss_dice_4: 0.9625  loss_ce_5: 0.187  loss_cate_5: 0  loss_mask_5: 0.8541  loss_dice_5: 0.9243  loss_ce_6: 0.1667  loss_cate_6: 0  loss_mask_6: 0.8732  loss_dice_6: 0.9681  loss_ce_7: 0.1617  loss_cate_7: 0  loss_mask_7: 0.8529  loss_dice_7: 0.9647  loss_ce_8: 0.1643  loss_cate_8: 0  loss_mask_8: 0.8719  loss_dice_8: 0.9696  time: 1.1031  data_time: 0.0127  lr: 9.1204e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:54:46 d2.utils.events]: \u001b[0m eta: 22:02:07  iter: 7799  total_loss: 20.43  loss_ce: 0.2625  loss_cate: 0.1666  loss_mask: 0.8048  loss_dice: 1.013  loss_ce_0: 0.3719  loss_cate_0: 0  loss_mask_0: 0.8024  loss_dice_0: 1.035  loss_ce_1: 0.2405  loss_cate_1: 0  loss_mask_1: 0.8146  loss_dice_1: 1.026  loss_ce_2: 0.2746  loss_cate_2: 0  loss_mask_2: 0.7928  loss_dice_2: 1.05  loss_ce_3: 0.2698  loss_cate_3: 0  loss_mask_3: 0.8256  loss_dice_3: 1.01  loss_ce_4: 0.2814  loss_cate_4: 0  loss_mask_4: 0.8343  loss_dice_4: 1.031  loss_ce_5: 0.2916  loss_cate_5: 0  loss_mask_5: 0.8275  loss_dice_5: 0.9792  loss_ce_6: 0.2729  loss_cate_6: 0  loss_mask_6: 0.824  loss_dice_6: 1.044  loss_ce_7: 0.2574  loss_cate_7: 0  loss_mask_7: 0.819  loss_dice_7: 1.048  loss_ce_8: 0.2589  loss_cate_8: 0  loss_mask_8: 0.7952  loss_dice_8: 1.02  time: 1.1031  data_time: 0.0128  lr: 9.1182e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:55:09 d2.utils.events]: \u001b[0m eta: 22:01:54  iter: 7819  total_loss: 22.82  loss_ce: 0.3082  loss_cate: 0.1738  loss_mask: 0.7781  loss_dice: 1.041  loss_ce_0: 0.4878  loss_cate_0: 0  loss_mask_0: 0.821  loss_dice_0: 1.055  loss_ce_1: 0.3511  loss_cate_1: 0  loss_mask_1: 0.7907  loss_dice_1: 1.068  loss_ce_2: 0.3469  loss_cate_2: 0  loss_mask_2: 0.8023  loss_dice_2: 1.031  loss_ce_3: 0.3061  loss_cate_3: 0  loss_mask_3: 0.8244  loss_dice_3: 1.028  loss_ce_4: 0.2853  loss_cate_4: 0  loss_mask_4: 0.7819  loss_dice_4: 1.026  loss_ce_5: 0.3342  loss_cate_5: 0  loss_mask_5: 0.77  loss_dice_5: 1.058  loss_ce_6: 0.307  loss_cate_6: 0  loss_mask_6: 0.8154  loss_dice_6: 1.032  loss_ce_7: 0.3001  loss_cate_7: 0  loss_mask_7: 0.8413  loss_dice_7: 1.068  loss_ce_8: 0.3137  loss_cate_8: 0  loss_mask_8: 0.837  loss_dice_8: 1.027  time: 1.1032  data_time: 0.0140  lr: 9.1159e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:55:32 d2.utils.events]: \u001b[0m eta: 22:01:25  iter: 7839  total_loss: 22.82  loss_ce: 0.2201  loss_cate: 0.1633  loss_mask: 0.8658  loss_dice: 1.08  loss_ce_0: 0.3979  loss_cate_0: 0  loss_mask_0: 0.8818  loss_dice_0: 1.099  loss_ce_1: 0.2521  loss_cate_1: 0  loss_mask_1: 0.8656  loss_dice_1: 1.095  loss_ce_2: 0.2732  loss_cate_2: 0  loss_mask_2: 0.8735  loss_dice_2: 1.093  loss_ce_3: 0.3001  loss_cate_3: 0  loss_mask_3: 0.9159  loss_dice_3: 1.093  loss_ce_4: 0.2635  loss_cate_4: 0  loss_mask_4: 0.9025  loss_dice_4: 1.072  loss_ce_5: 0.2706  loss_cate_5: 0  loss_mask_5: 0.8821  loss_dice_5: 1.069  loss_ce_6: 0.2136  loss_cate_6: 0  loss_mask_6: 0.8935  loss_dice_6: 1.101  loss_ce_7: 0.2378  loss_cate_7: 0  loss_mask_7: 0.895  loss_dice_7: 1.092  loss_ce_8: 0.1974  loss_cate_8: 0  loss_mask_8: 0.8818  loss_dice_8: 1.073  time: 1.1031  data_time: 0.0133  lr: 9.1136e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:55:57 d2.utils.events]: \u001b[0m eta: 22:01:01  iter: 7859  total_loss: 22  loss_ce: 0.2493  loss_cate: 0.1743  loss_mask: 0.8662  loss_dice: 0.9754  loss_ce_0: 0.3281  loss_cate_0: 0  loss_mask_0: 0.9205  loss_dice_0: 0.9701  loss_ce_1: 0.2498  loss_cate_1: 0  loss_mask_1: 0.8934  loss_dice_1: 0.9828  loss_ce_2: 0.2162  loss_cate_2: 0  loss_mask_2: 0.9088  loss_dice_2: 0.9619  loss_ce_3: 0.2011  loss_cate_3: 0  loss_mask_3: 0.9222  loss_dice_3: 0.9654  loss_ce_4: 0.2909  loss_cate_4: 0  loss_mask_4: 0.8995  loss_dice_4: 0.957  loss_ce_5: 0.2635  loss_cate_5: 0  loss_mask_5: 0.8934  loss_dice_5: 0.9531  loss_ce_6: 0.2368  loss_cate_6: 0  loss_mask_6: 0.8927  loss_dice_6: 0.9805  loss_ce_7: 0.2374  loss_cate_7: 0  loss_mask_7: 0.8882  loss_dice_7: 0.9766  loss_ce_8: 0.2426  loss_cate_8: 0  loss_mask_8: 0.862  loss_dice_8: 0.986  time: 1.1031  data_time: 0.0127  lr: 9.1114e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:56:20 d2.utils.events]: \u001b[0m eta: 22:00:38  iter: 7879  total_loss: 21.29  loss_ce: 0.1635  loss_cate: 0.1766  loss_mask: 0.8698  loss_dice: 0.9063  loss_ce_0: 0.2617  loss_cate_0: 0  loss_mask_0: 0.9145  loss_dice_0: 1.053  loss_ce_1: 0.2285  loss_cate_1: 0  loss_mask_1: 0.8538  loss_dice_1: 0.9815  loss_ce_2: 0.1928  loss_cate_2: 0  loss_mask_2: 0.9202  loss_dice_2: 0.9768  loss_ce_3: 0.2021  loss_cate_3: 0  loss_mask_3: 0.9145  loss_dice_3: 0.9262  loss_ce_4: 0.1984  loss_cate_4: 0  loss_mask_4: 0.9027  loss_dice_4: 0.9345  loss_ce_5: 0.1764  loss_cate_5: 0  loss_mask_5: 0.8924  loss_dice_5: 0.9655  loss_ce_6: 0.2043  loss_cate_6: 0  loss_mask_6: 0.9183  loss_dice_6: 0.944  loss_ce_7: 0.2046  loss_cate_7: 0  loss_mask_7: 0.8579  loss_dice_7: 0.9746  loss_ce_8: 0.1982  loss_cate_8: 0  loss_mask_8: 0.8537  loss_dice_8: 0.9565  time: 1.1031  data_time: 0.0129  lr: 9.1091e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:56:43 d2.utils.events]: \u001b[0m eta: 22:00:19  iter: 7899  total_loss: 20.36  loss_ce: 0.3  loss_cate: 0.1579  loss_mask: 0.7737  loss_dice: 0.9066  loss_ce_0: 0.3323  loss_cate_0: 0  loss_mask_0: 0.8085  loss_dice_0: 1.06  loss_ce_1: 0.2826  loss_cate_1: 0  loss_mask_1: 0.8102  loss_dice_1: 0.9671  loss_ce_2: 0.1992  loss_cate_2: 0  loss_mask_2: 0.7837  loss_dice_2: 1.027  loss_ce_3: 0.2472  loss_cate_3: 0  loss_mask_3: 0.7728  loss_dice_3: 0.9747  loss_ce_4: 0.2785  loss_cate_4: 0  loss_mask_4: 0.7732  loss_dice_4: 0.9446  loss_ce_5: 0.2411  loss_cate_5: 0  loss_mask_5: 0.7998  loss_dice_5: 0.9651  loss_ce_6: 0.2265  loss_cate_6: 0  loss_mask_6: 0.7757  loss_dice_6: 0.9219  loss_ce_7: 0.2328  loss_cate_7: 0  loss_mask_7: 0.791  loss_dice_7: 0.9193  loss_ce_8: 0.2953  loss_cate_8: 0  loss_mask_8: 0.7924  loss_dice_8: 0.9395  time: 1.1031  data_time: 0.0141  lr: 9.1068e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:57:05 d2.utils.events]: \u001b[0m eta: 21:59:57  iter: 7919  total_loss: 20.54  loss_ce: 0.2352  loss_cate: 0.174  loss_mask: 0.7309  loss_dice: 1.01  loss_ce_0: 0.3315  loss_cate_0: 0  loss_mask_0: 0.7715  loss_dice_0: 1.077  loss_ce_1: 0.2316  loss_cate_1: 0  loss_mask_1: 0.8562  loss_dice_1: 0.9758  loss_ce_2: 0.1719  loss_cate_2: 0  loss_mask_2: 0.7279  loss_dice_2: 1.029  loss_ce_3: 0.1869  loss_cate_3: 0  loss_mask_3: 0.7637  loss_dice_3: 1.008  loss_ce_4: 0.24  loss_cate_4: 0  loss_mask_4: 0.726  loss_dice_4: 1.004  loss_ce_5: 0.219  loss_cate_5: 0  loss_mask_5: 0.7314  loss_dice_5: 1.012  loss_ce_6: 0.223  loss_cate_6: 0  loss_mask_6: 0.7499  loss_dice_6: 1.015  loss_ce_7: 0.2361  loss_cate_7: 0  loss_mask_7: 0.7564  loss_dice_7: 1.003  loss_ce_8: 0.2462  loss_cate_8: 0  loss_mask_8: 0.7324  loss_dice_8: 0.9924  time: 1.1031  data_time: 0.0133  lr: 9.1045e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:57:28 d2.utils.events]: \u001b[0m eta: 21:59:17  iter: 7939  total_loss: 21.86  loss_ce: 0.2465  loss_cate: 0.1861  loss_mask: 0.8957  loss_dice: 0.9522  loss_ce_0: 0.4169  loss_cate_0: 0  loss_mask_0: 0.9661  loss_dice_0: 1.071  loss_ce_1: 0.3115  loss_cate_1: 0  loss_mask_1: 0.9055  loss_dice_1: 1.018  loss_ce_2: 0.2788  loss_cate_2: 0  loss_mask_2: 0.8957  loss_dice_2: 0.9462  loss_ce_3: 0.2583  loss_cate_3: 0  loss_mask_3: 0.9198  loss_dice_3: 0.9734  loss_ce_4: 0.2728  loss_cate_4: 0  loss_mask_4: 0.9163  loss_dice_4: 1.013  loss_ce_5: 0.2611  loss_cate_5: 0  loss_mask_5: 0.9153  loss_dice_5: 0.973  loss_ce_6: 0.253  loss_cate_6: 0  loss_mask_6: 0.9112  loss_dice_6: 0.982  loss_ce_7: 0.2543  loss_cate_7: 0  loss_mask_7: 0.882  loss_dice_7: 0.9612  loss_ce_8: 0.2359  loss_cate_8: 0  loss_mask_8: 0.9064  loss_dice_8: 0.9657  time: 1.1031  data_time: 0.0125  lr: 9.1023e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:57:50 d2.utils.events]: \u001b[0m eta: 21:58:50  iter: 7959  total_loss: 20.05  loss_ce: 0.1688  loss_cate: 0.1498  loss_mask: 0.7697  loss_dice: 0.9092  loss_ce_0: 0.2404  loss_cate_0: 0  loss_mask_0: 0.8075  loss_dice_0: 1.006  loss_ce_1: 0.1429  loss_cate_1: 0  loss_mask_1: 0.7708  loss_dice_1: 0.9461  loss_ce_2: 0.1977  loss_cate_2: 0  loss_mask_2: 0.7833  loss_dice_2: 0.9178  loss_ce_3: 0.1016  loss_cate_3: 0  loss_mask_3: 0.7704  loss_dice_3: 0.9419  loss_ce_4: 0.1011  loss_cate_4: 0  loss_mask_4: 0.7695  loss_dice_4: 0.8991  loss_ce_5: 0.2252  loss_cate_5: 0  loss_mask_5: 0.752  loss_dice_5: 0.8725  loss_ce_6: 0.2194  loss_cate_6: 0  loss_mask_6: 0.7686  loss_dice_6: 0.9321  loss_ce_7: 0.1259  loss_cate_7: 0  loss_mask_7: 0.7923  loss_dice_7: 0.8849  loss_ce_8: 0.1177  loss_cate_8: 0  loss_mask_8: 0.7497  loss_dice_8: 0.8879  time: 1.1031  data_time: 0.0128  lr: 9.1e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:58:13 d2.utils.events]: \u001b[0m eta: 21:58:13  iter: 7979  total_loss: 23.82  loss_ce: 0.283  loss_cate: 0.1738  loss_mask: 0.8826  loss_dice: 1.057  loss_ce_0: 0.4733  loss_cate_0: 0  loss_mask_0: 0.9026  loss_dice_0: 1.07  loss_ce_1: 0.323  loss_cate_1: 0  loss_mask_1: 0.8676  loss_dice_1: 1.11  loss_ce_2: 0.3283  loss_cate_2: 0  loss_mask_2: 0.9172  loss_dice_2: 1.146  loss_ce_3: 0.3195  loss_cate_3: 0  loss_mask_3: 0.9008  loss_dice_3: 1.039  loss_ce_4: 0.3503  loss_cate_4: 0  loss_mask_4: 0.8838  loss_dice_4: 1.047  loss_ce_5: 0.2745  loss_cate_5: 0  loss_mask_5: 0.9051  loss_dice_5: 1.084  loss_ce_6: 0.2991  loss_cate_6: 0  loss_mask_6: 0.9056  loss_dice_6: 1.033  loss_ce_7: 0.237  loss_cate_7: 0  loss_mask_7: 0.93  loss_dice_7: 1.064  loss_ce_8: 0.2509  loss_cate_8: 0  loss_mask_8: 0.9159  loss_dice_8: 1.103  time: 1.1031  data_time: 0.0127  lr: 9.0977e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 14:59:51 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 14:59:51 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 14:59:51 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 15:00:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 15:00:59 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0023 s/iter. Inference: 0.1633 s/iter. Eval: 0.0751 s/iter. Total: 0.2408 s/iter. ETA=0:08:02\n",
      "\u001b[32m[10/12 15:01:04 d2.evaluation.evaluator]: \u001b[0mInference done 37/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0387 s/iter. Total: 0.2047 s/iter. ETA=0:06:45\n",
      "\u001b[32m[10/12 15:01:09 d2.evaluation.evaluator]: \u001b[0mInference done 64/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0325 s/iter. Total: 0.1985 s/iter. ETA=0:06:27\n",
      "\u001b[32m[10/12 15:01:14 d2.evaluation.evaluator]: \u001b[0mInference done 90/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0312 s/iter. Total: 0.1971 s/iter. ETA=0:06:19\n",
      "\u001b[32m[10/12 15:01:19 d2.evaluation.evaluator]: \u001b[0mInference done 115/2016. Dataloading: 0.0024 s/iter. Inference: 0.1632 s/iter. Eval: 0.0322 s/iter. Total: 0.1980 s/iter. ETA=0:06:16\n",
      "\u001b[32m[10/12 15:01:24 d2.evaluation.evaluator]: \u001b[0mInference done 141/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0314 s/iter. Total: 0.1976 s/iter. ETA=0:06:10\n",
      "\u001b[32m[10/12 15:01:29 d2.evaluation.evaluator]: \u001b[0mInference done 166/2016. Dataloading: 0.0024 s/iter. Inference: 0.1637 s/iter. Eval: 0.0324 s/iter. Total: 0.1986 s/iter. ETA=0:06:07\n",
      "\u001b[32m[10/12 15:01:35 d2.evaluation.evaluator]: \u001b[0mInference done 193/2016. Dataloading: 0.0024 s/iter. Inference: 0.1636 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:05:59\n",
      "\u001b[32m[10/12 15:01:40 d2.evaluation.evaluator]: \u001b[0mInference done 219/2016. Dataloading: 0.0024 s/iter. Inference: 0.1635 s/iter. Eval: 0.0309 s/iter. Total: 0.1970 s/iter. ETA=0:05:54\n",
      "\u001b[32m[10/12 15:01:45 d2.evaluation.evaluator]: \u001b[0mInference done 245/2016. Dataloading: 0.0024 s/iter. Inference: 0.1635 s/iter. Eval: 0.0306 s/iter. Total: 0.1966 s/iter. ETA=0:05:48\n",
      "\u001b[32m[10/12 15:01:50 d2.evaluation.evaluator]: \u001b[0mInference done 269/2016. Dataloading: 0.0024 s/iter. Inference: 0.1635 s/iter. Eval: 0.0317 s/iter. Total: 0.1977 s/iter. ETA=0:05:45\n",
      "\u001b[32m[10/12 15:01:55 d2.evaluation.evaluator]: \u001b[0mInference done 295/2016. Dataloading: 0.0024 s/iter. Inference: 0.1634 s/iter. Eval: 0.0317 s/iter. Total: 0.1978 s/iter. ETA=0:05:40\n",
      "\u001b[32m[10/12 15:02:00 d2.evaluation.evaluator]: \u001b[0mInference done 322/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0313 s/iter. Total: 0.1972 s/iter. ETA=0:05:34\n",
      "\u001b[32m[10/12 15:02:05 d2.evaluation.evaluator]: \u001b[0mInference done 347/2016. Dataloading: 0.0024 s/iter. Inference: 0.1633 s/iter. Eval: 0.0317 s/iter. Total: 0.1976 s/iter. ETA=0:05:29\n",
      "\u001b[32m[10/12 15:02:10 d2.evaluation.evaluator]: \u001b[0mInference done 372/2016. Dataloading: 0.0034 s/iter. Inference: 0.1633 s/iter. Eval: 0.0309 s/iter. Total: 0.1978 s/iter. ETA=0:05:25\n",
      "\u001b[32m[10/12 15:02:15 d2.evaluation.evaluator]: \u001b[0mInference done 398/2016. Dataloading: 0.0033 s/iter. Inference: 0.1632 s/iter. Eval: 0.0311 s/iter. Total: 0.1978 s/iter. ETA=0:05:20\n",
      "\u001b[32m[10/12 15:02:20 d2.evaluation.evaluator]: \u001b[0mInference done 424/2016. Dataloading: 0.0033 s/iter. Inference: 0.1633 s/iter. Eval: 0.0310 s/iter. Total: 0.1976 s/iter. ETA=0:05:14\n",
      "\u001b[32m[10/12 15:02:25 d2.evaluation.evaluator]: \u001b[0mInference done 451/2016. Dataloading: 0.0032 s/iter. Inference: 0.1632 s/iter. Eval: 0.0306 s/iter. Total: 0.1972 s/iter. ETA=0:05:08\n",
      "\u001b[32m[10/12 15:02:31 d2.evaluation.evaluator]: \u001b[0mInference done 476/2016. Dataloading: 0.0032 s/iter. Inference: 0.1632 s/iter. Eval: 0.0310 s/iter. Total: 0.1976 s/iter. ETA=0:05:04\n",
      "\u001b[32m[10/12 15:02:36 d2.evaluation.evaluator]: \u001b[0mInference done 502/2016. Dataloading: 0.0031 s/iter. Inference: 0.1633 s/iter. Eval: 0.0309 s/iter. Total: 0.1975 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 15:02:41 d2.evaluation.evaluator]: \u001b[0mInference done 528/2016. Dataloading: 0.0031 s/iter. Inference: 0.1633 s/iter. Eval: 0.0308 s/iter. Total: 0.1973 s/iter. ETA=0:04:53\n",
      "\u001b[32m[10/12 15:02:46 d2.evaluation.evaluator]: \u001b[0mInference done 555/2016. Dataloading: 0.0030 s/iter. Inference: 0.1633 s/iter. Eval: 0.0306 s/iter. Total: 0.1970 s/iter. ETA=0:04:47\n",
      "\u001b[32m[10/12 15:02:51 d2.evaluation.evaluator]: \u001b[0mInference done 581/2016. Dataloading: 0.0030 s/iter. Inference: 0.1632 s/iter. Eval: 0.0305 s/iter. Total: 0.1969 s/iter. ETA=0:04:42\n",
      "\u001b[32m[10/12 15:02:56 d2.evaluation.evaluator]: \u001b[0mInference done 607/2016. Dataloading: 0.0030 s/iter. Inference: 0.1632 s/iter. Eval: 0.0306 s/iter. Total: 0.1970 s/iter. ETA=0:04:37\n",
      "\u001b[32m[10/12 15:03:01 d2.evaluation.evaluator]: \u001b[0mInference done 633/2016. Dataloading: 0.0029 s/iter. Inference: 0.1632 s/iter. Eval: 0.0307 s/iter. Total: 0.1970 s/iter. ETA=0:04:32\n",
      "\u001b[32m[10/12 15:03:06 d2.evaluation.evaluator]: \u001b[0mInference done 659/2016. Dataloading: 0.0030 s/iter. Inference: 0.1632 s/iter. Eval: 0.0306 s/iter. Total: 0.1970 s/iter. ETA=0:04:27\n",
      "\u001b[32m[10/12 15:03:11 d2.evaluation.evaluator]: \u001b[0mInference done 686/2016. Dataloading: 0.0029 s/iter. Inference: 0.1632 s/iter. Eval: 0.0304 s/iter. Total: 0.1968 s/iter. ETA=0:04:21\n",
      "\u001b[32m[10/12 15:03:17 d2.evaluation.evaluator]: \u001b[0mInference done 711/2016. Dataloading: 0.0029 s/iter. Inference: 0.1632 s/iter. Eval: 0.0306 s/iter. Total: 0.1969 s/iter. ETA=0:04:16\n",
      "\u001b[32m[10/12 15:03:22 d2.evaluation.evaluator]: \u001b[0mInference done 738/2016. Dataloading: 0.0029 s/iter. Inference: 0.1632 s/iter. Eval: 0.0303 s/iter. Total: 0.1966 s/iter. ETA=0:04:11\n",
      "\u001b[32m[10/12 15:03:27 d2.evaluation.evaluator]: \u001b[0mInference done 763/2016. Dataloading: 0.0029 s/iter. Inference: 0.1633 s/iter. Eval: 0.0306 s/iter. Total: 0.1969 s/iter. ETA=0:04:06\n",
      "\u001b[32m[10/12 15:03:32 d2.evaluation.evaluator]: \u001b[0mInference done 790/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0303 s/iter. Total: 0.1966 s/iter. ETA=0:04:01\n",
      "\u001b[32m[10/12 15:03:37 d2.evaluation.evaluator]: \u001b[0mInference done 814/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0307 s/iter. Total: 0.1970 s/iter. ETA=0:03:56\n",
      "\u001b[32m[10/12 15:03:42 d2.evaluation.evaluator]: \u001b[0mInference done 840/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0307 s/iter. Total: 0.1969 s/iter. ETA=0:03:51\n",
      "\u001b[32m[10/12 15:03:47 d2.evaluation.evaluator]: \u001b[0mInference done 866/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0306 s/iter. Total: 0.1968 s/iter. ETA=0:03:46\n",
      "\u001b[32m[10/12 15:03:52 d2.evaluation.evaluator]: \u001b[0mInference done 891/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0308 s/iter. Total: 0.1970 s/iter. ETA=0:03:41\n",
      "\u001b[32m[10/12 15:03:57 d2.evaluation.evaluator]: \u001b[0mInference done 918/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0305 s/iter. Total: 0.1967 s/iter. ETA=0:03:36\n",
      "\u001b[32m[10/12 15:04:02 d2.evaluation.evaluator]: \u001b[0mInference done 943/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0307 s/iter. Total: 0.1968 s/iter. ETA=0:03:31\n",
      "\u001b[32m[10/12 15:04:07 d2.evaluation.evaluator]: \u001b[0mInference done 969/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0306 s/iter. Total: 0.1967 s/iter. ETA=0:03:25\n",
      "\u001b[32m[10/12 15:04:12 d2.evaluation.evaluator]: \u001b[0mInference done 995/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0306 s/iter. Total: 0.1967 s/iter. ETA=0:03:20\n",
      "\u001b[32m[10/12 15:04:17 d2.evaluation.evaluator]: \u001b[0mInference done 1019/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0309 s/iter. Total: 0.1970 s/iter. ETA=0:03:16\n",
      "\u001b[32m[10/12 15:04:22 d2.evaluation.evaluator]: \u001b[0mInference done 1046/2016. Dataloading: 0.0027 s/iter. Inference: 0.1632 s/iter. Eval: 0.0307 s/iter. Total: 0.1969 s/iter. ETA=0:03:10\n",
      "\u001b[32m[10/12 15:04:28 d2.evaluation.evaluator]: \u001b[0mInference done 1071/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0309 s/iter. Total: 0.1971 s/iter. ETA=0:03:06\n",
      "\u001b[32m[10/12 15:04:33 d2.evaluation.evaluator]: \u001b[0mInference done 1096/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0310 s/iter. Total: 0.1972 s/iter. ETA=0:03:01\n",
      "\u001b[32m[10/12 15:04:38 d2.evaluation.evaluator]: \u001b[0mInference done 1123/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0309 s/iter. Total: 0.1970 s/iter. ETA=0:02:55\n",
      "\u001b[32m[10/12 15:04:43 d2.evaluation.evaluator]: \u001b[0mInference done 1149/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0309 s/iter. Total: 0.1971 s/iter. ETA=0:02:50\n",
      "\u001b[32m[10/12 15:04:48 d2.evaluation.evaluator]: \u001b[0mInference done 1175/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0309 s/iter. Total: 0.1971 s/iter. ETA=0:02:45\n",
      "\u001b[32m[10/12 15:04:53 d2.evaluation.evaluator]: \u001b[0mInference done 1199/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0312 s/iter. Total: 0.1973 s/iter. ETA=0:02:41\n",
      "\u001b[32m[10/12 15:04:58 d2.evaluation.evaluator]: \u001b[0mInference done 1224/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0312 s/iter. Total: 0.1974 s/iter. ETA=0:02:36\n",
      "\u001b[32m[10/12 15:05:03 d2.evaluation.evaluator]: \u001b[0mInference done 1249/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0314 s/iter. Total: 0.1976 s/iter. ETA=0:02:31\n",
      "\u001b[32m[10/12 15:05:08 d2.evaluation.evaluator]: \u001b[0mInference done 1276/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1973 s/iter. ETA=0:02:26\n",
      "\u001b[32m[10/12 15:05:13 d2.evaluation.evaluator]: \u001b[0mInference done 1302/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:02:20\n",
      "\u001b[32m[10/12 15:05:18 d2.evaluation.evaluator]: \u001b[0mInference done 1327/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1974 s/iter. ETA=0:02:15\n",
      "\u001b[32m[10/12 15:05:23 d2.evaluation.evaluator]: \u001b[0mInference done 1353/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:02:10\n",
      "\u001b[32m[10/12 15:05:29 d2.evaluation.evaluator]: \u001b[0mInference done 1379/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:02:05\n",
      "\u001b[32m[10/12 15:05:34 d2.evaluation.evaluator]: \u001b[0mInference done 1406/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0313 s/iter. Total: 0.1975 s/iter. ETA=0:02:00\n",
      "\u001b[32m[10/12 15:05:39 d2.evaluation.evaluator]: \u001b[0mInference done 1433/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1974 s/iter. ETA=0:01:55\n",
      "\u001b[32m[10/12 15:05:44 d2.evaluation.evaluator]: \u001b[0mInference done 1458/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0313 s/iter. Total: 0.1974 s/iter. ETA=0:01:50\n",
      "\u001b[32m[10/12 15:05:50 d2.evaluation.evaluator]: \u001b[0mInference done 1485/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1973 s/iter. ETA=0:01:44\n",
      "\u001b[32m[10/12 15:05:55 d2.evaluation.evaluator]: \u001b[0mInference done 1511/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:01:39\n",
      "\u001b[32m[10/12 15:06:00 d2.evaluation.evaluator]: \u001b[0mInference done 1537/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1972 s/iter. ETA=0:01:34\n",
      "\u001b[32m[10/12 15:06:05 d2.evaluation.evaluator]: \u001b[0mInference done 1561/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0313 s/iter. Total: 0.1975 s/iter. ETA=0:01:29\n",
      "\u001b[32m[10/12 15:06:10 d2.evaluation.evaluator]: \u001b[0mInference done 1587/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1974 s/iter. ETA=0:01:24\n",
      "\u001b[32m[10/12 15:06:15 d2.evaluation.evaluator]: \u001b[0mInference done 1613/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1974 s/iter. ETA=0:01:19\n",
      "\u001b[32m[10/12 15:06:20 d2.evaluation.evaluator]: \u001b[0mInference done 1639/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1973 s/iter. ETA=0:01:14\n",
      "\u001b[32m[10/12 15:06:25 d2.evaluation.evaluator]: \u001b[0mInference done 1664/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1974 s/iter. ETA=0:01:09\n",
      "\u001b[32m[10/12 15:06:30 d2.evaluation.evaluator]: \u001b[0mInference done 1691/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:01:04\n",
      "\u001b[32m[10/12 15:06:35 d2.evaluation.evaluator]: \u001b[0mInference done 1717/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1972 s/iter. ETA=0:00:58\n",
      "\u001b[32m[10/12 15:06:40 d2.evaluation.evaluator]: \u001b[0mInference done 1743/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0310 s/iter. Total: 0.1972 s/iter. ETA=0:00:53\n",
      "\u001b[32m[10/12 15:06:45 d2.evaluation.evaluator]: \u001b[0mInference done 1768/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1972 s/iter. ETA=0:00:48\n",
      "\u001b[32m[10/12 15:06:50 d2.evaluation.evaluator]: \u001b[0mInference done 1794/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:00:43\n",
      "\u001b[32m[10/12 15:06:55 d2.evaluation.evaluator]: \u001b[0mInference done 1821/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0310 s/iter. Total: 0.1971 s/iter. ETA=0:00:38\n",
      "\u001b[32m[10/12 15:07:01 d2.evaluation.evaluator]: \u001b[0mInference done 1846/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0310 s/iter. Total: 0.1972 s/iter. ETA=0:00:33\n",
      "\u001b[32m[10/12 15:07:06 d2.evaluation.evaluator]: \u001b[0mInference done 1873/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0309 s/iter. Total: 0.1971 s/iter. ETA=0:00:28\n",
      "\u001b[32m[10/12 15:07:11 d2.evaluation.evaluator]: \u001b[0mInference done 1899/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0309 s/iter. Total: 0.1971 s/iter. ETA=0:00:23\n",
      "\u001b[32m[10/12 15:07:16 d2.evaluation.evaluator]: \u001b[0mInference done 1923/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1973 s/iter. ETA=0:00:18\n",
      "\u001b[32m[10/12 15:07:21 d2.evaluation.evaluator]: \u001b[0mInference done 1948/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0312 s/iter. Total: 0.1973 s/iter. ETA=0:00:13\n",
      "\u001b[32m[10/12 15:07:26 d2.evaluation.evaluator]: \u001b[0mInference done 1975/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0311 s/iter. Total: 0.1972 s/iter. ETA=0:00:08\n",
      "\u001b[32m[10/12 15:07:31 d2.evaluation.evaluator]: \u001b[0mInference done 2002/2016. Dataloading: 0.0025 s/iter. Inference: 0.1634 s/iter. Eval: 0.0310 s/iter. Total: 0.1971 s/iter. ETA=0:00:02\n",
      "\u001b[32m[10/12 15:07:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:36.269487 (0.197051 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 15:07:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:28 (0.163409 s / iter per device, on 1 devices)\n",
      "miou = 75.24042511579368\n",
      "OA = 88.73295954295567\n",
      "Kappa = 85.33521153104068\n",
      "F1_score = 71.81222077640007\n",
      "\u001b[32m[10/12 15:07:35 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 75.24042511579368, 'fwIoU': 80.5414601480406, 'IoU-Background': 41.26307035392917, 'IoU-Surfaces': 84.20888531091266, 'IoU-Building': 91.52039561014614, 'IoU-Low vegetation': 74.91984512345091, 'IoU-tree': 75.6419300138653, 'IoU-Car': 83.88842428245783, 'mACC': 84.5387246405734, 'pACC': 88.73295954295567, 'ACC-Background': 59.71215801627376, 'ACC-Surfaces': 91.53094565011727, 'ACC-Building': 95.39693236703215, 'ACC-Low vegetation': 87.3077078841347, 'ACC-tree': 83.82405063110994, 'ACC-Car': 89.46055329477257})])\n",
      "\u001b[32m[10/12 15:07:35 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 15:07:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 15:07:35 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 15:07:35 d2.evaluation.testing]: \u001b[0mcopypaste: 75.2404,80.5415,84.5387,88.7330\n",
      "\u001b[32m[10/12 15:07:35 d2.utils.events]: \u001b[0m eta: 21:57:39  iter: 7999  total_loss: 21.29  loss_ce: 0.1584  loss_cate: 0.1741  loss_mask: 0.8698  loss_dice: 1.022  loss_ce_0: 0.3219  loss_cate_0: 0  loss_mask_0: 0.9113  loss_dice_0: 1.058  loss_ce_1: 0.2023  loss_cate_1: 0  loss_mask_1: 0.8957  loss_dice_1: 1.023  loss_ce_2: 0.1528  loss_cate_2: 0  loss_mask_2: 0.8586  loss_dice_2: 1.007  loss_ce_3: 0.1446  loss_cate_3: 0  loss_mask_3: 0.8745  loss_dice_3: 1.019  loss_ce_4: 0.1289  loss_cate_4: 0  loss_mask_4: 0.8694  loss_dice_4: 1.025  loss_ce_5: 0.1807  loss_cate_5: 0  loss_mask_5: 0.8728  loss_dice_5: 0.9942  loss_ce_6: 0.1563  loss_cate_6: 0  loss_mask_6: 0.8729  loss_dice_6: 0.9893  loss_ce_7: 0.1968  loss_cate_7: 0  loss_mask_7: 0.8583  loss_dice_7: 0.9968  loss_ce_8: 0.2201  loss_cate_8: 0  loss_mask_8: 0.8681  loss_dice_8: 0.9972  time: 1.1031  data_time: 0.0125  lr: 9.0954e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:07:57 d2.utils.events]: \u001b[0m eta: 21:57:17  iter: 8019  total_loss: 23.96  loss_ce: 0.2907  loss_cate: 0.1588  loss_mask: 0.9067  loss_dice: 1.115  loss_ce_0: 0.3522  loss_cate_0: 0  loss_mask_0: 0.9108  loss_dice_0: 1.173  loss_ce_1: 0.2742  loss_cate_1: 0  loss_mask_1: 0.8836  loss_dice_1: 1.123  loss_ce_2: 0.2761  loss_cate_2: 0  loss_mask_2: 0.8625  loss_dice_2: 1.062  loss_ce_3: 0.2688  loss_cate_3: 0  loss_mask_3: 0.8567  loss_dice_3: 1.097  loss_ce_4: 0.2727  loss_cate_4: 0  loss_mask_4: 0.8462  loss_dice_4: 1.088  loss_ce_5: 0.2948  loss_cate_5: 0  loss_mask_5: 0.87  loss_dice_5: 1.098  loss_ce_6: 0.3043  loss_cate_6: 0  loss_mask_6: 0.8505  loss_dice_6: 1.105  loss_ce_7: 0.3053  loss_cate_7: 0  loss_mask_7: 0.8544  loss_dice_7: 1.1  loss_ce_8: 0.2704  loss_cate_8: 0  loss_mask_8: 0.8823  loss_dice_8: 1.087  time: 1.1031  data_time: 0.0130  lr: 9.0932e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:08:19 d2.utils.events]: \u001b[0m eta: 21:56:55  iter: 8039  total_loss: 20.97  loss_ce: 0.2877  loss_cate: 0.1454  loss_mask: 0.7376  loss_dice: 0.9553  loss_ce_0: 0.3536  loss_cate_0: 0  loss_mask_0: 0.7775  loss_dice_0: 1.023  loss_ce_1: 0.3312  loss_cate_1: 0  loss_mask_1: 0.7567  loss_dice_1: 0.9617  loss_ce_2: 0.277  loss_cate_2: 0  loss_mask_2: 0.755  loss_dice_2: 0.9647  loss_ce_3: 0.2902  loss_cate_3: 0  loss_mask_3: 0.7587  loss_dice_3: 0.9709  loss_ce_4: 0.3153  loss_cate_4: 0  loss_mask_4: 0.759  loss_dice_4: 0.9492  loss_ce_5: 0.3051  loss_cate_5: 0  loss_mask_5: 0.7676  loss_dice_5: 0.9503  loss_ce_6: 0.3039  loss_cate_6: 0  loss_mask_6: 0.7599  loss_dice_6: 0.9587  loss_ce_7: 0.2695  loss_cate_7: 0  loss_mask_7: 0.7481  loss_dice_7: 0.9626  loss_ce_8: 0.2902  loss_cate_8: 0  loss_mask_8: 0.7455  loss_dice_8: 0.9736  time: 1.1031  data_time: 0.0135  lr: 9.0909e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:08:42 d2.utils.events]: \u001b[0m eta: 21:56:22  iter: 8059  total_loss: 22.13  loss_ce: 0.252  loss_cate: 0.1526  loss_mask: 0.8968  loss_dice: 1.007  loss_ce_0: 0.4353  loss_cate_0: 0  loss_mask_0: 0.8356  loss_dice_0: 1.032  loss_ce_1: 0.2947  loss_cate_1: 0  loss_mask_1: 0.8973  loss_dice_1: 1.035  loss_ce_2: 0.2829  loss_cate_2: 0  loss_mask_2: 0.8048  loss_dice_2: 1.005  loss_ce_3: 0.1891  loss_cate_3: 0  loss_mask_3: 0.8236  loss_dice_3: 1.035  loss_ce_4: 0.2863  loss_cate_4: 0  loss_mask_4: 0.8274  loss_dice_4: 1.038  loss_ce_5: 0.1961  loss_cate_5: 0  loss_mask_5: 0.8096  loss_dice_5: 1.012  loss_ce_6: 0.2779  loss_cate_6: 0  loss_mask_6: 0.8032  loss_dice_6: 0.9578  loss_ce_7: 0.2014  loss_cate_7: 0  loss_mask_7: 0.8036  loss_dice_7: 0.9717  loss_ce_8: 0.189  loss_cate_8: 0  loss_mask_8: 0.8402  loss_dice_8: 1  time: 1.1030  data_time: 0.0132  lr: 9.0886e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:09:04 d2.utils.events]: \u001b[0m eta: 21:55:57  iter: 8079  total_loss: 21.6  loss_ce: 0.2404  loss_cate: 0.1475  loss_mask: 0.8252  loss_dice: 0.9541  loss_ce_0: 0.3103  loss_cate_0: 0  loss_mask_0: 0.8417  loss_dice_0: 1.018  loss_ce_1: 0.2195  loss_cate_1: 0  loss_mask_1: 0.8167  loss_dice_1: 0.9973  loss_ce_2: 0.2336  loss_cate_2: 0  loss_mask_2: 0.7822  loss_dice_2: 0.9479  loss_ce_3: 0.1734  loss_cate_3: 0  loss_mask_3: 0.8121  loss_dice_3: 0.9853  loss_ce_4: 0.1719  loss_cate_4: 0  loss_mask_4: 0.8037  loss_dice_4: 1.01  loss_ce_5: 0.2428  loss_cate_5: 0  loss_mask_5: 0.8028  loss_dice_5: 0.994  loss_ce_6: 0.2008  loss_cate_6: 0  loss_mask_6: 0.8085  loss_dice_6: 0.947  loss_ce_7: 0.2446  loss_cate_7: 0  loss_mask_7: 0.82  loss_dice_7: 0.9606  loss_ce_8: 0.2442  loss_cate_8: 0  loss_mask_8: 0.8051  loss_dice_8: 0.9732  time: 1.1030  data_time: 0.0138  lr: 9.0863e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:09:26 d2.utils.events]: \u001b[0m eta: 21:55:33  iter: 8099  total_loss: 21.69  loss_ce: 0.2603  loss_cate: 0.1522  loss_mask: 0.8499  loss_dice: 0.9746  loss_ce_0: 0.3352  loss_cate_0: 0  loss_mask_0: 0.8587  loss_dice_0: 0.9629  loss_ce_1: 0.2536  loss_cate_1: 0  loss_mask_1: 0.8442  loss_dice_1: 0.9385  loss_ce_2: 0.257  loss_cate_2: 0  loss_mask_2: 0.8467  loss_dice_2: 0.9533  loss_ce_3: 0.2461  loss_cate_3: 0  loss_mask_3: 0.8586  loss_dice_3: 0.9216  loss_ce_4: 0.2036  loss_cate_4: 0  loss_mask_4: 0.8527  loss_dice_4: 0.9464  loss_ce_5: 0.2498  loss_cate_5: 0  loss_mask_5: 0.8505  loss_dice_5: 0.9421  loss_ce_6: 0.2031  loss_cate_6: 0  loss_mask_6: 0.8318  loss_dice_6: 0.9676  loss_ce_7: 0.2603  loss_cate_7: 0  loss_mask_7: 0.8454  loss_dice_7: 0.9495  loss_ce_8: 0.2596  loss_cate_8: 0  loss_mask_8: 0.8662  loss_dice_8: 0.9368  time: 1.1030  data_time: 0.0129  lr: 9.0841e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:09:49 d2.utils.events]: \u001b[0m eta: 21:55:11  iter: 8119  total_loss: 21.54  loss_ce: 0.2368  loss_cate: 0.1598  loss_mask: 0.8082  loss_dice: 0.9435  loss_ce_0: 0.4411  loss_cate_0: 0  loss_mask_0: 0.8263  loss_dice_0: 1.019  loss_ce_1: 0.2923  loss_cate_1: 0  loss_mask_1: 0.821  loss_dice_1: 0.9342  loss_ce_2: 0.2507  loss_cate_2: 0  loss_mask_2: 0.8115  loss_dice_2: 0.9576  loss_ce_3: 0.2476  loss_cate_3: 0  loss_mask_3: 0.8222  loss_dice_3: 0.9586  loss_ce_4: 0.2556  loss_cate_4: 0  loss_mask_4: 0.8256  loss_dice_4: 0.9298  loss_ce_5: 0.2488  loss_cate_5: 0  loss_mask_5: 0.785  loss_dice_5: 0.9138  loss_ce_6: 0.2647  loss_cate_6: 0  loss_mask_6: 0.7924  loss_dice_6: 0.9654  loss_ce_7: 0.2621  loss_cate_7: 0  loss_mask_7: 0.7974  loss_dice_7: 0.8991  loss_ce_8: 0.2508  loss_cate_8: 0  loss_mask_8: 0.8001  loss_dice_8: 0.9803  time: 1.1030  data_time: 0.0128  lr: 9.0818e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:10:12 d2.utils.events]: \u001b[0m eta: 21:54:51  iter: 8139  total_loss: 21.48  loss_ce: 0.2454  loss_cate: 0.1457  loss_mask: 0.7221  loss_dice: 0.9091  loss_ce_0: 0.3353  loss_cate_0: 0  loss_mask_0: 0.8245  loss_dice_0: 1.021  loss_ce_1: 0.2127  loss_cate_1: 0  loss_mask_1: 0.8189  loss_dice_1: 0.9596  loss_ce_2: 0.2328  loss_cate_2: 0  loss_mask_2: 0.7871  loss_dice_2: 0.929  loss_ce_3: 0.2661  loss_cate_3: 0  loss_mask_3: 0.7859  loss_dice_3: 0.8875  loss_ce_4: 0.2606  loss_cate_4: 0  loss_mask_4: 0.7787  loss_dice_4: 0.8939  loss_ce_5: 0.2586  loss_cate_5: 0  loss_mask_5: 0.7496  loss_dice_5: 0.8976  loss_ce_6: 0.2405  loss_cate_6: 0  loss_mask_6: 0.7748  loss_dice_6: 0.9026  loss_ce_7: 0.2477  loss_cate_7: 0  loss_mask_7: 0.7293  loss_dice_7: 0.8899  loss_ce_8: 0.2269  loss_cate_8: 0  loss_mask_8: 0.7207  loss_dice_8: 0.9223  time: 1.1030  data_time: 0.0143  lr: 9.0795e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:10:34 d2.utils.events]: \u001b[0m eta: 21:54:04  iter: 8159  total_loss: 20.85  loss_ce: 0.2045  loss_cate: 0.1615  loss_mask: 0.7498  loss_dice: 0.9567  loss_ce_0: 0.3663  loss_cate_0: 0  loss_mask_0: 0.7924  loss_dice_0: 1.015  loss_ce_1: 0.2455  loss_cate_1: 0  loss_mask_1: 0.7626  loss_dice_1: 0.8887  loss_ce_2: 0.249  loss_cate_2: 0  loss_mask_2: 0.7448  loss_dice_2: 0.8912  loss_ce_3: 0.2242  loss_cate_3: 0  loss_mask_3: 0.7574  loss_dice_3: 0.9999  loss_ce_4: 0.2085  loss_cate_4: 0  loss_mask_4: 0.7559  loss_dice_4: 0.9894  loss_ce_5: 0.2529  loss_cate_5: 0  loss_mask_5: 0.7468  loss_dice_5: 0.9522  loss_ce_6: 0.2  loss_cate_6: 0  loss_mask_6: 0.7547  loss_dice_6: 0.9693  loss_ce_7: 0.2167  loss_cate_7: 0  loss_mask_7: 0.7498  loss_dice_7: 0.9712  loss_ce_8: 0.2335  loss_cate_8: 0  loss_mask_8: 0.7448  loss_dice_8: 0.9791  time: 1.1030  data_time: 0.0123  lr: 9.0772e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:10:57 d2.utils.events]: \u001b[0m eta: 21:53:42  iter: 8179  total_loss: 22.49  loss_ce: 0.23  loss_cate: 0.1599  loss_mask: 0.8809  loss_dice: 1.059  loss_ce_0: 0.3693  loss_cate_0: 0  loss_mask_0: 0.8539  loss_dice_0: 1.044  loss_ce_1: 0.2909  loss_cate_1: 0  loss_mask_1: 0.8578  loss_dice_1: 1.09  loss_ce_2: 0.3033  loss_cate_2: 0  loss_mask_2: 0.8425  loss_dice_2: 1.061  loss_ce_3: 0.3228  loss_cate_3: 0  loss_mask_3: 0.8395  loss_dice_3: 1.083  loss_ce_4: 0.2718  loss_cate_4: 0  loss_mask_4: 0.8353  loss_dice_4: 1.086  loss_ce_5: 0.2541  loss_cate_5: 0  loss_mask_5: 0.8451  loss_dice_5: 1.063  loss_ce_6: 0.2578  loss_cate_6: 0  loss_mask_6: 0.869  loss_dice_6: 1.068  loss_ce_7: 0.2551  loss_cate_7: 0  loss_mask_7: 0.8633  loss_dice_7: 1.112  loss_ce_8: 0.2238  loss_cate_8: 0  loss_mask_8: 0.8778  loss_dice_8: 1.059  time: 1.1030  data_time: 0.0136  lr: 9.075e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:11:19 d2.utils.events]: \u001b[0m eta: 21:53:23  iter: 8199  total_loss: 22.02  loss_ce: 0.2361  loss_cate: 0.1893  loss_mask: 0.854  loss_dice: 1.019  loss_ce_0: 0.3248  loss_cate_0: 0  loss_mask_0: 0.9347  loss_dice_0: 1.046  loss_ce_1: 0.2191  loss_cate_1: 0  loss_mask_1: 0.8673  loss_dice_1: 1.05  loss_ce_2: 0.2279  loss_cate_2: 0  loss_mask_2: 0.8564  loss_dice_2: 1.02  loss_ce_3: 0.2132  loss_cate_3: 0  loss_mask_3: 0.8456  loss_dice_3: 1.034  loss_ce_4: 0.2229  loss_cate_4: 0  loss_mask_4: 0.8486  loss_dice_4: 1.027  loss_ce_5: 0.2353  loss_cate_5: 0  loss_mask_5: 0.8587  loss_dice_5: 1.062  loss_ce_6: 0.227  loss_cate_6: 0  loss_mask_6: 0.8475  loss_dice_6: 1.034  loss_ce_7: 0.2145  loss_cate_7: 0  loss_mask_7: 0.8718  loss_dice_7: 1.039  loss_ce_8: 0.2097  loss_cate_8: 0  loss_mask_8: 0.8607  loss_dice_8: 1.033  time: 1.1030  data_time: 0.0140  lr: 9.0727e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:11:42 d2.utils.events]: \u001b[0m eta: 21:53:12  iter: 8219  total_loss: 21.9  loss_ce: 0.2561  loss_cate: 0.1443  loss_mask: 0.8264  loss_dice: 1.054  loss_ce_0: 0.3234  loss_cate_0: 0  loss_mask_0: 0.8447  loss_dice_0: 1.123  loss_ce_1: 0.3345  loss_cate_1: 0  loss_mask_1: 0.8508  loss_dice_1: 1.108  loss_ce_2: 0.2384  loss_cate_2: 0  loss_mask_2: 0.8359  loss_dice_2: 1.042  loss_ce_3: 0.2451  loss_cate_3: 0  loss_mask_3: 0.8352  loss_dice_3: 1.042  loss_ce_4: 0.247  loss_cate_4: 0  loss_mask_4: 0.8374  loss_dice_4: 1.03  loss_ce_5: 0.2561  loss_cate_5: 0  loss_mask_5: 0.8612  loss_dice_5: 1.031  loss_ce_6: 0.2546  loss_cate_6: 0  loss_mask_6: 0.8212  loss_dice_6: 1.027  loss_ce_7: 0.2655  loss_cate_7: 0  loss_mask_7: 0.8572  loss_dice_7: 1.02  loss_ce_8: 0.2266  loss_cate_8: 0  loss_mask_8: 0.8429  loss_dice_8: 1.047  time: 1.1030  data_time: 0.0135  lr: 9.0704e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:12:04 d2.utils.events]: \u001b[0m eta: 21:53:00  iter: 8239  total_loss: 20.46  loss_ce: 0.1131  loss_cate: 0.1565  loss_mask: 0.883  loss_dice: 1.054  loss_ce_0: 0.1963  loss_cate_0: 0  loss_mask_0: 0.9127  loss_dice_0: 1.059  loss_ce_1: 0.1444  loss_cate_1: 0  loss_mask_1: 0.8785  loss_dice_1: 1.07  loss_ce_2: 0.1268  loss_cate_2: 0  loss_mask_2: 0.9298  loss_dice_2: 1.076  loss_ce_3: 0.1352  loss_cate_3: 0  loss_mask_3: 0.8934  loss_dice_3: 1.032  loss_ce_4: 0.1222  loss_cate_4: 0  loss_mask_4: 0.889  loss_dice_4: 1.068  loss_ce_5: 0.174  loss_cate_5: 0  loss_mask_5: 0.8909  loss_dice_5: 1.059  loss_ce_6: 0.1601  loss_cate_6: 0  loss_mask_6: 0.8671  loss_dice_6: 1.015  loss_ce_7: 0.1212  loss_cate_7: 0  loss_mask_7: 0.879  loss_dice_7: 1.034  loss_ce_8: 0.1215  loss_cate_8: 0  loss_mask_8: 0.8707  loss_dice_8: 1.035  time: 1.1029  data_time: 0.0132  lr: 9.0681e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:12:27 d2.utils.events]: \u001b[0m eta: 21:52:29  iter: 8259  total_loss: 22.14  loss_ce: 0.1856  loss_cate: 0.1492  loss_mask: 0.7758  loss_dice: 0.9937  loss_ce_0: 0.2794  loss_cate_0: 0  loss_mask_0: 0.796  loss_dice_0: 1.054  loss_ce_1: 0.2337  loss_cate_1: 0  loss_mask_1: 0.762  loss_dice_1: 1.02  loss_ce_2: 0.2209  loss_cate_2: 0  loss_mask_2: 0.762  loss_dice_2: 1.041  loss_ce_3: 0.1928  loss_cate_3: 0  loss_mask_3: 0.7524  loss_dice_3: 1.027  loss_ce_4: 0.2192  loss_cate_4: 0  loss_mask_4: 0.7639  loss_dice_4: 1.022  loss_ce_5: 0.1877  loss_cate_5: 0  loss_mask_5: 0.7642  loss_dice_5: 1.028  loss_ce_6: 0.1661  loss_cate_6: 0  loss_mask_6: 0.7792  loss_dice_6: 1.025  loss_ce_7: 0.1961  loss_cate_7: 0  loss_mask_7: 0.7747  loss_dice_7: 1.003  loss_ce_8: 0.1853  loss_cate_8: 0  loss_mask_8: 0.7736  loss_dice_8: 1.01  time: 1.1029  data_time: 0.0130  lr: 9.0659e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:12:49 d2.utils.events]: \u001b[0m eta: 21:52:07  iter: 8279  total_loss: 22.14  loss_ce: 0.2295  loss_cate: 0.1595  loss_mask: 0.8685  loss_dice: 1.074  loss_ce_0: 0.3617  loss_cate_0: 0  loss_mask_0: 0.8342  loss_dice_0: 1.067  loss_ce_1: 0.2222  loss_cate_1: 0  loss_mask_1: 0.883  loss_dice_1: 1.041  loss_ce_2: 0.2545  loss_cate_2: 0  loss_mask_2: 0.875  loss_dice_2: 1.02  loss_ce_3: 0.2941  loss_cate_3: 0  loss_mask_3: 0.8638  loss_dice_3: 1.004  loss_ce_4: 0.2368  loss_cate_4: 0  loss_mask_4: 0.867  loss_dice_4: 1.039  loss_ce_5: 0.2718  loss_cate_5: 0  loss_mask_5: 0.8629  loss_dice_5: 1.01  loss_ce_6: 0.2712  loss_cate_6: 0  loss_mask_6: 0.9163  loss_dice_6: 1.007  loss_ce_7: 0.2606  loss_cate_7: 0  loss_mask_7: 0.8609  loss_dice_7: 1.006  loss_ce_8: 0.2304  loss_cate_8: 0  loss_mask_8: 0.8615  loss_dice_8: 1.043  time: 1.1029  data_time: 0.0139  lr: 9.0636e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:13:34 d2.utils.events]: \u001b[0m eta: 21:51:06  iter: 8319  total_loss: 23.48  loss_ce: 0.2067  loss_cate: 0.1803  loss_mask: 0.9235  loss_dice: 1.076  loss_ce_0: 0.3929  loss_cate_0: 0  loss_mask_0: 0.9548  loss_dice_0: 1.124  loss_ce_1: 0.3328  loss_cate_1: 0  loss_mask_1: 0.8896  loss_dice_1: 1.108  loss_ce_2: 0.2802  loss_cate_2: 0  loss_mask_2: 0.9015  loss_dice_2: 1.098  loss_ce_3: 0.2888  loss_cate_3: 0  loss_mask_3: 0.9233  loss_dice_3: 1.056  loss_ce_4: 0.2914  loss_cate_4: 0  loss_mask_4: 0.9487  loss_dice_4: 1.085  loss_ce_5: 0.2508  loss_cate_5: 0  loss_mask_5: 0.9572  loss_dice_5: 1.063  loss_ce_6: 0.2327  loss_cate_6: 0  loss_mask_6: 0.9708  loss_dice_6: 1.073  loss_ce_7: 0.2719  loss_cate_7: 0  loss_mask_7: 0.9287  loss_dice_7: 1.081  loss_ce_8: 0.2657  loss_cate_8: 0  loss_mask_8: 0.961  loss_dice_8: 1.07  time: 1.1029  data_time: 0.0131  lr: 9.0591e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:13:57 d2.utils.events]: \u001b[0m eta: 21:50:43  iter: 8339  total_loss: 22.86  loss_ce: 0.2128  loss_cate: 0.1687  loss_mask: 0.8432  loss_dice: 1.112  loss_ce_0: 0.4437  loss_cate_0: 0  loss_mask_0: 0.8725  loss_dice_0: 1.027  loss_ce_1: 0.2821  loss_cate_1: 0  loss_mask_1: 0.8426  loss_dice_1: 1.135  loss_ce_2: 0.3014  loss_cate_2: 0  loss_mask_2: 0.8497  loss_dice_2: 1.107  loss_ce_3: 0.2508  loss_cate_3: 0  loss_mask_3: 0.8529  loss_dice_3: 1.102  loss_ce_4: 0.2095  loss_cate_4: 0  loss_mask_4: 0.8491  loss_dice_4: 1.094  loss_ce_5: 0.2102  loss_cate_5: 0  loss_mask_5: 0.8476  loss_dice_5: 1.107  loss_ce_6: 0.2294  loss_cate_6: 0  loss_mask_6: 0.8442  loss_dice_6: 1.123  loss_ce_7: 0.2462  loss_cate_7: 0  loss_mask_7: 0.844  loss_dice_7: 1.092  loss_ce_8: 0.258  loss_cate_8: 0  loss_mask_8: 0.8526  loss_dice_8: 1.059  time: 1.1029  data_time: 0.0132  lr: 9.0568e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:14:19 d2.utils.events]: \u001b[0m eta: 21:50:20  iter: 8359  total_loss: 22.06  loss_ce: 0.2384  loss_cate: 0.1644  loss_mask: 0.8564  loss_dice: 1.071  loss_ce_0: 0.3474  loss_cate_0: 0  loss_mask_0: 0.8606  loss_dice_0: 1.062  loss_ce_1: 0.204  loss_cate_1: 0  loss_mask_1: 0.88  loss_dice_1: 1.06  loss_ce_2: 0.2563  loss_cate_2: 0  loss_mask_2: 0.8231  loss_dice_2: 1.045  loss_ce_3: 0.246  loss_cate_3: 0  loss_mask_3: 0.8555  loss_dice_3: 1.054  loss_ce_4: 0.211  loss_cate_4: 0  loss_mask_4: 0.8657  loss_dice_4: 1.079  loss_ce_5: 0.2427  loss_cate_5: 0  loss_mask_5: 0.8675  loss_dice_5: 1.044  loss_ce_6: 0.2203  loss_cate_6: 0  loss_mask_6: 0.8619  loss_dice_6: 1.062  loss_ce_7: 0.2181  loss_cate_7: 0  loss_mask_7: 0.8847  loss_dice_7: 1.095  loss_ce_8: 0.1892  loss_cate_8: 0  loss_mask_8: 0.852  loss_dice_8: 1.091  time: 1.1029  data_time: 0.0141  lr: 9.0545e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:14:43 d2.utils.events]: \u001b[0m eta: 21:50:03  iter: 8379  total_loss: 23.32  loss_ce: 0.2259  loss_cate: 0.1953  loss_mask: 0.8864  loss_dice: 1.092  loss_ce_0: 0.3471  loss_cate_0: 0  loss_mask_0: 0.9216  loss_dice_0: 1.121  loss_ce_1: 0.1967  loss_cate_1: 0  loss_mask_1: 0.8878  loss_dice_1: 1.188  loss_ce_2: 0.2611  loss_cate_2: 0  loss_mask_2: 0.8787  loss_dice_2: 1.132  loss_ce_3: 0.2243  loss_cate_3: 0  loss_mask_3: 0.8837  loss_dice_3: 1.101  loss_ce_4: 0.2217  loss_cate_4: 0  loss_mask_4: 0.8843  loss_dice_4: 1.114  loss_ce_5: 0.2512  loss_cate_5: 0  loss_mask_5: 0.9285  loss_dice_5: 1.092  loss_ce_6: 0.2125  loss_cate_6: 0  loss_mask_6: 0.935  loss_dice_6: 1.111  loss_ce_7: 0.2287  loss_cate_7: 0  loss_mask_7: 0.8587  loss_dice_7: 1.104  loss_ce_8: 0.1929  loss_cate_8: 0  loss_mask_8: 0.881  loss_dice_8: 1.078  time: 1.1029  data_time: 0.0140  lr: 9.0522e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:15:06 d2.utils.events]: \u001b[0m eta: 21:49:37  iter: 8399  total_loss: 22.76  loss_ce: 0.2567  loss_cate: 0.1603  loss_mask: 0.8045  loss_dice: 1.065  loss_ce_0: 0.4026  loss_cate_0: 0  loss_mask_0: 0.8081  loss_dice_0: 1.064  loss_ce_1: 0.3121  loss_cate_1: 0  loss_mask_1: 0.8199  loss_dice_1: 1.066  loss_ce_2: 0.2998  loss_cate_2: 0  loss_mask_2: 0.834  loss_dice_2: 1.029  loss_ce_3: 0.2914  loss_cate_3: 0  loss_mask_3: 0.8229  loss_dice_3: 1.077  loss_ce_4: 0.2828  loss_cate_4: 0  loss_mask_4: 0.8201  loss_dice_4: 1.075  loss_ce_5: 0.2742  loss_cate_5: 0  loss_mask_5: 0.831  loss_dice_5: 1.04  loss_ce_6: 0.2554  loss_cate_6: 0  loss_mask_6: 0.8405  loss_dice_6: 1.049  loss_ce_7: 0.2745  loss_cate_7: 0  loss_mask_7: 0.7968  loss_dice_7: 1.031  loss_ce_8: 0.2865  loss_cate_8: 0  loss_mask_8: 0.8091  loss_dice_8: 1.041  time: 1.1029  data_time: 0.0130  lr: 9.05e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:15:28 d2.utils.events]: \u001b[0m eta: 21:49:17  iter: 8419  total_loss: 22.75  loss_ce: 0.2713  loss_cate: 0.1649  loss_mask: 0.9362  loss_dice: 1.077  loss_ce_0: 0.3806  loss_cate_0: 0  loss_mask_0: 0.9382  loss_dice_0: 1.1  loss_ce_1: 0.2872  loss_cate_1: 0  loss_mask_1: 0.9162  loss_dice_1: 1.067  loss_ce_2: 0.229  loss_cate_2: 0  loss_mask_2: 0.9492  loss_dice_2: 1.052  loss_ce_3: 0.28  loss_cate_3: 0  loss_mask_3: 0.9053  loss_dice_3: 1.078  loss_ce_4: 0.2657  loss_cate_4: 0  loss_mask_4: 0.9341  loss_dice_4: 1.075  loss_ce_5: 0.2452  loss_cate_5: 0  loss_mask_5: 0.9119  loss_dice_5: 1.054  loss_ce_6: 0.3126  loss_cate_6: 0  loss_mask_6: 0.8962  loss_dice_6: 1.078  loss_ce_7: 0.2642  loss_cate_7: 0  loss_mask_7: 0.8968  loss_dice_7: 1.075  loss_ce_8: 0.2434  loss_cate_8: 0  loss_mask_8: 0.9133  loss_dice_8: 1.09  time: 1.1029  data_time: 0.0126  lr: 9.0477e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:15:50 d2.utils.events]: \u001b[0m eta: 21:49:19  iter: 8439  total_loss: 21.78  loss_ce: 0.1492  loss_cate: 0.1651  loss_mask: 0.9127  loss_dice: 0.9675  loss_ce_0: 0.3509  loss_cate_0: 0  loss_mask_0: 0.9395  loss_dice_0: 0.988  loss_ce_1: 0.2418  loss_cate_1: 0  loss_mask_1: 0.9531  loss_dice_1: 0.9434  loss_ce_2: 0.2061  loss_cate_2: 0  loss_mask_2: 0.948  loss_dice_2: 0.8954  loss_ce_3: 0.211  loss_cate_3: 0  loss_mask_3: 0.938  loss_dice_3: 0.89  loss_ce_4: 0.2572  loss_cate_4: 0  loss_mask_4: 0.9076  loss_dice_4: 0.9211  loss_ce_5: 0.198  loss_cate_5: 0  loss_mask_5: 0.887  loss_dice_5: 0.9216  loss_ce_6: 0.204  loss_cate_6: 0  loss_mask_6: 0.9227  loss_dice_6: 0.9587  loss_ce_7: 0.1734  loss_cate_7: 0  loss_mask_7: 0.9052  loss_dice_7: 0.9475  loss_ce_8: 0.1572  loss_cate_8: 0  loss_mask_8: 0.9262  loss_dice_8: 0.9512  time: 1.1029  data_time: 0.0135  lr: 9.0454e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:16:13 d2.utils.events]: \u001b[0m eta: 21:48:30  iter: 8459  total_loss: 21.84  loss_ce: 0.2113  loss_cate: 0.1573  loss_mask: 0.7883  loss_dice: 1.067  loss_ce_0: 0.3327  loss_cate_0: 0  loss_mask_0: 0.8638  loss_dice_0: 1.055  loss_ce_1: 0.2673  loss_cate_1: 0  loss_mask_1: 0.8181  loss_dice_1: 1.136  loss_ce_2: 0.2501  loss_cate_2: 0  loss_mask_2: 0.7969  loss_dice_2: 1.061  loss_ce_3: 0.2503  loss_cate_3: 0  loss_mask_3: 0.7921  loss_dice_3: 1.084  loss_ce_4: 0.2683  loss_cate_4: 0  loss_mask_4: 0.803  loss_dice_4: 1.055  loss_ce_5: 0.2009  loss_cate_5: 0  loss_mask_5: 0.7655  loss_dice_5: 1.124  loss_ce_6: 0.2382  loss_cate_6: 0  loss_mask_6: 0.7818  loss_dice_6: 1.091  loss_ce_7: 0.229  loss_cate_7: 0  loss_mask_7: 0.7928  loss_dice_7: 1.069  loss_ce_8: 0.2369  loss_cate_8: 0  loss_mask_8: 0.7819  loss_dice_8: 1.068  time: 1.1028  data_time: 0.0136  lr: 9.0431e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:16:35 d2.utils.events]: \u001b[0m eta: 21:48:10  iter: 8479  total_loss: 21.71  loss_ce: 0.27  loss_cate: 0.1414  loss_mask: 0.8562  loss_dice: 1.051  loss_ce_0: 0.3845  loss_cate_0: 0  loss_mask_0: 0.8663  loss_dice_0: 1.088  loss_ce_1: 0.2683  loss_cate_1: 0  loss_mask_1: 0.877  loss_dice_1: 1.043  loss_ce_2: 0.2816  loss_cate_2: 0  loss_mask_2: 0.8791  loss_dice_2: 1.029  loss_ce_3: 0.2681  loss_cate_3: 0  loss_mask_3: 0.8872  loss_dice_3: 1.047  loss_ce_4: 0.2293  loss_cate_4: 0  loss_mask_4: 0.8663  loss_dice_4: 1.047  loss_ce_5: 0.2417  loss_cate_5: 0  loss_mask_5: 0.862  loss_dice_5: 1.079  loss_ce_6: 0.258  loss_cate_6: 0  loss_mask_6: 0.8449  loss_dice_6: 1.037  loss_ce_7: 0.2596  loss_cate_7: 0  loss_mask_7: 0.8432  loss_dice_7: 1.066  loss_ce_8: 0.2654  loss_cate_8: 0  loss_mask_8: 0.8523  loss_dice_8: 1.061  time: 1.1028  data_time: 0.0125  lr: 9.0408e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:16:57 d2.utils.events]: \u001b[0m eta: 21:47:46  iter: 8499  total_loss: 21.92  loss_ce: 0.1647  loss_cate: 0.1643  loss_mask: 0.8376  loss_dice: 1.03  loss_ce_0: 0.3384  loss_cate_0: 0  loss_mask_0: 0.861  loss_dice_0: 1.077  loss_ce_1: 0.1807  loss_cate_1: 0  loss_mask_1: 0.8609  loss_dice_1: 1.117  loss_ce_2: 0.1972  loss_cate_2: 0  loss_mask_2: 0.8414  loss_dice_2: 1.025  loss_ce_3: 0.1935  loss_cate_3: 0  loss_mask_3: 0.8629  loss_dice_3: 1.039  loss_ce_4: 0.1897  loss_cate_4: 0  loss_mask_4: 0.8076  loss_dice_4: 1.042  loss_ce_5: 0.2083  loss_cate_5: 0  loss_mask_5: 0.8426  loss_dice_5: 1.059  loss_ce_6: 0.1534  loss_cate_6: 0  loss_mask_6: 0.8471  loss_dice_6: 1.022  loss_ce_7: 0.1816  loss_cate_7: 0  loss_mask_7: 0.8316  loss_dice_7: 0.9992  loss_ce_8: 0.1672  loss_cate_8: 0  loss_mask_8: 0.8224  loss_dice_8: 1.047  time: 1.1028  data_time: 0.0131  lr: 9.0386e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:17:20 d2.utils.events]: \u001b[0m eta: 21:47:26  iter: 8519  total_loss: 22.36  loss_ce: 0.2597  loss_cate: 0.1666  loss_mask: 0.831  loss_dice: 0.9999  loss_ce_0: 0.4222  loss_cate_0: 0  loss_mask_0: 0.8155  loss_dice_0: 1.087  loss_ce_1: 0.3921  loss_cate_1: 0  loss_mask_1: 0.8358  loss_dice_1: 1.067  loss_ce_2: 0.3743  loss_cate_2: 0  loss_mask_2: 0.8181  loss_dice_2: 0.98  loss_ce_3: 0.3344  loss_cate_3: 0  loss_mask_3: 0.7934  loss_dice_3: 0.9633  loss_ce_4: 0.3163  loss_cate_4: 0  loss_mask_4: 0.8151  loss_dice_4: 0.9928  loss_ce_5: 0.2878  loss_cate_5: 0  loss_mask_5: 0.819  loss_dice_5: 1.018  loss_ce_6: 0.2842  loss_cate_6: 0  loss_mask_6: 0.8225  loss_dice_6: 0.996  loss_ce_7: 0.3181  loss_cate_7: 0  loss_mask_7: 0.835  loss_dice_7: 1.022  loss_ce_8: 0.2955  loss_cate_8: 0  loss_mask_8: 0.8101  loss_dice_8: 1.021  time: 1.1028  data_time: 0.0136  lr: 9.0363e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:17:42 d2.utils.events]: \u001b[0m eta: 21:47:10  iter: 8539  total_loss: 21.18  loss_ce: 0.2248  loss_cate: 0.1625  loss_mask: 0.8218  loss_dice: 0.9592  loss_ce_0: 0.3386  loss_cate_0: 0  loss_mask_0: 0.8315  loss_dice_0: 0.9587  loss_ce_1: 0.3048  loss_cate_1: 0  loss_mask_1: 0.8237  loss_dice_1: 0.9674  loss_ce_2: 0.2393  loss_cate_2: 0  loss_mask_2: 0.8215  loss_dice_2: 0.9372  loss_ce_3: 0.2349  loss_cate_3: 0  loss_mask_3: 0.8071  loss_dice_3: 0.9056  loss_ce_4: 0.2412  loss_cate_4: 0  loss_mask_4: 0.8067  loss_dice_4: 0.9304  loss_ce_5: 0.2232  loss_cate_5: 0  loss_mask_5: 0.809  loss_dice_5: 0.9458  loss_ce_6: 0.2295  loss_cate_6: 0  loss_mask_6: 0.817  loss_dice_6: 0.9524  loss_ce_7: 0.2434  loss_cate_7: 0  loss_mask_7: 0.7966  loss_dice_7: 0.9695  loss_ce_8: 0.2025  loss_cate_8: 0  loss_mask_8: 0.8181  loss_dice_8: 0.9709  time: 1.1028  data_time: 0.0134  lr: 9.034e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:18:05 d2.utils.events]: \u001b[0m eta: 21:46:48  iter: 8559  total_loss: 21.31  loss_ce: 0.3007  loss_cate: 0.142  loss_mask: 0.8923  loss_dice: 0.9953  loss_ce_0: 0.4093  loss_cate_0: 0  loss_mask_0: 0.87  loss_dice_0: 0.9532  loss_ce_1: 0.2494  loss_cate_1: 0  loss_mask_1: 0.926  loss_dice_1: 0.9928  loss_ce_2: 0.1942  loss_cate_2: 0  loss_mask_2: 0.9296  loss_dice_2: 0.9525  loss_ce_3: 0.3314  loss_cate_3: 0  loss_mask_3: 0.9098  loss_dice_3: 0.9616  loss_ce_4: 0.2524  loss_cate_4: 0  loss_mask_4: 0.9053  loss_dice_4: 0.9985  loss_ce_5: 0.2668  loss_cate_5: 0  loss_mask_5: 0.926  loss_dice_5: 1  loss_ce_6: 0.3147  loss_cate_6: 0  loss_mask_6: 0.8935  loss_dice_6: 0.9493  loss_ce_7: 0.3055  loss_cate_7: 0  loss_mask_7: 0.899  loss_dice_7: 0.9572  loss_ce_8: 0.2944  loss_cate_8: 0  loss_mask_8: 0.8978  loss_dice_8: 0.9823  time: 1.1028  data_time: 0.0132  lr: 9.0317e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:18:27 d2.utils.events]: \u001b[0m eta: 21:46:21  iter: 8579  total_loss: 21.26  loss_ce: 0.2468  loss_cate: 0.1501  loss_mask: 0.7961  loss_dice: 1.129  loss_ce_0: 0.4099  loss_cate_0: 0  loss_mask_0: 0.8126  loss_dice_0: 1.097  loss_ce_1: 0.2566  loss_cate_1: 0  loss_mask_1: 0.8953  loss_dice_1: 1.13  loss_ce_2: 0.2533  loss_cate_2: 0  loss_mask_2: 0.8716  loss_dice_2: 1.115  loss_ce_3: 0.2568  loss_cate_3: 0  loss_mask_3: 0.849  loss_dice_3: 1.092  loss_ce_4: 0.2148  loss_cate_4: 0  loss_mask_4: 0.8803  loss_dice_4: 1.1  loss_ce_5: 0.2278  loss_cate_5: 0  loss_mask_5: 0.8392  loss_dice_5: 1.087  loss_ce_6: 0.271  loss_cate_6: 0  loss_mask_6: 0.8055  loss_dice_6: 1.095  loss_ce_7: 0.2788  loss_cate_7: 0  loss_mask_7: 0.7981  loss_dice_7: 1.088  loss_ce_8: 0.2243  loss_cate_8: 0  loss_mask_8: 0.8286  loss_dice_8: 1.112  time: 1.1028  data_time: 0.0129  lr: 9.0295e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:18:49 d2.utils.events]: \u001b[0m eta: 21:46:01  iter: 8599  total_loss: 21.32  loss_ce: 0.1987  loss_cate: 0.15  loss_mask: 0.8744  loss_dice: 1.002  loss_ce_0: 0.33  loss_cate_0: 0  loss_mask_0: 0.8806  loss_dice_0: 1.093  loss_ce_1: 0.2759  loss_cate_1: 0  loss_mask_1: 0.8647  loss_dice_1: 1.042  loss_ce_2: 0.2291  loss_cate_2: 0  loss_mask_2: 0.862  loss_dice_2: 1.07  loss_ce_3: 0.2456  loss_cate_3: 0  loss_mask_3: 0.8582  loss_dice_3: 1.036  loss_ce_4: 0.2645  loss_cate_4: 0  loss_mask_4: 0.8673  loss_dice_4: 1.053  loss_ce_5: 0.2493  loss_cate_5: 0  loss_mask_5: 0.8665  loss_dice_5: 1.022  loss_ce_6: 0.2088  loss_cate_6: 0  loss_mask_6: 0.8596  loss_dice_6: 1.009  loss_ce_7: 0.2208  loss_cate_7: 0  loss_mask_7: 0.8655  loss_dice_7: 1.035  loss_ce_8: 0.1975  loss_cate_8: 0  loss_mask_8: 0.8594  loss_dice_8: 1.01  time: 1.1028  data_time: 0.0142  lr: 9.0272e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:19:12 d2.utils.events]: \u001b[0m eta: 21:46:00  iter: 8619  total_loss: 20.2  loss_ce: 0.08208  loss_cate: 0.1704  loss_mask: 0.8993  loss_dice: 0.9953  loss_ce_0: 0.2833  loss_cate_0: 0  loss_mask_0: 0.8818  loss_dice_0: 1.021  loss_ce_1: 0.1182  loss_cate_1: 0  loss_mask_1: 0.8931  loss_dice_1: 0.9911  loss_ce_2: 0.1252  loss_cate_2: 0  loss_mask_2: 0.9098  loss_dice_2: 0.9697  loss_ce_3: 0.1339  loss_cate_3: 0  loss_mask_3: 0.8915  loss_dice_3: 0.9578  loss_ce_4: 0.1194  loss_cate_4: 0  loss_mask_4: 0.8705  loss_dice_4: 0.95  loss_ce_5: 0.136  loss_cate_5: 0  loss_mask_5: 0.872  loss_dice_5: 0.9261  loss_ce_6: 0.1716  loss_cate_6: 0  loss_mask_6: 0.8441  loss_dice_6: 0.9483  loss_ce_7: 0.122  loss_cate_7: 0  loss_mask_7: 0.8703  loss_dice_7: 0.9256  loss_ce_8: 0.1258  loss_cate_8: 0  loss_mask_8: 0.8721  loss_dice_8: 0.9404  time: 1.1028  data_time: 0.0136  lr: 9.0249e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:19:34 d2.utils.events]: \u001b[0m eta: 21:45:25  iter: 8639  total_loss: 20.83  loss_ce: 0.3894  loss_cate: 0.1447  loss_mask: 0.8053  loss_dice: 0.9914  loss_ce_0: 0.3497  loss_cate_0: 0  loss_mask_0: 0.8261  loss_dice_0: 1.04  loss_ce_1: 0.3765  loss_cate_1: 0  loss_mask_1: 0.7553  loss_dice_1: 1.041  loss_ce_2: 0.2412  loss_cate_2: 0  loss_mask_2: 0.8152  loss_dice_2: 1.012  loss_ce_3: 0.292  loss_cate_3: 0  loss_mask_3: 0.7699  loss_dice_3: 1.003  loss_ce_4: 0.2403  loss_cate_4: 0  loss_mask_4: 0.8059  loss_dice_4: 1.03  loss_ce_5: 0.2459  loss_cate_5: 0  loss_mask_5: 0.7829  loss_dice_5: 1.027  loss_ce_6: 0.3533  loss_cate_6: 0  loss_mask_6: 0.8187  loss_dice_6: 1.005  loss_ce_7: 0.375  loss_cate_7: 0  loss_mask_7: 0.7944  loss_dice_7: 1  loss_ce_8: 0.3119  loss_cate_8: 0  loss_mask_8: 0.7869  loss_dice_8: 1.016  time: 1.1028  data_time: 0.0131  lr: 9.0226e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:19:57 d2.utils.events]: \u001b[0m eta: 21:44:57  iter: 8659  total_loss: 19.75  loss_ce: 0.2926  loss_cate: 0.1563  loss_mask: 0.6451  loss_dice: 0.884  loss_ce_0: 0.4733  loss_cate_0: 0  loss_mask_0: 0.7216  loss_dice_0: 0.9132  loss_ce_1: 0.3294  loss_cate_1: 0  loss_mask_1: 0.6518  loss_dice_1: 0.9288  loss_ce_2: 0.2779  loss_cate_2: 0  loss_mask_2: 0.6415  loss_dice_2: 0.8874  loss_ce_3: 0.3029  loss_cate_3: 0  loss_mask_3: 0.6378  loss_dice_3: 0.8525  loss_ce_4: 0.2518  loss_cate_4: 0  loss_mask_4: 0.6265  loss_dice_4: 0.8663  loss_ce_5: 0.294  loss_cate_5: 0  loss_mask_5: 0.6686  loss_dice_5: 0.8484  loss_ce_6: 0.2499  loss_cate_6: 0  loss_mask_6: 0.6296  loss_dice_6: 0.8483  loss_ce_7: 0.2272  loss_cate_7: 0  loss_mask_7: 0.6524  loss_dice_7: 0.8849  loss_ce_8: 0.3089  loss_cate_8: 0  loss_mask_8: 0.6662  loss_dice_8: 0.8783  time: 1.1028  data_time: 0.0142  lr: 9.0204e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:20:20 d2.utils.events]: \u001b[0m eta: 21:44:30  iter: 8679  total_loss: 19.6  loss_ce: 0.2483  loss_cate: 0.1416  loss_mask: 0.7459  loss_dice: 0.8862  loss_ce_0: 0.3303  loss_cate_0: 0  loss_mask_0: 0.7725  loss_dice_0: 0.9878  loss_ce_1: 0.2031  loss_cate_1: 0  loss_mask_1: 0.7525  loss_dice_1: 0.9437  loss_ce_2: 0.1569  loss_cate_2: 0  loss_mask_2: 0.7551  loss_dice_2: 0.9235  loss_ce_3: 0.223  loss_cate_3: 0  loss_mask_3: 0.759  loss_dice_3: 0.8528  loss_ce_4: 0.1877  loss_cate_4: 0  loss_mask_4: 0.7532  loss_dice_4: 0.8561  loss_ce_5: 0.2202  loss_cate_5: 0  loss_mask_5: 0.7424  loss_dice_5: 0.8531  loss_ce_6: 0.2162  loss_cate_6: 0  loss_mask_6: 0.7427  loss_dice_6: 0.8622  loss_ce_7: 0.2007  loss_cate_7: 0  loss_mask_7: 0.7467  loss_dice_7: 0.9371  loss_ce_8: 0.1488  loss_cate_8: 0  loss_mask_8: 0.7456  loss_dice_8: 0.8903  time: 1.1028  data_time: 0.0132  lr: 9.0181e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:20:42 d2.utils.events]: \u001b[0m eta: 21:44:05  iter: 8699  total_loss: 20.67  loss_ce: 0.3095  loss_cate: 0.1559  loss_mask: 0.7733  loss_dice: 0.9514  loss_ce_0: 0.4565  loss_cate_0: 0  loss_mask_0: 0.8302  loss_dice_0: 1.024  loss_ce_1: 0.3737  loss_cate_1: 0  loss_mask_1: 0.7997  loss_dice_1: 1.006  loss_ce_2: 0.3361  loss_cate_2: 0  loss_mask_2: 0.8177  loss_dice_2: 0.9095  loss_ce_3: 0.31  loss_cate_3: 0  loss_mask_3: 0.777  loss_dice_3: 0.9101  loss_ce_4: 0.3296  loss_cate_4: 0  loss_mask_4: 0.7924  loss_dice_4: 0.8897  loss_ce_5: 0.3018  loss_cate_5: 0  loss_mask_5: 0.7892  loss_dice_5: 0.9118  loss_ce_6: 0.3146  loss_cate_6: 0  loss_mask_6: 0.785  loss_dice_6: 0.9397  loss_ce_7: 0.3252  loss_cate_7: 0  loss_mask_7: 0.7706  loss_dice_7: 0.9182  loss_ce_8: 0.3268  loss_cate_8: 0  loss_mask_8: 0.7763  loss_dice_8: 0.9105  time: 1.1028  data_time: 0.0138  lr: 9.0158e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:21:04 d2.utils.events]: \u001b[0m eta: 21:43:40  iter: 8719  total_loss: 21.27  loss_ce: 0.1612  loss_cate: 0.1444  loss_mask: 0.9487  loss_dice: 0.981  loss_ce_0: 0.3237  loss_cate_0: 0  loss_mask_0: 0.845  loss_dice_0: 1.017  loss_ce_1: 0.1898  loss_cate_1: 0  loss_mask_1: 0.8535  loss_dice_1: 1.009  loss_ce_2: 0.2549  loss_cate_2: 0  loss_mask_2: 0.8446  loss_dice_2: 0.9829  loss_ce_3: 0.2161  loss_cate_3: 0  loss_mask_3: 0.8867  loss_dice_3: 0.9444  loss_ce_4: 0.1977  loss_cate_4: 0  loss_mask_4: 0.8689  loss_dice_4: 0.9517  loss_ce_5: 0.1735  loss_cate_5: 0  loss_mask_5: 0.8766  loss_dice_5: 0.9523  loss_ce_6: 0.2049  loss_cate_6: 0  loss_mask_6: 0.8709  loss_dice_6: 0.9644  loss_ce_7: 0.1777  loss_cate_7: 0  loss_mask_7: 0.9015  loss_dice_7: 0.9661  loss_ce_8: 0.1963  loss_cate_8: 0  loss_mask_8: 0.925  loss_dice_8: 1.024  time: 1.1027  data_time: 0.0130  lr: 9.0135e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:21:27 d2.utils.events]: \u001b[0m eta: 21:43:18  iter: 8739  total_loss: 20.98  loss_ce: 0.2163  loss_cate: 0.172  loss_mask: 0.8275  loss_dice: 1.025  loss_ce_0: 0.3248  loss_cate_0: 0  loss_mask_0: 0.8699  loss_dice_0: 1.077  loss_ce_1: 0.2504  loss_cate_1: 0  loss_mask_1: 0.8452  loss_dice_1: 1.037  loss_ce_2: 0.2657  loss_cate_2: 0  loss_mask_2: 0.8359  loss_dice_2: 0.9964  loss_ce_3: 0.2188  loss_cate_3: 0  loss_mask_3: 0.8331  loss_dice_3: 1.036  loss_ce_4: 0.227  loss_cate_4: 0  loss_mask_4: 0.8202  loss_dice_4: 1.022  loss_ce_5: 0.2105  loss_cate_5: 0  loss_mask_5: 0.8206  loss_dice_5: 1.057  loss_ce_6: 0.2115  loss_cate_6: 0  loss_mask_6: 0.8434  loss_dice_6: 0.9974  loss_ce_7: 0.1969  loss_cate_7: 0  loss_mask_7: 0.8207  loss_dice_7: 1.017  loss_ce_8: 0.2047  loss_cate_8: 0  loss_mask_8: 0.8444  loss_dice_8: 1.056  time: 1.1027  data_time: 0.0134  lr: 9.0113e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:21:49 d2.utils.events]: \u001b[0m eta: 21:42:53  iter: 8759  total_loss: 21.08  loss_ce: 0.2171  loss_cate: 0.1479  loss_mask: 0.7689  loss_dice: 0.9618  loss_ce_0: 0.3395  loss_cate_0: 0  loss_mask_0: 0.8243  loss_dice_0: 0.9782  loss_ce_1: 0.2878  loss_cate_1: 0  loss_mask_1: 0.7775  loss_dice_1: 0.9879  loss_ce_2: 0.2465  loss_cate_2: 0  loss_mask_2: 0.7976  loss_dice_2: 0.8872  loss_ce_3: 0.2047  loss_cate_3: 0  loss_mask_3: 0.7856  loss_dice_3: 0.9282  loss_ce_4: 0.1561  loss_cate_4: 0  loss_mask_4: 0.743  loss_dice_4: 0.9483  loss_ce_5: 0.1684  loss_cate_5: 0  loss_mask_5: 0.7519  loss_dice_5: 0.9226  loss_ce_6: 0.2017  loss_cate_6: 0  loss_mask_6: 0.758  loss_dice_6: 0.918  loss_ce_7: 0.2127  loss_cate_7: 0  loss_mask_7: 0.7502  loss_dice_7: 0.962  loss_ce_8: 0.1664  loss_cate_8: 0  loss_mask_8: 0.7673  loss_dice_8: 0.9486  time: 1.1027  data_time: 0.0130  lr: 9.009e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:22:12 d2.utils.events]: \u001b[0m eta: 21:42:32  iter: 8779  total_loss: 23.36  loss_ce: 0.2607  loss_cate: 0.1646  loss_mask: 0.9443  loss_dice: 1.121  loss_ce_0: 0.4055  loss_cate_0: 0  loss_mask_0: 0.9621  loss_dice_0: 1.127  loss_ce_1: 0.3235  loss_cate_1: 0  loss_mask_1: 0.973  loss_dice_1: 1.124  loss_ce_2: 0.2737  loss_cate_2: 0  loss_mask_2: 0.9112  loss_dice_2: 1.1  loss_ce_3: 0.3388  loss_cate_3: 0  loss_mask_3: 0.9312  loss_dice_3: 1.088  loss_ce_4: 0.2963  loss_cate_4: 0  loss_mask_4: 0.9388  loss_dice_4: 1.134  loss_ce_5: 0.278  loss_cate_5: 0  loss_mask_5: 0.9047  loss_dice_5: 1.113  loss_ce_6: 0.282  loss_cate_6: 0  loss_mask_6: 0.9436  loss_dice_6: 1.109  loss_ce_7: 0.2748  loss_cate_7: 0  loss_mask_7: 0.9119  loss_dice_7: 1.123  loss_ce_8: 0.2643  loss_cate_8: 0  loss_mask_8: 0.9066  loss_dice_8: 1.135  time: 1.1027  data_time: 0.0126  lr: 9.0067e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:22:34 d2.utils.events]: \u001b[0m eta: 21:42:00  iter: 8799  total_loss: 21.81  loss_ce: 0.2068  loss_cate: 0.1484  loss_mask: 0.8816  loss_dice: 1.046  loss_ce_0: 0.3252  loss_cate_0: 0  loss_mask_0: 0.9398  loss_dice_0: 1.065  loss_ce_1: 0.2089  loss_cate_1: 0  loss_mask_1: 0.8847  loss_dice_1: 1.039  loss_ce_2: 0.2378  loss_cate_2: 0  loss_mask_2: 0.862  loss_dice_2: 1.079  loss_ce_3: 0.2572  loss_cate_3: 0  loss_mask_3: 0.876  loss_dice_3: 1.02  loss_ce_4: 0.2329  loss_cate_4: 0  loss_mask_4: 0.8936  loss_dice_4: 1.029  loss_ce_5: 0.2281  loss_cate_5: 0  loss_mask_5: 0.8886  loss_dice_5: 1.034  loss_ce_6: 0.2082  loss_cate_6: 0  loss_mask_6: 0.8838  loss_dice_6: 1.039  loss_ce_7: 0.2302  loss_cate_7: 0  loss_mask_7: 0.8645  loss_dice_7: 1.071  loss_ce_8: 0.2078  loss_cate_8: 0  loss_mask_8: 0.864  loss_dice_8: 1.044  time: 1.1027  data_time: 0.0130  lr: 9.0044e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:22:56 d2.utils.events]: \u001b[0m eta: 21:41:35  iter: 8819  total_loss: 21.25  loss_ce: 0.237  loss_cate: 0.1604  loss_mask: 0.924  loss_dice: 1.016  loss_ce_0: 0.3636  loss_cate_0: 0  loss_mask_0: 0.9141  loss_dice_0: 1.034  loss_ce_1: 0.264  loss_cate_1: 0  loss_mask_1: 0.9218  loss_dice_1: 1.021  loss_ce_2: 0.2658  loss_cate_2: 0  loss_mask_2: 0.9023  loss_dice_2: 0.993  loss_ce_3: 0.2276  loss_cate_3: 0  loss_mask_3: 0.9269  loss_dice_3: 1.051  loss_ce_4: 0.2529  loss_cate_4: 0  loss_mask_4: 0.9135  loss_dice_4: 1.021  loss_ce_5: 0.2419  loss_cate_5: 0  loss_mask_5: 0.9054  loss_dice_5: 1.018  loss_ce_6: 0.252  loss_cate_6: 0  loss_mask_6: 0.9137  loss_dice_6: 1.013  loss_ce_7: 0.2612  loss_cate_7: 0  loss_mask_7: 0.8993  loss_dice_7: 1.021  loss_ce_8: 0.2416  loss_cate_8: 0  loss_mask_8: 0.932  loss_dice_8: 0.9832  time: 1.1027  data_time: 0.0125  lr: 9.0022e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:23:19 d2.utils.events]: \u001b[0m eta: 21:41:13  iter: 8839  total_loss: 19.66  loss_ce: 0.1349  loss_cate: 0.1525  loss_mask: 0.7759  loss_dice: 0.9929  loss_ce_0: 0.3198  loss_cate_0: 0  loss_mask_0: 0.7781  loss_dice_0: 1.007  loss_ce_1: 0.2175  loss_cate_1: 0  loss_mask_1: 0.7859  loss_dice_1: 0.9879  loss_ce_2: 0.1713  loss_cate_2: 0  loss_mask_2: 0.7383  loss_dice_2: 1.036  loss_ce_3: 0.1668  loss_cate_3: 0  loss_mask_3: 0.7333  loss_dice_3: 0.9888  loss_ce_4: 0.1749  loss_cate_4: 0  loss_mask_4: 0.7491  loss_dice_4: 0.977  loss_ce_5: 0.1543  loss_cate_5: 0  loss_mask_5: 0.7542  loss_dice_5: 0.9737  loss_ce_6: 0.1745  loss_cate_6: 0  loss_mask_6: 0.7398  loss_dice_6: 0.9597  loss_ce_7: 0.1572  loss_cate_7: 0  loss_mask_7: 0.7783  loss_dice_7: 0.946  loss_ce_8: 0.1708  loss_cate_8: 0  loss_mask_8: 0.7627  loss_dice_8: 0.9011  time: 1.1027  data_time: 0.0131  lr: 8.9999e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:23:43 d2.utils.events]: \u001b[0m eta: 21:40:53  iter: 8859  total_loss: 21.86  loss_ce: 0.2576  loss_cate: 0.1618  loss_mask: 0.8143  loss_dice: 0.9368  loss_ce_0: 0.4418  loss_cate_0: 0  loss_mask_0: 0.8415  loss_dice_0: 1.01  loss_ce_1: 0.3187  loss_cate_1: 0  loss_mask_1: 0.8453  loss_dice_1: 0.9922  loss_ce_2: 0.2949  loss_cate_2: 0  loss_mask_2: 0.8101  loss_dice_2: 0.9386  loss_ce_3: 0.3218  loss_cate_3: 0  loss_mask_3: 0.8435  loss_dice_3: 0.9439  loss_ce_4: 0.2832  loss_cate_4: 0  loss_mask_4: 0.848  loss_dice_4: 0.9642  loss_ce_5: 0.2846  loss_cate_5: 0  loss_mask_5: 0.8744  loss_dice_5: 0.9376  loss_ce_6: 0.3198  loss_cate_6: 0  loss_mask_6: 0.8465  loss_dice_6: 0.9375  loss_ce_7: 0.2288  loss_cate_7: 0  loss_mask_7: 0.8352  loss_dice_7: 0.9122  loss_ce_8: 0.2659  loss_cate_8: 0  loss_mask_8: 0.8201  loss_dice_8: 0.9306  time: 1.1027  data_time: 0.0136  lr: 8.9976e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:24:05 d2.utils.events]: \u001b[0m eta: 21:40:33  iter: 8879  total_loss: 19.14  loss_ce: 0.1837  loss_cate: 0.1419  loss_mask: 0.7851  loss_dice: 0.9196  loss_ce_0: 0.3091  loss_cate_0: 0  loss_mask_0: 0.7972  loss_dice_0: 0.9675  loss_ce_1: 0.1894  loss_cate_1: 0  loss_mask_1: 0.8262  loss_dice_1: 0.97  loss_ce_2: 0.188  loss_cate_2: 0  loss_mask_2: 0.7761  loss_dice_2: 0.9238  loss_ce_3: 0.1814  loss_cate_3: 0  loss_mask_3: 0.7705  loss_dice_3: 0.9331  loss_ce_4: 0.1965  loss_cate_4: 0  loss_mask_4: 0.8073  loss_dice_4: 0.9226  loss_ce_5: 0.1697  loss_cate_5: 0  loss_mask_5: 0.7876  loss_dice_5: 0.9113  loss_ce_6: 0.2151  loss_cate_6: 0  loss_mask_6: 0.7828  loss_dice_6: 0.9109  loss_ce_7: 0.1834  loss_cate_7: 0  loss_mask_7: 0.7948  loss_dice_7: 0.9244  loss_ce_8: 0.209  loss_cate_8: 0  loss_mask_8: 0.8142  loss_dice_8: 0.9231  time: 1.1027  data_time: 0.0128  lr: 8.9953e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:24:28 d2.utils.events]: \u001b[0m eta: 21:40:08  iter: 8899  total_loss: 20.08  loss_ce: 0.138  loss_cate: 0.17  loss_mask: 0.8314  loss_dice: 0.9776  loss_ce_0: 0.32  loss_cate_0: 0  loss_mask_0: 0.8508  loss_dice_0: 0.9493  loss_ce_1: 0.2312  loss_cate_1: 0  loss_mask_1: 0.8373  loss_dice_1: 0.9776  loss_ce_2: 0.1934  loss_cate_2: 0  loss_mask_2: 0.8466  loss_dice_2: 0.968  loss_ce_3: 0.2422  loss_cate_3: 0  loss_mask_3: 0.8137  loss_dice_3: 0.9454  loss_ce_4: 0.2257  loss_cate_4: 0  loss_mask_4: 0.8323  loss_dice_4: 0.9671  loss_ce_5: 0.1824  loss_cate_5: 0  loss_mask_5: 0.8262  loss_dice_5: 0.9676  loss_ce_6: 0.1061  loss_cate_6: 0  loss_mask_6: 0.8336  loss_dice_6: 0.9862  loss_ce_7: 0.2071  loss_cate_7: 0  loss_mask_7: 0.8306  loss_dice_7: 0.9847  loss_ce_8: 0.175  loss_cate_8: 0  loss_mask_8: 0.8377  loss_dice_8: 0.9869  time: 1.1027  data_time: 0.0156  lr: 8.9931e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:24:51 d2.utils.events]: \u001b[0m eta: 21:39:47  iter: 8919  total_loss: 22.18  loss_ce: 0.1953  loss_cate: 0.1794  loss_mask: 0.8567  loss_dice: 1.044  loss_ce_0: 0.3264  loss_cate_0: 0  loss_mask_0: 0.9516  loss_dice_0: 1.087  loss_ce_1: 0.2252  loss_cate_1: 0  loss_mask_1: 0.8687  loss_dice_1: 1.065  loss_ce_2: 0.22  loss_cate_2: 0  loss_mask_2: 0.8902  loss_dice_2: 1.078  loss_ce_3: 0.245  loss_cate_3: 0  loss_mask_3: 0.8717  loss_dice_3: 1.03  loss_ce_4: 0.2369  loss_cate_4: 0  loss_mask_4: 0.921  loss_dice_4: 1.023  loss_ce_5: 0.2278  loss_cate_5: 0  loss_mask_5: 0.8536  loss_dice_5: 1.022  loss_ce_6: 0.244  loss_cate_6: 0  loss_mask_6: 0.862  loss_dice_6: 1.029  loss_ce_7: 0.2233  loss_cate_7: 0  loss_mask_7: 0.8825  loss_dice_7: 1.065  loss_ce_8: 0.1921  loss_cate_8: 0  loss_mask_8: 0.8614  loss_dice_8: 1.055  time: 1.1027  data_time: 0.0141  lr: 8.9908e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:25:13 d2.utils.events]: \u001b[0m eta: 21:39:27  iter: 8939  total_loss: 22.71  loss_ce: 0.2625  loss_cate: 0.1738  loss_mask: 0.9549  loss_dice: 1.111  loss_ce_0: 0.3499  loss_cate_0: 0  loss_mask_0: 0.9863  loss_dice_0: 1.169  loss_ce_1: 0.3413  loss_cate_1: 0  loss_mask_1: 0.9257  loss_dice_1: 1.084  loss_ce_2: 0.3878  loss_cate_2: 0  loss_mask_2: 0.8832  loss_dice_2: 1.053  loss_ce_3: 0.3887  loss_cate_3: 0  loss_mask_3: 0.8829  loss_dice_3: 1.054  loss_ce_4: 0.3554  loss_cate_4: 0  loss_mask_4: 0.8898  loss_dice_4: 1.085  loss_ce_5: 0.3294  loss_cate_5: 0  loss_mask_5: 0.8941  loss_dice_5: 1.097  loss_ce_6: 0.2699  loss_cate_6: 0  loss_mask_6: 0.8678  loss_dice_6: 1.082  loss_ce_7: 0.2753  loss_cate_7: 0  loss_mask_7: 0.8791  loss_dice_7: 1.075  loss_ce_8: 0.2586  loss_cate_8: 0  loss_mask_8: 0.915  loss_dice_8: 1.11  time: 1.1027  data_time: 0.0120  lr: 8.9885e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:25:36 d2.utils.events]: \u001b[0m eta: 21:39:17  iter: 8959  total_loss: 20.06  loss_ce: 0.2836  loss_cate: 0.1505  loss_mask: 0.6806  loss_dice: 0.9767  loss_ce_0: 0.4069  loss_cate_0: 0  loss_mask_0: 0.7849  loss_dice_0: 1.02  loss_ce_1: 0.2898  loss_cate_1: 0  loss_mask_1: 0.7317  loss_dice_1: 1.002  loss_ce_2: 0.4116  loss_cate_2: 0  loss_mask_2: 0.7568  loss_dice_2: 0.9387  loss_ce_3: 0.313  loss_cate_3: 0  loss_mask_3: 0.712  loss_dice_3: 0.9706  loss_ce_4: 0.3466  loss_cate_4: 0  loss_mask_4: 0.7134  loss_dice_4: 0.9539  loss_ce_5: 0.2909  loss_cate_5: 0  loss_mask_5: 0.6699  loss_dice_5: 0.9214  loss_ce_6: 0.3179  loss_cate_6: 0  loss_mask_6: 0.7324  loss_dice_6: 0.9669  loss_ce_7: 0.2555  loss_cate_7: 0  loss_mask_7: 0.6926  loss_dice_7: 0.994  loss_ce_8: 0.2965  loss_cate_8: 0  loss_mask_8: 0.68  loss_dice_8: 0.9885  time: 1.1027  data_time: 0.0135  lr: 8.9862e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:25:58 d2.utils.events]: \u001b[0m eta: 21:39:23  iter: 8979  total_loss: 21.01  loss_ce: 0.1966  loss_cate: 0.1383  loss_mask: 0.8981  loss_dice: 1.097  loss_ce_0: 0.3289  loss_cate_0: 0  loss_mask_0: 0.8676  loss_dice_0: 1.104  loss_ce_1: 0.2282  loss_cate_1: 0  loss_mask_1: 0.8612  loss_dice_1: 1.141  loss_ce_2: 0.2421  loss_cate_2: 0  loss_mask_2: 0.8542  loss_dice_2: 1.061  loss_ce_3: 0.2293  loss_cate_3: 0  loss_mask_3: 0.8798  loss_dice_3: 1.077  loss_ce_4: 0.1948  loss_cate_4: 0  loss_mask_4: 0.8544  loss_dice_4: 1.08  loss_ce_5: 0.2087  loss_cate_5: 0  loss_mask_5: 0.8585  loss_dice_5: 1.07  loss_ce_6: 0.2112  loss_cate_6: 0  loss_mask_6: 0.8227  loss_dice_6: 1.08  loss_ce_7: 0.1979  loss_cate_7: 0  loss_mask_7: 0.8723  loss_dice_7: 1.089  loss_ce_8: 0.1973  loss_cate_8: 0  loss_mask_8: 0.8332  loss_dice_8: 1.1  time: 1.1027  data_time: 0.0136  lr: 8.9839e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:27:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 15:27:24 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 15:27:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 15:28:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 15:28:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0018 s/iter. Inference: 0.1629 s/iter. Eval: 0.0135 s/iter. Total: 0.1783 s/iter. ETA=0:05:57\n",
      "\u001b[32m[10/12 15:28:37 d2.evaluation.evaluator]: \u001b[0mInference done 38/2016. Dataloading: 0.0026 s/iter. Inference: 0.1644 s/iter. Eval: 0.0190 s/iter. Total: 0.1861 s/iter. ETA=0:06:08\n",
      "\u001b[32m[10/12 15:28:42 d2.evaluation.evaluator]: \u001b[0mInference done 63/2016. Dataloading: 0.0027 s/iter. Inference: 0.1639 s/iter. Eval: 0.0272 s/iter. Total: 0.1941 s/iter. ETA=0:06:18\n",
      "\u001b[32m[10/12 15:28:47 d2.evaluation.evaluator]: \u001b[0mInference done 89/2016. Dataloading: 0.0028 s/iter. Inference: 0.1643 s/iter. Eval: 0.0280 s/iter. Total: 0.1952 s/iter. ETA=0:06:16\n",
      "\u001b[32m[10/12 15:28:52 d2.evaluation.evaluator]: \u001b[0mInference done 114/2016. Dataloading: 0.0027 s/iter. Inference: 0.1647 s/iter. Eval: 0.0287 s/iter. Total: 0.1963 s/iter. ETA=0:06:13\n",
      "\u001b[32m[10/12 15:28:57 d2.evaluation.evaluator]: \u001b[0mInference done 141/2016. Dataloading: 0.0027 s/iter. Inference: 0.1647 s/iter. Eval: 0.0276 s/iter. Total: 0.1951 s/iter. ETA=0:06:05\n",
      "\u001b[32m[10/12 15:29:02 d2.evaluation.evaluator]: \u001b[0mInference done 166/2016. Dataloading: 0.0026 s/iter. Inference: 0.1645 s/iter. Eval: 0.0296 s/iter. Total: 0.1970 s/iter. ETA=0:06:04\n",
      "\u001b[32m[10/12 15:29:07 d2.evaluation.evaluator]: \u001b[0mInference done 193/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0287 s/iter. Total: 0.1956 s/iter. ETA=0:05:56\n",
      "\u001b[32m[10/12 15:29:12 d2.evaluation.evaluator]: \u001b[0mInference done 218/2016. Dataloading: 0.0026 s/iter. Inference: 0.1642 s/iter. Eval: 0.0299 s/iter. Total: 0.1968 s/iter. ETA=0:05:53\n",
      "\u001b[32m[10/12 15:29:17 d2.evaluation.evaluator]: \u001b[0mInference done 244/2016. Dataloading: 0.0026 s/iter. Inference: 0.1642 s/iter. Eval: 0.0301 s/iter. Total: 0.1970 s/iter. ETA=0:05:49\n",
      "\u001b[32m[10/12 15:29:23 d2.evaluation.evaluator]: \u001b[0mInference done 270/2016. Dataloading: 0.0026 s/iter. Inference: 0.1645 s/iter. Eval: 0.0297 s/iter. Total: 0.1969 s/iter. ETA=0:05:43\n",
      "\u001b[32m[10/12 15:29:28 d2.evaluation.evaluator]: \u001b[0mInference done 296/2016. Dataloading: 0.0026 s/iter. Inference: 0.1644 s/iter. Eval: 0.0297 s/iter. Total: 0.1969 s/iter. ETA=0:05:38\n",
      "\u001b[32m[10/12 15:29:33 d2.evaluation.evaluator]: \u001b[0mInference done 322/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0299 s/iter. Total: 0.1969 s/iter. ETA=0:05:33\n",
      "\u001b[32m[10/12 15:29:38 d2.evaluation.evaluator]: \u001b[0mInference done 348/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0299 s/iter. Total: 0.1969 s/iter. ETA=0:05:28\n",
      "\u001b[32m[10/12 15:29:43 d2.evaluation.evaluator]: \u001b[0mInference done 374/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0299 s/iter. Total: 0.1970 s/iter. ETA=0:05:23\n",
      "\u001b[32m[10/12 15:29:48 d2.evaluation.evaluator]: \u001b[0mInference done 401/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0296 s/iter. Total: 0.1965 s/iter. ETA=0:05:17\n",
      "\u001b[32m[10/12 15:29:53 d2.evaluation.evaluator]: \u001b[0mInference done 426/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0300 s/iter. Total: 0.1969 s/iter. ETA=0:05:13\n",
      "\u001b[32m[10/12 15:29:58 d2.evaluation.evaluator]: \u001b[0mInference done 453/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0297 s/iter. Total: 0.1964 s/iter. ETA=0:05:07\n",
      "\u001b[32m[10/12 15:30:04 d2.evaluation.evaluator]: \u001b[0mInference done 479/2016. Dataloading: 0.0026 s/iter. Inference: 0.1641 s/iter. Eval: 0.0298 s/iter. Total: 0.1966 s/iter. ETA=0:05:02\n",
      "\u001b[32m[10/12 15:30:09 d2.evaluation.evaluator]: \u001b[0mInference done 505/2016. Dataloading: 0.0026 s/iter. Inference: 0.1640 s/iter. Eval: 0.0299 s/iter. Total: 0.1966 s/iter. ETA=0:04:57\n",
      "\u001b[32m[10/12 15:30:14 d2.evaluation.evaluator]: \u001b[0mInference done 532/2016. Dataloading: 0.0026 s/iter. Inference: 0.1640 s/iter. Eval: 0.0295 s/iter. Total: 0.1963 s/iter. ETA=0:04:51\n",
      "\u001b[32m[10/12 15:30:19 d2.evaluation.evaluator]: \u001b[0mInference done 557/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0297 s/iter. Total: 0.1966 s/iter. ETA=0:04:46\n",
      "\u001b[32m[10/12 15:30:24 d2.evaluation.evaluator]: \u001b[0mInference done 583/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0297 s/iter. Total: 0.1965 s/iter. ETA=0:04:41\n",
      "\u001b[32m[10/12 15:30:29 d2.evaluation.evaluator]: \u001b[0mInference done 609/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0296 s/iter. Total: 0.1965 s/iter. ETA=0:04:36\n",
      "\u001b[32m[10/12 15:30:34 d2.evaluation.evaluator]: \u001b[0mInference done 636/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0293 s/iter. Total: 0.1961 s/iter. ETA=0:04:30\n",
      "\u001b[32m[10/12 15:30:39 d2.evaluation.evaluator]: \u001b[0mInference done 655/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0313 s/iter. Total: 0.1981 s/iter. ETA=0:04:29\n",
      "\u001b[32m[10/12 15:30:44 d2.evaluation.evaluator]: \u001b[0mInference done 682/2016. Dataloading: 0.0025 s/iter. Inference: 0.1640 s/iter. Eval: 0.0309 s/iter. Total: 0.1977 s/iter. ETA=0:04:23\n",
      "\u001b[32m[10/12 15:30:49 d2.evaluation.evaluator]: \u001b[0mInference done 707/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0310 s/iter. Total: 0.1978 s/iter. ETA=0:04:18\n",
      "\u001b[32m[10/12 15:30:54 d2.evaluation.evaluator]: \u001b[0mInference done 734/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0307 s/iter. Total: 0.1975 s/iter. ETA=0:04:13\n",
      "\u001b[32m[10/12 15:31:00 d2.evaluation.evaluator]: \u001b[0mInference done 760/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0307 s/iter. Total: 0.1975 s/iter. ETA=0:04:08\n",
      "\u001b[32m[10/12 15:31:05 d2.evaluation.evaluator]: \u001b[0mInference done 786/2016. Dataloading: 0.0025 s/iter. Inference: 0.1640 s/iter. Eval: 0.0306 s/iter. Total: 0.1973 s/iter. ETA=0:04:02\n",
      "\u001b[32m[10/12 15:31:10 d2.evaluation.evaluator]: \u001b[0mInference done 811/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0308 s/iter. Total: 0.1975 s/iter. ETA=0:03:58\n",
      "\u001b[32m[10/12 15:31:15 d2.evaluation.evaluator]: \u001b[0mInference done 838/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0304 s/iter. Total: 0.1973 s/iter. ETA=0:03:52\n",
      "\u001b[32m[10/12 15:31:20 d2.evaluation.evaluator]: \u001b[0mInference done 864/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0308 s/iter. Total: 0.1977 s/iter. ETA=0:03:47\n",
      "\u001b[32m[10/12 15:31:25 d2.evaluation.evaluator]: \u001b[0mInference done 890/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0309 s/iter. Total: 0.1977 s/iter. ETA=0:03:42\n",
      "\u001b[32m[10/12 15:31:30 d2.evaluation.evaluator]: \u001b[0mInference done 917/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0306 s/iter. Total: 0.1975 s/iter. ETA=0:03:37\n",
      "\u001b[32m[10/12 15:31:36 d2.evaluation.evaluator]: \u001b[0mInference done 943/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0306 s/iter. Total: 0.1975 s/iter. ETA=0:03:31\n",
      "\u001b[32m[10/12 15:31:41 d2.evaluation.evaluator]: \u001b[0mInference done 969/2016. Dataloading: 0.0025 s/iter. Inference: 0.1642 s/iter. Eval: 0.0305 s/iter. Total: 0.1975 s/iter. ETA=0:03:26\n",
      "\u001b[32m[10/12 15:31:46 d2.evaluation.evaluator]: \u001b[0mInference done 995/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0305 s/iter. Total: 0.1975 s/iter. ETA=0:03:21\n",
      "\u001b[32m[10/12 15:31:51 d2.evaluation.evaluator]: \u001b[0mInference done 1022/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0303 s/iter. Total: 0.1974 s/iter. ETA=0:03:16\n",
      "\u001b[32m[10/12 15:31:56 d2.evaluation.evaluator]: \u001b[0mInference done 1048/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0303 s/iter. Total: 0.1973 s/iter. ETA=0:03:11\n",
      "\u001b[32m[10/12 15:32:01 d2.evaluation.evaluator]: \u001b[0mInference done 1074/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0303 s/iter. Total: 0.1973 s/iter. ETA=0:03:05\n",
      "\u001b[32m[10/12 15:32:06 d2.evaluation.evaluator]: \u001b[0mInference done 1100/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0303 s/iter. Total: 0.1973 s/iter. ETA=0:03:00\n",
      "\u001b[32m[10/12 15:32:12 d2.evaluation.evaluator]: \u001b[0mInference done 1126/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0302 s/iter. Total: 0.1973 s/iter. ETA=0:02:55\n",
      "\u001b[32m[10/12 15:32:17 d2.evaluation.evaluator]: \u001b[0mInference done 1151/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0303 s/iter. Total: 0.1974 s/iter. ETA=0:02:50\n",
      "\u001b[32m[10/12 15:32:22 d2.evaluation.evaluator]: \u001b[0mInference done 1177/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0302 s/iter. Total: 0.1973 s/iter. ETA=0:02:45\n",
      "\u001b[32m[10/12 15:32:27 d2.evaluation.evaluator]: \u001b[0mInference done 1203/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0301 s/iter. Total: 0.1973 s/iter. ETA=0:02:40\n",
      "\u001b[32m[10/12 15:32:32 d2.evaluation.evaluator]: \u001b[0mInference done 1229/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0302 s/iter. Total: 0.1973 s/iter. ETA=0:02:35\n",
      "\u001b[32m[10/12 15:32:37 d2.evaluation.evaluator]: \u001b[0mInference done 1255/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0301 s/iter. Total: 0.1972 s/iter. ETA=0:02:30\n",
      "\u001b[32m[10/12 15:32:42 d2.evaluation.evaluator]: \u001b[0mInference done 1281/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0301 s/iter. Total: 0.1972 s/iter. ETA=0:02:24\n",
      "\u001b[32m[10/12 15:32:47 d2.evaluation.evaluator]: \u001b[0mInference done 1308/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0300 s/iter. Total: 0.1971 s/iter. ETA=0:02:19\n",
      "\u001b[32m[10/12 15:32:52 d2.evaluation.evaluator]: \u001b[0mInference done 1332/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0303 s/iter. Total: 0.1973 s/iter. ETA=0:02:14\n",
      "\u001b[32m[10/12 15:32:57 d2.evaluation.evaluator]: \u001b[0mInference done 1358/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0302 s/iter. Total: 0.1973 s/iter. ETA=0:02:09\n",
      "\u001b[32m[10/12 15:33:03 d2.evaluation.evaluator]: \u001b[0mInference done 1385/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0301 s/iter. Total: 0.1972 s/iter. ETA=0:02:04\n",
      "\u001b[32m[10/12 15:33:08 d2.evaluation.evaluator]: \u001b[0mInference done 1410/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0301 s/iter. Total: 0.1973 s/iter. ETA=0:01:59\n",
      "\u001b[32m[10/12 15:33:13 d2.evaluation.evaluator]: \u001b[0mInference done 1437/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0300 s/iter. Total: 0.1971 s/iter. ETA=0:01:54\n",
      "\u001b[32m[10/12 15:33:18 d2.evaluation.evaluator]: \u001b[0mInference done 1462/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0301 s/iter. Total: 0.1972 s/iter. ETA=0:01:49\n",
      "\u001b[32m[10/12 15:33:23 d2.evaluation.evaluator]: \u001b[0mInference done 1489/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0300 s/iter. Total: 0.1971 s/iter. ETA=0:01:43\n",
      "\u001b[32m[10/12 15:33:28 d2.evaluation.evaluator]: \u001b[0mInference done 1513/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0302 s/iter. Total: 0.1973 s/iter. ETA=0:01:39\n",
      "\u001b[32m[10/12 15:33:33 d2.evaluation.evaluator]: \u001b[0mInference done 1539/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0302 s/iter. Total: 0.1972 s/iter. ETA=0:01:34\n",
      "\u001b[32m[10/12 15:33:38 d2.evaluation.evaluator]: \u001b[0mInference done 1566/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0300 s/iter. Total: 0.1971 s/iter. ETA=0:01:28\n",
      "\u001b[32m[10/12 15:33:43 d2.evaluation.evaluator]: \u001b[0mInference done 1588/2016. Dataloading: 0.0032 s/iter. Inference: 0.1644 s/iter. Eval: 0.0298 s/iter. Total: 0.1976 s/iter. ETA=0:01:24\n",
      "\u001b[32m[10/12 15:33:48 d2.evaluation.evaluator]: \u001b[0mInference done 1614/2016. Dataloading: 0.0031 s/iter. Inference: 0.1644 s/iter. Eval: 0.0298 s/iter. Total: 0.1975 s/iter. ETA=0:01:19\n",
      "\u001b[32m[10/12 15:33:53 d2.evaluation.evaluator]: \u001b[0mInference done 1641/2016. Dataloading: 0.0031 s/iter. Inference: 0.1644 s/iter. Eval: 0.0297 s/iter. Total: 0.1974 s/iter. ETA=0:01:14\n",
      "\u001b[32m[10/12 15:33:58 d2.evaluation.evaluator]: \u001b[0mInference done 1667/2016. Dataloading: 0.0031 s/iter. Inference: 0.1643 s/iter. Eval: 0.0297 s/iter. Total: 0.1973 s/iter. ETA=0:01:08\n",
      "\u001b[32m[10/12 15:34:03 d2.evaluation.evaluator]: \u001b[0mInference done 1692/2016. Dataloading: 0.0031 s/iter. Inference: 0.1643 s/iter. Eval: 0.0298 s/iter. Total: 0.1974 s/iter. ETA=0:01:03\n",
      "\u001b[32m[10/12 15:34:09 d2.evaluation.evaluator]: \u001b[0mInference done 1719/2016. Dataloading: 0.0031 s/iter. Inference: 0.1643 s/iter. Eval: 0.0297 s/iter. Total: 0.1973 s/iter. ETA=0:00:58\n",
      "\u001b[32m[10/12 15:34:14 d2.evaluation.evaluator]: \u001b[0mInference done 1745/2016. Dataloading: 0.0031 s/iter. Inference: 0.1643 s/iter. Eval: 0.0297 s/iter. Total: 0.1972 s/iter. ETA=0:00:53\n",
      "\u001b[32m[10/12 15:34:19 d2.evaluation.evaluator]: \u001b[0mInference done 1772/2016. Dataloading: 0.0031 s/iter. Inference: 0.1643 s/iter. Eval: 0.0295 s/iter. Total: 0.1971 s/iter. ETA=0:00:48\n",
      "\u001b[32m[10/12 15:34:24 d2.evaluation.evaluator]: \u001b[0mInference done 1798/2016. Dataloading: 0.0031 s/iter. Inference: 0.1642 s/iter. Eval: 0.0295 s/iter. Total: 0.1970 s/iter. ETA=0:00:42\n",
      "\u001b[32m[10/12 15:34:29 d2.evaluation.evaluator]: \u001b[0mInference done 1825/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0294 s/iter. Total: 0.1969 s/iter. ETA=0:00:37\n",
      "\u001b[32m[10/12 15:34:34 d2.evaluation.evaluator]: \u001b[0mInference done 1851/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0295 s/iter. Total: 0.1969 s/iter. ETA=0:00:32\n",
      "\u001b[32m[10/12 15:34:39 d2.evaluation.evaluator]: \u001b[0mInference done 1878/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0294 s/iter. Total: 0.1968 s/iter. ETA=0:00:27\n",
      "\u001b[32m[10/12 15:34:44 d2.evaluation.evaluator]: \u001b[0mInference done 1902/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0296 s/iter. Total: 0.1969 s/iter. ETA=0:00:22\n",
      "\u001b[32m[10/12 15:34:49 d2.evaluation.evaluator]: \u001b[0mInference done 1928/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0296 s/iter. Total: 0.1969 s/iter. ETA=0:00:17\n",
      "\u001b[32m[10/12 15:34:54 d2.evaluation.evaluator]: \u001b[0mInference done 1954/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0295 s/iter. Total: 0.1969 s/iter. ETA=0:00:12\n",
      "\u001b[32m[10/12 15:34:59 d2.evaluation.evaluator]: \u001b[0mInference done 1981/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0295 s/iter. Total: 0.1968 s/iter. ETA=0:00:06\n",
      "\u001b[32m[10/12 15:35:04 d2.evaluation.evaluator]: \u001b[0mInference done 2007/2016. Dataloading: 0.0030 s/iter. Inference: 0.1642 s/iter. Eval: 0.0295 s/iter. Total: 0.1968 s/iter. ETA=0:00:01\n",
      "\u001b[32m[10/12 15:35:07 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:36.214762 (0.197024 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 15:35:07 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:30 (0.164153 s / iter per device, on 1 devices)\n",
      "miou = 75.29872248533823\n",
      "OA = 88.89865288658748\n",
      "Kappa = 85.50944265988377\n",
      "F1_score = 71.89604692061965\n",
      "\u001b[32m[10/12 15:35:07 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 75.29872248533823, 'fwIoU': 80.48282926885979, 'IoU-Background': 41.575062268314696, 'IoU-Surfaces': 84.45431189572462, 'IoU-Building': 91.4086643340174, 'IoU-Low vegetation': 74.29288232630995, 'IoU-tree': 75.62976403049937, 'IoU-Car': 84.4316500571634, 'mACC': 83.85987335943827, 'pACC': 88.89865288658748, 'ACC-Background': 51.756362557623156, 'ACC-Surfaces': 91.65030658428869, 'ACC-Building': 96.48864733866391, 'ACC-Low vegetation': 88.42046234453204, 'ACC-tree': 83.63199453283072, 'ACC-Car': 91.21146679869103})])\n",
      "\u001b[32m[10/12 15:35:07 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 15:35:07 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 15:35:07 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 15:35:07 d2.evaluation.testing]: \u001b[0mcopypaste: 75.2987,80.4828,83.8599,88.8987\n",
      "\u001b[32m[10/12 15:35:07 d2.utils.events]: \u001b[0m eta: 21:39:07  iter: 8999  total_loss: 19.72  loss_ce: 0.1838  loss_cate: 0.162  loss_mask: 0.7861  loss_dice: 0.958  loss_ce_0: 0.3134  loss_cate_0: 0  loss_mask_0: 0.8105  loss_dice_0: 1.026  loss_ce_1: 0.2401  loss_cate_1: 0  loss_mask_1: 0.7849  loss_dice_1: 0.9841  loss_ce_2: 0.1735  loss_cate_2: 0  loss_mask_2: 0.7775  loss_dice_2: 0.9576  loss_ce_3: 0.1833  loss_cate_3: 0  loss_mask_3: 0.7717  loss_dice_3: 0.9601  loss_ce_4: 0.1785  loss_cate_4: 0  loss_mask_4: 0.7718  loss_dice_4: 0.9812  loss_ce_5: 0.191  loss_cate_5: 0  loss_mask_5: 0.7655  loss_dice_5: 0.9243  loss_ce_6: 0.2013  loss_cate_6: 0  loss_mask_6: 0.7892  loss_dice_6: 0.9351  loss_ce_7: 0.1855  loss_cate_7: 0  loss_mask_7: 0.7816  loss_dice_7: 0.9612  loss_ce_8: 0.1954  loss_cate_8: 0  loss_mask_8: 0.7842  loss_dice_8: 0.924  time: 1.1027  data_time: 0.0134  lr: 8.9817e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:35:30 d2.utils.events]: \u001b[0m eta: 21:39:00  iter: 9019  total_loss: 20.48  loss_ce: 0.2487  loss_cate: 0.1771  loss_mask: 0.773  loss_dice: 0.9785  loss_ce_0: 0.4925  loss_cate_0: 0  loss_mask_0: 0.8068  loss_dice_0: 1.029  loss_ce_1: 0.2616  loss_cate_1: 0  loss_mask_1: 0.74  loss_dice_1: 0.9602  loss_ce_2: 0.3636  loss_cate_2: 0  loss_mask_2: 0.765  loss_dice_2: 0.8953  loss_ce_3: 0.296  loss_cate_3: 0  loss_mask_3: 0.743  loss_dice_3: 0.8895  loss_ce_4: 0.3053  loss_cate_4: 0  loss_mask_4: 0.768  loss_dice_4: 0.9106  loss_ce_5: 0.28  loss_cate_5: 0  loss_mask_5: 0.7645  loss_dice_5: 0.9253  loss_ce_6: 0.2427  loss_cate_6: 0  loss_mask_6: 0.7566  loss_dice_6: 0.9568  loss_ce_7: 0.205  loss_cate_7: 0  loss_mask_7: 0.7677  loss_dice_7: 0.9524  loss_ce_8: 0.2127  loss_cate_8: 0  loss_mask_8: 0.7647  loss_dice_8: 0.9649  time: 1.1027  data_time: 0.0146  lr: 8.9794e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:35:55 d2.utils.events]: \u001b[0m eta: 21:38:41  iter: 9039  total_loss: 21.87  loss_ce: 0.2121  loss_cate: 0.1448  loss_mask: 0.7442  loss_dice: 0.9591  loss_ce_0: 0.3453  loss_cate_0: 0  loss_mask_0: 0.8759  loss_dice_0: 0.9975  loss_ce_1: 0.1961  loss_cate_1: 0  loss_mask_1: 0.7853  loss_dice_1: 0.9962  loss_ce_2: 0.2208  loss_cate_2: 0  loss_mask_2: 0.759  loss_dice_2: 0.9357  loss_ce_3: 0.2511  loss_cate_3: 0  loss_mask_3: 0.7669  loss_dice_3: 0.949  loss_ce_4: 0.2728  loss_cate_4: 0  loss_mask_4: 0.7188  loss_dice_4: 0.9256  loss_ce_5: 0.2331  loss_cate_5: 0  loss_mask_5: 0.7223  loss_dice_5: 0.9235  loss_ce_6: 0.2414  loss_cate_6: 0  loss_mask_6: 0.7194  loss_dice_6: 0.9041  loss_ce_7: 0.2035  loss_cate_7: 0  loss_mask_7: 0.696  loss_dice_7: 0.9242  loss_ce_8: 0.2373  loss_cate_8: 0  loss_mask_8: 0.7523  loss_dice_8: 0.9747  time: 1.1027  data_time: 0.0134  lr: 8.9771e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:36:19 d2.utils.events]: \u001b[0m eta: 21:38:23  iter: 9059  total_loss: 21.96  loss_ce: 0.1861  loss_cate: 0.1846  loss_mask: 0.7624  loss_dice: 0.9803  loss_ce_0: 0.3633  loss_cate_0: 0  loss_mask_0: 0.8039  loss_dice_0: 1.073  loss_ce_1: 0.3218  loss_cate_1: 0  loss_mask_1: 0.809  loss_dice_1: 1.038  loss_ce_2: 0.2727  loss_cate_2: 0  loss_mask_2: 0.7515  loss_dice_2: 1.02  loss_ce_3: 0.2983  loss_cate_3: 0  loss_mask_3: 0.8007  loss_dice_3: 0.9672  loss_ce_4: 0.2496  loss_cate_4: 0  loss_mask_4: 0.838  loss_dice_4: 0.9952  loss_ce_5: 0.245  loss_cate_5: 0  loss_mask_5: 0.8043  loss_dice_5: 0.9762  loss_ce_6: 0.2004  loss_cate_6: 0  loss_mask_6: 0.7835  loss_dice_6: 0.9619  loss_ce_7: 0.1935  loss_cate_7: 0  loss_mask_7: 0.7696  loss_dice_7: 0.9804  loss_ce_8: 0.1746  loss_cate_8: 0  loss_mask_8: 0.7395  loss_dice_8: 1.028  time: 1.1027  data_time: 0.0133  lr: 8.9748e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:36:41 d2.utils.events]: \u001b[0m eta: 21:38:08  iter: 9079  total_loss: 17.99  loss_ce: 0.2016  loss_cate: 0.14  loss_mask: 0.6354  loss_dice: 0.8529  loss_ce_0: 0.3453  loss_cate_0: 0  loss_mask_0: 0.6395  loss_dice_0: 0.9393  loss_ce_1: 0.2766  loss_cate_1: 0  loss_mask_1: 0.649  loss_dice_1: 0.9077  loss_ce_2: 0.2113  loss_cate_2: 0  loss_mask_2: 0.62  loss_dice_2: 0.9779  loss_ce_3: 0.2092  loss_cate_3: 0  loss_mask_3: 0.6199  loss_dice_3: 0.9154  loss_ce_4: 0.2123  loss_cate_4: 0  loss_mask_4: 0.6336  loss_dice_4: 0.9433  loss_ce_5: 0.2085  loss_cate_5: 0  loss_mask_5: 0.637  loss_dice_5: 0.9544  loss_ce_6: 0.2147  loss_cate_6: 0  loss_mask_6: 0.635  loss_dice_6: 0.8833  loss_ce_7: 0.2318  loss_cate_7: 0  loss_mask_7: 0.6391  loss_dice_7: 0.9183  loss_ce_8: 0.2118  loss_cate_8: 0  loss_mask_8: 0.6308  loss_dice_8: 0.9369  time: 1.1027  data_time: 0.0123  lr: 8.9726e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:37:08 d2.utils.events]: \u001b[0m eta: 21:37:55  iter: 9099  total_loss: 23.07  loss_ce: 0.3119  loss_cate: 0.1565  loss_mask: 0.8287  loss_dice: 0.9673  loss_ce_0: 0.4347  loss_cate_0: 0  loss_mask_0: 0.8454  loss_dice_0: 1.015  loss_ce_1: 0.2645  loss_cate_1: 0  loss_mask_1: 0.8466  loss_dice_1: 1.039  loss_ce_2: 0.2984  loss_cate_2: 0  loss_mask_2: 0.8399  loss_dice_2: 0.9916  loss_ce_3: 0.3091  loss_cate_3: 0  loss_mask_3: 0.8358  loss_dice_3: 0.9923  loss_ce_4: 0.2909  loss_cate_4: 0  loss_mask_4: 0.8375  loss_dice_4: 0.9706  loss_ce_5: 0.3227  loss_cate_5: 0  loss_mask_5: 0.8281  loss_dice_5: 0.9525  loss_ce_6: 0.316  loss_cate_6: 0  loss_mask_6: 0.8298  loss_dice_6: 0.9808  loss_ce_7: 0.3089  loss_cate_7: 0  loss_mask_7: 0.8657  loss_dice_7: 1.022  loss_ce_8: 0.2824  loss_cate_8: 0  loss_mask_8: 0.8379  loss_dice_8: 1.007  time: 1.1027  data_time: 0.0141  lr: 8.9703e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:37:31 d2.utils.events]: \u001b[0m eta: 21:37:27  iter: 9119  total_loss: 18.71  loss_ce: 0.2335  loss_cate: 0.1318  loss_mask: 0.6682  loss_dice: 0.9164  loss_ce_0: 0.3221  loss_cate_0: 0  loss_mask_0: 0.7471  loss_dice_0: 0.9514  loss_ce_1: 0.2319  loss_cate_1: 0  loss_mask_1: 0.7245  loss_dice_1: 0.9663  loss_ce_2: 0.2031  loss_cate_2: 0  loss_mask_2: 0.6949  loss_dice_2: 0.9249  loss_ce_3: 0.2203  loss_cate_3: 0  loss_mask_3: 0.7142  loss_dice_3: 0.9098  loss_ce_4: 0.2198  loss_cate_4: 0  loss_mask_4: 0.7165  loss_dice_4: 0.9194  loss_ce_5: 0.1936  loss_cate_5: 0  loss_mask_5: 0.6944  loss_dice_5: 0.869  loss_ce_6: 0.1644  loss_cate_6: 0  loss_mask_6: 0.7044  loss_dice_6: 0.9129  loss_ce_7: 0.1679  loss_cate_7: 0  loss_mask_7: 0.7048  loss_dice_7: 0.8898  loss_ce_8: 0.2908  loss_cate_8: 0  loss_mask_8: 0.6782  loss_dice_8: 0.8704  time: 1.1027  data_time: 0.0125  lr: 8.968e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:37:54 d2.utils.events]: \u001b[0m eta: 21:37:05  iter: 9139  total_loss: 20.47  loss_ce: 0.1981  loss_cate: 0.1488  loss_mask: 0.8769  loss_dice: 0.9635  loss_ce_0: 0.3236  loss_cate_0: 0  loss_mask_0: 0.8793  loss_dice_0: 1.026  loss_ce_1: 0.2603  loss_cate_1: 0  loss_mask_1: 0.8704  loss_dice_1: 0.9284  loss_ce_2: 0.1913  loss_cate_2: 0  loss_mask_2: 0.8932  loss_dice_2: 0.9732  loss_ce_3: 0.1634  loss_cate_3: 0  loss_mask_3: 0.8687  loss_dice_3: 0.9302  loss_ce_4: 0.1899  loss_cate_4: 0  loss_mask_4: 0.8623  loss_dice_4: 0.9307  loss_ce_5: 0.1844  loss_cate_5: 0  loss_mask_5: 0.8745  loss_dice_5: 0.9835  loss_ce_6: 0.1901  loss_cate_6: 0  loss_mask_6: 0.8514  loss_dice_6: 0.9799  loss_ce_7: 0.1948  loss_cate_7: 0  loss_mask_7: 0.8707  loss_dice_7: 0.9768  loss_ce_8: 0.2012  loss_cate_8: 0  loss_mask_8: 0.869  loss_dice_8: 0.9786  time: 1.1027  data_time: 0.0242  lr: 8.9657e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:38:16 d2.utils.events]: \u001b[0m eta: 21:36:47  iter: 9159  total_loss: 20.49  loss_ce: 0.2771  loss_cate: 0.1579  loss_mask: 0.7886  loss_dice: 1.023  loss_ce_0: 0.4121  loss_cate_0: 0  loss_mask_0: 0.7725  loss_dice_0: 1.066  loss_ce_1: 0.3306  loss_cate_1: 0  loss_mask_1: 0.7865  loss_dice_1: 1.004  loss_ce_2: 0.3522  loss_cate_2: 0  loss_mask_2: 0.7949  loss_dice_2: 1.023  loss_ce_3: 0.2582  loss_cate_3: 0  loss_mask_3: 0.7886  loss_dice_3: 1.025  loss_ce_4: 0.3374  loss_cate_4: 0  loss_mask_4: 0.782  loss_dice_4: 1.025  loss_ce_5: 0.2877  loss_cate_5: 0  loss_mask_5: 0.795  loss_dice_5: 1.054  loss_ce_6: 0.2912  loss_cate_6: 0  loss_mask_6: 0.7831  loss_dice_6: 1.003  loss_ce_7: 0.2561  loss_cate_7: 0  loss_mask_7: 0.7968  loss_dice_7: 1.047  loss_ce_8: 0.2691  loss_cate_8: 0  loss_mask_8: 0.7967  loss_dice_8: 1.012  time: 1.1027  data_time: 0.0130  lr: 8.9635e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:38:39 d2.utils.events]: \u001b[0m eta: 21:36:10  iter: 9179  total_loss: 21.11  loss_ce: 0.2608  loss_cate: 0.149  loss_mask: 0.8964  loss_dice: 1.024  loss_ce_0: 0.3527  loss_cate_0: 0  loss_mask_0: 0.9054  loss_dice_0: 1.015  loss_ce_1: 0.2243  loss_cate_1: 0  loss_mask_1: 0.9065  loss_dice_1: 1.117  loss_ce_2: 0.2056  loss_cate_2: 0  loss_mask_2: 0.8696  loss_dice_2: 1.061  loss_ce_3: 0.2379  loss_cate_3: 0  loss_mask_3: 0.8528  loss_dice_3: 1.015  loss_ce_4: 0.2248  loss_cate_4: 0  loss_mask_4: 0.8396  loss_dice_4: 1.038  loss_ce_5: 0.2737  loss_cate_5: 0  loss_mask_5: 0.8698  loss_dice_5: 1.074  loss_ce_6: 0.2561  loss_cate_6: 0  loss_mask_6: 0.9018  loss_dice_6: 1.051  loss_ce_7: 0.251  loss_cate_7: 0  loss_mask_7: 0.891  loss_dice_7: 1.049  loss_ce_8: 0.2503  loss_cate_8: 0  loss_mask_8: 0.8953  loss_dice_8: 1.058  time: 1.1027  data_time: 0.0133  lr: 8.9612e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:39:01 d2.utils.events]: \u001b[0m eta: 21:35:48  iter: 9199  total_loss: 20.89  loss_ce: 0.1668  loss_cate: 0.148  loss_mask: 0.8378  loss_dice: 1.042  loss_ce_0: 0.2946  loss_cate_0: 0  loss_mask_0: 0.8366  loss_dice_0: 1.002  loss_ce_1: 0.2258  loss_cate_1: 0  loss_mask_1: 0.8534  loss_dice_1: 0.9916  loss_ce_2: 0.2176  loss_cate_2: 0  loss_mask_2: 0.8393  loss_dice_2: 0.9656  loss_ce_3: 0.2058  loss_cate_3: 0  loss_mask_3: 0.8211  loss_dice_3: 0.9761  loss_ce_4: 0.1837  loss_cate_4: 0  loss_mask_4: 0.8219  loss_dice_4: 0.9696  loss_ce_5: 0.1993  loss_cate_5: 0  loss_mask_5: 0.8264  loss_dice_5: 0.9838  loss_ce_6: 0.1743  loss_cate_6: 0  loss_mask_6: 0.8437  loss_dice_6: 1.007  loss_ce_7: 0.159  loss_cate_7: 0  loss_mask_7: 0.8495  loss_dice_7: 1.018  loss_ce_8: 0.206  loss_cate_8: 0  loss_mask_8: 0.8332  loss_dice_8: 1.031  time: 1.1026  data_time: 0.0126  lr: 8.9589e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:39:23 d2.utils.events]: \u001b[0m eta: 21:35:28  iter: 9219  total_loss: 19.51  loss_ce: 0.2547  loss_cate: 0.1481  loss_mask: 0.7341  loss_dice: 0.9465  loss_ce_0: 0.4003  loss_cate_0: 0  loss_mask_0: 0.7745  loss_dice_0: 1.009  loss_ce_1: 0.2316  loss_cate_1: 0  loss_mask_1: 0.7576  loss_dice_1: 0.9527  loss_ce_2: 0.282  loss_cate_2: 0  loss_mask_2: 0.7432  loss_dice_2: 0.927  loss_ce_3: 0.2685  loss_cate_3: 0  loss_mask_3: 0.7497  loss_dice_3: 0.9467  loss_ce_4: 0.2912  loss_cate_4: 0  loss_mask_4: 0.756  loss_dice_4: 0.9226  loss_ce_5: 0.2958  loss_cate_5: 0  loss_mask_5: 0.752  loss_dice_5: 0.9738  loss_ce_6: 0.243  loss_cate_6: 0  loss_mask_6: 0.7707  loss_dice_6: 0.9185  loss_ce_7: 0.2825  loss_cate_7: 0  loss_mask_7: 0.7603  loss_dice_7: 0.9459  loss_ce_8: 0.2568  loss_cate_8: 0  loss_mask_8: 0.7422  loss_dice_8: 0.9435  time: 1.1026  data_time: 0.0128  lr: 8.9566e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:39:45 d2.utils.events]: \u001b[0m eta: 21:35:08  iter: 9239  total_loss: 19  loss_ce: 0.18  loss_cate: 0.1639  loss_mask: 0.7635  loss_dice: 0.9414  loss_ce_0: 0.4296  loss_cate_0: 0  loss_mask_0: 0.7808  loss_dice_0: 1.008  loss_ce_1: 0.3004  loss_cate_1: 0  loss_mask_1: 0.7693  loss_dice_1: 0.9543  loss_ce_2: 0.2227  loss_cate_2: 0  loss_mask_2: 0.7756  loss_dice_2: 0.9644  loss_ce_3: 0.2306  loss_cate_3: 0  loss_mask_3: 0.7446  loss_dice_3: 0.9614  loss_ce_4: 0.1882  loss_cate_4: 0  loss_mask_4: 0.7494  loss_dice_4: 0.9737  loss_ce_5: 0.231  loss_cate_5: 0  loss_mask_5: 0.7351  loss_dice_5: 0.9394  loss_ce_6: 0.2039  loss_cate_6: 0  loss_mask_6: 0.7739  loss_dice_6: 0.9038  loss_ce_7: 0.1914  loss_cate_7: 0  loss_mask_7: 0.7744  loss_dice_7: 0.9465  loss_ce_8: 0.1758  loss_cate_8: 0  loss_mask_8: 0.7818  loss_dice_8: 0.9552  time: 1.1026  data_time: 0.0128  lr: 8.9543e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:40:08 d2.utils.events]: \u001b[0m eta: 21:34:48  iter: 9259  total_loss: 22.53  loss_ce: 0.2618  loss_cate: 0.173  loss_mask: 0.8749  loss_dice: 1.063  loss_ce_0: 0.4188  loss_cate_0: 0  loss_mask_0: 0.9351  loss_dice_0: 1.068  loss_ce_1: 0.2723  loss_cate_1: 0  loss_mask_1: 0.8891  loss_dice_1: 1.082  loss_ce_2: 0.2749  loss_cate_2: 0  loss_mask_2: 0.8197  loss_dice_2: 1.061  loss_ce_3: 0.2559  loss_cate_3: 0  loss_mask_3: 0.8657  loss_dice_3: 1.113  loss_ce_4: 0.2201  loss_cate_4: 0  loss_mask_4: 0.8514  loss_dice_4: 1.075  loss_ce_5: 0.2392  loss_cate_5: 0  loss_mask_5: 0.8779  loss_dice_5: 1.029  loss_ce_6: 0.2575  loss_cate_6: 0  loss_mask_6: 0.8586  loss_dice_6: 1.014  loss_ce_7: 0.2092  loss_cate_7: 0  loss_mask_7: 0.8743  loss_dice_7: 1.025  loss_ce_8: 0.2582  loss_cate_8: 0  loss_mask_8: 0.8743  loss_dice_8: 1.036  time: 1.1026  data_time: 0.0129  lr: 8.9521e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:40:30 d2.utils.events]: \u001b[0m eta: 21:34:32  iter: 9279  total_loss: 22.76  loss_ce: 0.1711  loss_cate: 0.1486  loss_mask: 0.8969  loss_dice: 1.085  loss_ce_0: 0.3143  loss_cate_0: 0  loss_mask_0: 0.9428  loss_dice_0: 1.122  loss_ce_1: 0.1907  loss_cate_1: 0  loss_mask_1: 0.9047  loss_dice_1: 1.072  loss_ce_2: 0.173  loss_cate_2: 0  loss_mask_2: 0.8761  loss_dice_2: 1.052  loss_ce_3: 0.1429  loss_cate_3: 0  loss_mask_3: 0.8945  loss_dice_3: 1.088  loss_ce_4: 0.1054  loss_cate_4: 0  loss_mask_4: 0.8741  loss_dice_4: 1.063  loss_ce_5: 0.1458  loss_cate_5: 0  loss_mask_5: 0.9144  loss_dice_5: 1.066  loss_ce_6: 0.1491  loss_cate_6: 0  loss_mask_6: 0.9034  loss_dice_6: 1.063  loss_ce_7: 0.1274  loss_cate_7: 0  loss_mask_7: 0.9154  loss_dice_7: 1.06  loss_ce_8: 0.158  loss_cate_8: 0  loss_mask_8: 0.9098  loss_dice_8: 1.094  time: 1.1026  data_time: 0.0132  lr: 8.9498e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:40:54 d2.utils.events]: \u001b[0m eta: 21:34:16  iter: 9299  total_loss: 21.26  loss_ce: 0.3002  loss_cate: 0.1347  loss_mask: 0.7821  loss_dice: 0.98  loss_ce_0: 0.4699  loss_cate_0: 0  loss_mask_0: 0.8649  loss_dice_0: 1.111  loss_ce_1: 0.3496  loss_cate_1: 0  loss_mask_1: 0.7959  loss_dice_1: 0.9719  loss_ce_2: 0.3618  loss_cate_2: 0  loss_mask_2: 0.8005  loss_dice_2: 0.9883  loss_ce_3: 0.3293  loss_cate_3: 0  loss_mask_3: 0.7841  loss_dice_3: 0.9444  loss_ce_4: 0.3228  loss_cate_4: 0  loss_mask_4: 0.7949  loss_dice_4: 0.9741  loss_ce_5: 0.2937  loss_cate_5: 0  loss_mask_5: 0.8236  loss_dice_5: 0.9699  loss_ce_6: 0.2906  loss_cate_6: 0  loss_mask_6: 0.7697  loss_dice_6: 0.9483  loss_ce_7: 0.294  loss_cate_7: 0  loss_mask_7: 0.7639  loss_dice_7: 0.9768  loss_ce_8: 0.2891  loss_cate_8: 0  loss_mask_8: 0.7713  loss_dice_8: 0.96  time: 1.1026  data_time: 0.0129  lr: 8.9475e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:41:16 d2.utils.events]: \u001b[0m eta: 21:33:53  iter: 9319  total_loss: 17.95  loss_ce: 0.1523  loss_cate: 0.1392  loss_mask: 0.7309  loss_dice: 0.9278  loss_ce_0: 0.278  loss_cate_0: 0  loss_mask_0: 0.7327  loss_dice_0: 0.9571  loss_ce_1: 0.1655  loss_cate_1: 0  loss_mask_1: 0.7165  loss_dice_1: 0.9445  loss_ce_2: 0.1903  loss_cate_2: 0  loss_mask_2: 0.733  loss_dice_2: 0.9345  loss_ce_3: 0.1373  loss_cate_3: 0  loss_mask_3: 0.7282  loss_dice_3: 0.9642  loss_ce_4: 0.1205  loss_cate_4: 0  loss_mask_4: 0.728  loss_dice_4: 0.9332  loss_ce_5: 0.1671  loss_cate_5: 0  loss_mask_5: 0.7493  loss_dice_5: 0.914  loss_ce_6: 0.1205  loss_cate_6: 0  loss_mask_6: 0.7275  loss_dice_6: 0.9598  loss_ce_7: 0.1285  loss_cate_7: 0  loss_mask_7: 0.7238  loss_dice_7: 0.9524  loss_ce_8: 0.1206  loss_cate_8: 0  loss_mask_8: 0.7244  loss_dice_8: 0.9239  time: 1.1026  data_time: 0.0134  lr: 8.9452e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:41:38 d2.utils.events]: \u001b[0m eta: 21:33:34  iter: 9339  total_loss: 21.12  loss_ce: 0.2029  loss_cate: 0.1725  loss_mask: 0.9563  loss_dice: 1.007  loss_ce_0: 0.2889  loss_cate_0: 0  loss_mask_0: 0.9365  loss_dice_0: 1.036  loss_ce_1: 0.2228  loss_cate_1: 0  loss_mask_1: 0.9315  loss_dice_1: 0.9978  loss_ce_2: 0.2089  loss_cate_2: 0  loss_mask_2: 0.9446  loss_dice_2: 1.008  loss_ce_3: 0.1406  loss_cate_3: 0  loss_mask_3: 0.9528  loss_dice_3: 1.013  loss_ce_4: 0.158  loss_cate_4: 0  loss_mask_4: 0.934  loss_dice_4: 1.005  loss_ce_5: 0.2076  loss_cate_5: 0  loss_mask_5: 0.9401  loss_dice_5: 0.997  loss_ce_6: 0.1492  loss_cate_6: 0  loss_mask_6: 0.9378  loss_dice_6: 1.018  loss_ce_7: 0.1797  loss_cate_7: 0  loss_mask_7: 0.934  loss_dice_7: 1.001  loss_ce_8: 0.1873  loss_cate_8: 0  loss_mask_8: 0.9461  loss_dice_8: 1.026  time: 1.1026  data_time: 0.0133  lr: 8.943e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:42:01 d2.utils.events]: \u001b[0m eta: 21:33:13  iter: 9359  total_loss: 21.31  loss_ce: 0.2472  loss_cate: 0.1402  loss_mask: 0.8249  loss_dice: 1.041  loss_ce_0: 0.3946  loss_cate_0: 0  loss_mask_0: 0.7892  loss_dice_0: 1.061  loss_ce_1: 0.2586  loss_cate_1: 0  loss_mask_1: 0.7886  loss_dice_1: 1.034  loss_ce_2: 0.2329  loss_cate_2: 0  loss_mask_2: 0.8215  loss_dice_2: 1.023  loss_ce_3: 0.2197  loss_cate_3: 0  loss_mask_3: 0.8229  loss_dice_3: 1.051  loss_ce_4: 0.2258  loss_cate_4: 0  loss_mask_4: 0.8273  loss_dice_4: 1.044  loss_ce_5: 0.2365  loss_cate_5: 0  loss_mask_5: 0.807  loss_dice_5: 1.022  loss_ce_6: 0.2372  loss_cate_6: 0  loss_mask_6: 0.832  loss_dice_6: 1.019  loss_ce_7: 0.2245  loss_cate_7: 0  loss_mask_7: 0.837  loss_dice_7: 1.023  loss_ce_8: 0.2315  loss_cate_8: 0  loss_mask_8: 0.8265  loss_dice_8: 1.004  time: 1.1026  data_time: 0.0135  lr: 8.9407e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:42:26 d2.utils.events]: \u001b[0m eta: 21:32:46  iter: 9379  total_loss: 18.81  loss_ce: 0.1923  loss_cate: 0.1447  loss_mask: 0.6428  loss_dice: 0.9533  loss_ce_0: 0.3219  loss_cate_0: 0  loss_mask_0: 0.7551  loss_dice_0: 0.9955  loss_ce_1: 0.238  loss_cate_1: 0  loss_mask_1: 0.676  loss_dice_1: 0.9701  loss_ce_2: 0.2655  loss_cate_2: 0  loss_mask_2: 0.7229  loss_dice_2: 0.9608  loss_ce_3: 0.1974  loss_cate_3: 0  loss_mask_3: 0.6881  loss_dice_3: 0.993  loss_ce_4: 0.2491  loss_cate_4: 0  loss_mask_4: 0.7248  loss_dice_4: 0.9429  loss_ce_5: 0.1698  loss_cate_5: 0  loss_mask_5: 0.6437  loss_dice_5: 0.9012  loss_ce_6: 0.2371  loss_cate_6: 0  loss_mask_6: 0.6327  loss_dice_6: 0.9114  loss_ce_7: 0.199  loss_cate_7: 0  loss_mask_7: 0.6416  loss_dice_7: 0.9012  loss_ce_8: 0.1873  loss_cate_8: 0  loss_mask_8: 0.631  loss_dice_8: 0.9376  time: 1.1026  data_time: 0.0131  lr: 8.9384e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:42:48 d2.utils.events]: \u001b[0m eta: 21:32:24  iter: 9399  total_loss: 18.89  loss_ce: 0.2061  loss_cate: 0.1249  loss_mask: 0.7035  loss_dice: 0.8845  loss_ce_0: 0.3338  loss_cate_0: 0  loss_mask_0: 0.7444  loss_dice_0: 0.9737  loss_ce_1: 0.2585  loss_cate_1: 0  loss_mask_1: 0.7316  loss_dice_1: 0.9661  loss_ce_2: 0.1951  loss_cate_2: 0  loss_mask_2: 0.7344  loss_dice_2: 0.9047  loss_ce_3: 0.2161  loss_cate_3: 0  loss_mask_3: 0.7519  loss_dice_3: 0.8983  loss_ce_4: 0.2211  loss_cate_4: 0  loss_mask_4: 0.7437  loss_dice_4: 0.9008  loss_ce_5: 0.2264  loss_cate_5: 0  loss_mask_5: 0.738  loss_dice_5: 0.8742  loss_ce_6: 0.2181  loss_cate_6: 0  loss_mask_6: 0.712  loss_dice_6: 0.8792  loss_ce_7: 0.2208  loss_cate_7: 0  loss_mask_7: 0.702  loss_dice_7: 0.8952  loss_ce_8: 0.219  loss_cate_8: 0  loss_mask_8: 0.6915  loss_dice_8: 0.9063  time: 1.1026  data_time: 0.0116  lr: 8.9361e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:43:11 d2.utils.events]: \u001b[0m eta: 21:32:07  iter: 9419  total_loss: 23.76  loss_ce: 0.3249  loss_cate: 0.1589  loss_mask: 0.8724  loss_dice: 1.156  loss_ce_0: 0.5062  loss_cate_0: 0  loss_mask_0: 0.8649  loss_dice_0: 1.197  loss_ce_1: 0.3348  loss_cate_1: 0  loss_mask_1: 0.8566  loss_dice_1: 1.154  loss_ce_2: 0.307  loss_cate_2: 0  loss_mask_2: 0.8534  loss_dice_2: 1.174  loss_ce_3: 0.3538  loss_cate_3: 0  loss_mask_3: 0.835  loss_dice_3: 1.174  loss_ce_4: 0.3196  loss_cate_4: 0  loss_mask_4: 0.841  loss_dice_4: 1.181  loss_ce_5: 0.2813  loss_cate_5: 0  loss_mask_5: 0.8728  loss_dice_5: 1.18  loss_ce_6: 0.3366  loss_cate_6: 0  loss_mask_6: 0.8521  loss_dice_6: 1.122  loss_ce_7: 0.283  loss_cate_7: 0  loss_mask_7: 0.8959  loss_dice_7: 1.164  loss_ce_8: 0.3152  loss_cate_8: 0  loss_mask_8: 0.9134  loss_dice_8: 1.171  time: 1.1026  data_time: 0.0138  lr: 8.9338e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:43:33 d2.utils.events]: \u001b[0m eta: 21:31:43  iter: 9439  total_loss: 21.93  loss_ce: 0.3125  loss_cate: 0.1645  loss_mask: 0.8495  loss_dice: 1.019  loss_ce_0: 0.3358  loss_cate_0: 0  loss_mask_0: 0.9571  loss_dice_0: 1.091  loss_ce_1: 0.3511  loss_cate_1: 0  loss_mask_1: 0.9033  loss_dice_1: 1.081  loss_ce_2: 0.3045  loss_cate_2: 0  loss_mask_2: 0.8617  loss_dice_2: 0.9965  loss_ce_3: 0.299  loss_cate_3: 0  loss_mask_3: 0.8766  loss_dice_3: 1.056  loss_ce_4: 0.3148  loss_cate_4: 0  loss_mask_4: 0.9173  loss_dice_4: 1.056  loss_ce_5: 0.3312  loss_cate_5: 0  loss_mask_5: 0.8677  loss_dice_5: 1.004  loss_ce_6: 0.2959  loss_cate_6: 0  loss_mask_6: 0.8877  loss_dice_6: 1.016  loss_ce_7: 0.3211  loss_cate_7: 0  loss_mask_7: 0.8475  loss_dice_7: 1.04  loss_ce_8: 0.3219  loss_cate_8: 0  loss_mask_8: 0.8488  loss_dice_8: 0.9927  time: 1.1026  data_time: 0.0123  lr: 8.9316e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:43:56 d2.utils.events]: \u001b[0m eta: 21:31:28  iter: 9459  total_loss: 20.56  loss_ce: 0.1514  loss_cate: 0.1383  loss_mask: 0.7746  loss_dice: 1.037  loss_ce_0: 0.2903  loss_cate_0: 0  loss_mask_0: 0.7901  loss_dice_0: 1.049  loss_ce_1: 0.1736  loss_cate_1: 0  loss_mask_1: 0.7653  loss_dice_1: 1.028  loss_ce_2: 0.1605  loss_cate_2: 0  loss_mask_2: 0.7682  loss_dice_2: 1.066  loss_ce_3: 0.1495  loss_cate_3: 0  loss_mask_3: 0.767  loss_dice_3: 0.9794  loss_ce_4: 0.1963  loss_cate_4: 0  loss_mask_4: 0.7725  loss_dice_4: 0.9868  loss_ce_5: 0.1467  loss_cate_5: 0  loss_mask_5: 0.7802  loss_dice_5: 1.041  loss_ce_6: 0.1747  loss_cate_6: 0  loss_mask_6: 0.7756  loss_dice_6: 1.049  loss_ce_7: 0.1886  loss_cate_7: 0  loss_mask_7: 0.7745  loss_dice_7: 1.036  loss_ce_8: 0.1946  loss_cate_8: 0  loss_mask_8: 0.7808  loss_dice_8: 1.008  time: 1.1026  data_time: 0.0123  lr: 8.9293e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:44:19 d2.utils.events]: \u001b[0m eta: 21:31:05  iter: 9479  total_loss: 19.26  loss_ce: 0.2362  loss_cate: 0.1459  loss_mask: 0.7586  loss_dice: 0.9277  loss_ce_0: 0.348  loss_cate_0: 0  loss_mask_0: 0.7702  loss_dice_0: 0.9485  loss_ce_1: 0.2718  loss_cate_1: 0  loss_mask_1: 0.7637  loss_dice_1: 0.882  loss_ce_2: 0.3204  loss_cate_2: 0  loss_mask_2: 0.7838  loss_dice_2: 0.8693  loss_ce_3: 0.217  loss_cate_3: 0  loss_mask_3: 0.7724  loss_dice_3: 0.8705  loss_ce_4: 0.243  loss_cate_4: 0  loss_mask_4: 0.7781  loss_dice_4: 0.8736  loss_ce_5: 0.1806  loss_cate_5: 0  loss_mask_5: 0.7604  loss_dice_5: 0.931  loss_ce_6: 0.2041  loss_cate_6: 0  loss_mask_6: 0.7604  loss_dice_6: 0.8841  loss_ce_7: 0.2593  loss_cate_7: 0  loss_mask_7: 0.7744  loss_dice_7: 0.8545  loss_ce_8: 0.2158  loss_cate_8: 0  loss_mask_8: 0.7641  loss_dice_8: 0.9021  time: 1.1026  data_time: 0.0138  lr: 8.927e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:44:41 d2.utils.events]: \u001b[0m eta: 21:30:43  iter: 9499  total_loss: 19.96  loss_ce: 0.1704  loss_cate: 0.1589  loss_mask: 0.7301  loss_dice: 0.9365  loss_ce_0: 0.2434  loss_cate_0: 0  loss_mask_0: 0.7769  loss_dice_0: 1.057  loss_ce_1: 0.1931  loss_cate_1: 0  loss_mask_1: 0.7111  loss_dice_1: 0.9611  loss_ce_2: 0.1932  loss_cate_2: 0  loss_mask_2: 0.7554  loss_dice_2: 0.9401  loss_ce_3: 0.2238  loss_cate_3: 0  loss_mask_3: 0.7002  loss_dice_3: 0.9713  loss_ce_4: 0.1482  loss_cate_4: 0  loss_mask_4: 0.7354  loss_dice_4: 0.9649  loss_ce_5: 0.1024  loss_cate_5: 0  loss_mask_5: 0.7199  loss_dice_5: 0.937  loss_ce_6: 0.1422  loss_cate_6: 0  loss_mask_6: 0.7155  loss_dice_6: 0.9236  loss_ce_7: 0.2031  loss_cate_7: 0  loss_mask_7: 0.72  loss_dice_7: 0.9612  loss_ce_8: 0.1714  loss_cate_8: 0  loss_mask_8: 0.7274  loss_dice_8: 0.9502  time: 1.1026  data_time: 0.0131  lr: 8.9247e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:45:03 d2.utils.events]: \u001b[0m eta: 21:30:21  iter: 9519  total_loss: 19.63  loss_ce: 0.2439  loss_cate: 0.1434  loss_mask: 0.6729  loss_dice: 0.9901  loss_ce_0: 0.3439  loss_cate_0: 0  loss_mask_0: 0.6951  loss_dice_0: 1.112  loss_ce_1: 0.2169  loss_cate_1: 0  loss_mask_1: 0.7142  loss_dice_1: 1.058  loss_ce_2: 0.2323  loss_cate_2: 0  loss_mask_2: 0.6802  loss_dice_2: 1.004  loss_ce_3: 0.2199  loss_cate_3: 0  loss_mask_3: 0.6803  loss_dice_3: 1.044  loss_ce_4: 0.2419  loss_cate_4: 0  loss_mask_4: 0.6754  loss_dice_4: 1.021  loss_ce_5: 0.2143  loss_cate_5: 0  loss_mask_5: 0.669  loss_dice_5: 1.029  loss_ce_6: 0.2177  loss_cate_6: 0  loss_mask_6: 0.6838  loss_dice_6: 1.006  loss_ce_7: 0.2148  loss_cate_7: 0  loss_mask_7: 0.6843  loss_dice_7: 1.015  loss_ce_8: 0.2313  loss_cate_8: 0  loss_mask_8: 0.6834  loss_dice_8: 1.043  time: 1.1026  data_time: 0.0134  lr: 8.9224e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:45:26 d2.utils.events]: \u001b[0m eta: 21:30:02  iter: 9539  total_loss: 20.61  loss_ce: 0.2288  loss_cate: 0.1438  loss_mask: 0.6625  loss_dice: 0.9341  loss_ce_0: 0.2634  loss_cate_0: 0  loss_mask_0: 0.6977  loss_dice_0: 0.9847  loss_ce_1: 0.2281  loss_cate_1: 0  loss_mask_1: 0.68  loss_dice_1: 0.904  loss_ce_2: 0.2114  loss_cate_2: 0  loss_mask_2: 0.6725  loss_dice_2: 0.9146  loss_ce_3: 0.2075  loss_cate_3: 0  loss_mask_3: 0.6839  loss_dice_3: 0.9021  loss_ce_4: 0.2003  loss_cate_4: 0  loss_mask_4: 0.6769  loss_dice_4: 0.9257  loss_ce_5: 0.1561  loss_cate_5: 0  loss_mask_5: 0.6701  loss_dice_5: 0.9086  loss_ce_6: 0.2234  loss_cate_6: 0  loss_mask_6: 0.6658  loss_dice_6: 0.9349  loss_ce_7: 0.2037  loss_cate_7: 0  loss_mask_7: 0.6729  loss_dice_7: 0.9305  loss_ce_8: 0.2259  loss_cate_8: 0  loss_mask_8: 0.6727  loss_dice_8: 0.9152  time: 1.1026  data_time: 0.0142  lr: 8.9202e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:45:48 d2.utils.events]: \u001b[0m eta: 21:29:40  iter: 9559  total_loss: 22.46  loss_ce: 0.3398  loss_cate: 0.1691  loss_mask: 0.9193  loss_dice: 1.073  loss_ce_0: 0.4262  loss_cate_0: 0  loss_mask_0: 0.959  loss_dice_0: 1.146  loss_ce_1: 0.3568  loss_cate_1: 0  loss_mask_1: 0.8099  loss_dice_1: 1.135  loss_ce_2: 0.3399  loss_cate_2: 0  loss_mask_2: 0.9634  loss_dice_2: 1.111  loss_ce_3: 0.3139  loss_cate_3: 0  loss_mask_3: 0.8997  loss_dice_3: 1.095  loss_ce_4: 0.3617  loss_cate_4: 0  loss_mask_4: 0.9211  loss_dice_4: 1.051  loss_ce_5: 0.2879  loss_cate_5: 0  loss_mask_5: 0.8365  loss_dice_5: 1.045  loss_ce_6: 0.3005  loss_cate_6: 0  loss_mask_6: 0.901  loss_dice_6: 1.052  loss_ce_7: 0.3216  loss_cate_7: 0  loss_mask_7: 0.9179  loss_dice_7: 1.058  loss_ce_8: 0.2944  loss_cate_8: 0  loss_mask_8: 0.9102  loss_dice_8: 1.075  time: 1.1025  data_time: 0.0129  lr: 8.9179e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:46:14 d2.utils.events]: \u001b[0m eta: 21:29:26  iter: 9579  total_loss: 20.04  loss_ce: 0.205  loss_cate: 0.153  loss_mask: 0.7262  loss_dice: 0.9812  loss_ce_0: 0.3279  loss_cate_0: 0  loss_mask_0: 0.7714  loss_dice_0: 0.986  loss_ce_1: 0.2823  loss_cate_1: 0  loss_mask_1: 0.7589  loss_dice_1: 0.9681  loss_ce_2: 0.2585  loss_cate_2: 0  loss_mask_2: 0.7123  loss_dice_2: 0.9349  loss_ce_3: 0.2171  loss_cate_3: 0  loss_mask_3: 0.7225  loss_dice_3: 0.9917  loss_ce_4: 0.2245  loss_cate_4: 0  loss_mask_4: 0.7427  loss_dice_4: 0.9492  loss_ce_5: 0.2078  loss_cate_5: 0  loss_mask_5: 0.7053  loss_dice_5: 0.9859  loss_ce_6: 0.1771  loss_cate_6: 0  loss_mask_6: 0.7254  loss_dice_6: 0.9778  loss_ce_7: 0.2149  loss_cate_7: 0  loss_mask_7: 0.7138  loss_dice_7: 0.971  loss_ce_8: 0.2379  loss_cate_8: 0  loss_mask_8: 0.7222  loss_dice_8: 0.9562  time: 1.1025  data_time: 0.0142  lr: 8.9156e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:46:36 d2.utils.events]: \u001b[0m eta: 21:29:09  iter: 9599  total_loss: 20.96  loss_ce: 0.1759  loss_cate: 0.1459  loss_mask: 0.6987  loss_dice: 1.085  loss_ce_0: 0.3091  loss_cate_0: 0  loss_mask_0: 0.7151  loss_dice_0: 1.104  loss_ce_1: 0.2615  loss_cate_1: 0  loss_mask_1: 0.7447  loss_dice_1: 1.058  loss_ce_2: 0.2324  loss_cate_2: 0  loss_mask_2: 0.728  loss_dice_2: 1.045  loss_ce_3: 0.1998  loss_cate_3: 0  loss_mask_3: 0.7486  loss_dice_3: 1.023  loss_ce_4: 0.2259  loss_cate_4: 0  loss_mask_4: 0.7307  loss_dice_4: 1.023  loss_ce_5: 0.205  loss_cate_5: 0  loss_mask_5: 0.7152  loss_dice_5: 1.02  loss_ce_6: 0.2285  loss_cate_6: 0  loss_mask_6: 0.7099  loss_dice_6: 1.048  loss_ce_7: 0.152  loss_cate_7: 0  loss_mask_7: 0.7152  loss_dice_7: 1.086  loss_ce_8: 0.1553  loss_cate_8: 0  loss_mask_8: 0.7168  loss_dice_8: 1.106  time: 1.1026  data_time: 0.0140  lr: 8.9133e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:46:59 d2.utils.events]: \u001b[0m eta: 21:28:48  iter: 9619  total_loss: 20.92  loss_ce: 0.1856  loss_cate: 0.143  loss_mask: 0.7872  loss_dice: 1.004  loss_ce_0: 0.3476  loss_cate_0: 0  loss_mask_0: 0.8009  loss_dice_0: 1.081  loss_ce_1: 0.2301  loss_cate_1: 0  loss_mask_1: 0.7946  loss_dice_1: 0.9784  loss_ce_2: 0.2091  loss_cate_2: 0  loss_mask_2: 0.8021  loss_dice_2: 1.036  loss_ce_3: 0.2301  loss_cate_3: 0  loss_mask_3: 0.7449  loss_dice_3: 0.9966  loss_ce_4: 0.2313  loss_cate_4: 0  loss_mask_4: 0.7417  loss_dice_4: 1.049  loss_ce_5: 0.2237  loss_cate_5: 0  loss_mask_5: 0.754  loss_dice_5: 1.002  loss_ce_6: 0.2353  loss_cate_6: 0  loss_mask_6: 0.7419  loss_dice_6: 1.007  loss_ce_7: 0.204  loss_cate_7: 0  loss_mask_7: 0.7441  loss_dice_7: 1.008  loss_ce_8: 0.1928  loss_cate_8: 0  loss_mask_8: 0.7876  loss_dice_8: 1.042  time: 1.1026  data_time: 0.0143  lr: 8.9111e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:47:21 d2.utils.events]: \u001b[0m eta: 21:28:26  iter: 9639  total_loss: 18.62  loss_ce: 0.1614  loss_cate: 0.1495  loss_mask: 0.6992  loss_dice: 0.8323  loss_ce_0: 0.2666  loss_cate_0: 0  loss_mask_0: 0.7501  loss_dice_0: 0.9755  loss_ce_1: 0.1795  loss_cate_1: 0  loss_mask_1: 0.7927  loss_dice_1: 0.9028  loss_ce_2: 0.1403  loss_cate_2: 0  loss_mask_2: 0.7457  loss_dice_2: 0.9413  loss_ce_3: 0.1959  loss_cate_3: 0  loss_mask_3: 0.7345  loss_dice_3: 0.8225  loss_ce_4: 0.1333  loss_cate_4: 0  loss_mask_4: 0.7274  loss_dice_4: 0.8891  loss_ce_5: 0.1932  loss_cate_5: 0  loss_mask_5: 0.7224  loss_dice_5: 0.8215  loss_ce_6: 0.2283  loss_cate_6: 0  loss_mask_6: 0.7154  loss_dice_6: 0.8293  loss_ce_7: 0.2445  loss_cate_7: 0  loss_mask_7: 0.7129  loss_dice_7: 0.8309  loss_ce_8: 0.1554  loss_cate_8: 0  loss_mask_8: 0.7143  loss_dice_8: 0.8533  time: 1.1026  data_time: 0.0126  lr: 8.9088e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:47:43 d2.utils.events]: \u001b[0m eta: 21:28:12  iter: 9659  total_loss: 21.88  loss_ce: 0.268  loss_cate: 0.1441  loss_mask: 0.8113  loss_dice: 0.9577  loss_ce_0: 0.4715  loss_cate_0: 0  loss_mask_0: 0.8266  loss_dice_0: 1.021  loss_ce_1: 0.2745  loss_cate_1: 0  loss_mask_1: 0.8499  loss_dice_1: 0.9884  loss_ce_2: 0.2191  loss_cate_2: 0  loss_mask_2: 0.8235  loss_dice_2: 0.9995  loss_ce_3: 0.2388  loss_cate_3: 0  loss_mask_3: 0.8291  loss_dice_3: 0.9673  loss_ce_4: 0.2444  loss_cate_4: 0  loss_mask_4: 0.8262  loss_dice_4: 0.9511  loss_ce_5: 0.2322  loss_cate_5: 0  loss_mask_5: 0.8289  loss_dice_5: 0.9752  loss_ce_6: 0.2423  loss_cate_6: 0  loss_mask_6: 0.8193  loss_dice_6: 0.925  loss_ce_7: 0.1939  loss_cate_7: 0  loss_mask_7: 0.8133  loss_dice_7: 0.9725  loss_ce_8: 0.186  loss_cate_8: 0  loss_mask_8: 0.8073  loss_dice_8: 0.9407  time: 1.1026  data_time: 0.0132  lr: 8.9065e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:48:06 d2.utils.events]: \u001b[0m eta: 21:27:54  iter: 9679  total_loss: 21.22  loss_ce: 0.2049  loss_cate: 0.1623  loss_mask: 0.9085  loss_dice: 1.07  loss_ce_0: 0.3375  loss_cate_0: 0  loss_mask_0: 0.9366  loss_dice_0: 1.008  loss_ce_1: 0.2624  loss_cate_1: 0  loss_mask_1: 0.8799  loss_dice_1: 0.9748  loss_ce_2: 0.2445  loss_cate_2: 0  loss_mask_2: 0.8758  loss_dice_2: 1.035  loss_ce_3: 0.2195  loss_cate_3: 0  loss_mask_3: 0.9203  loss_dice_3: 1.06  loss_ce_4: 0.1768  loss_cate_4: 0  loss_mask_4: 0.9213  loss_dice_4: 1.071  loss_ce_5: 0.203  loss_cate_5: 0  loss_mask_5: 0.9006  loss_dice_5: 1.066  loss_ce_6: 0.1981  loss_cate_6: 0  loss_mask_6: 0.9175  loss_dice_6: 1.047  loss_ce_7: 0.1793  loss_cate_7: 0  loss_mask_7: 0.9168  loss_dice_7: 1.078  loss_ce_8: 0.1698  loss_cate_8: 0  loss_mask_8: 0.9101  loss_dice_8: 1.061  time: 1.1026  data_time: 0.0124  lr: 8.9042e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:48:29 d2.utils.events]: \u001b[0m eta: 21:27:34  iter: 9699  total_loss: 21.08  loss_ce: 0.2  loss_cate: 0.1812  loss_mask: 0.8346  loss_dice: 0.8918  loss_ce_0: 0.3283  loss_cate_0: 0  loss_mask_0: 0.8498  loss_dice_0: 1.068  loss_ce_1: 0.2349  loss_cate_1: 0  loss_mask_1: 0.8367  loss_dice_1: 0.9745  loss_ce_2: 0.2146  loss_cate_2: 0  loss_mask_2: 0.8158  loss_dice_2: 0.9217  loss_ce_3: 0.2079  loss_cate_3: 0  loss_mask_3: 0.857  loss_dice_3: 0.9325  loss_ce_4: 0.2066  loss_cate_4: 0  loss_mask_4: 0.8135  loss_dice_4: 0.8974  loss_ce_5: 0.2159  loss_cate_5: 0  loss_mask_5: 0.766  loss_dice_5: 0.904  loss_ce_6: 0.2144  loss_cate_6: 0  loss_mask_6: 0.8154  loss_dice_6: 0.9005  loss_ce_7: 0.2131  loss_cate_7: 0  loss_mask_7: 0.8458  loss_dice_7: 0.9164  loss_ce_8: 0.2141  loss_cate_8: 0  loss_mask_8: 0.8644  loss_dice_8: 0.923  time: 1.1026  data_time: 0.0135  lr: 8.9019e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:48:51 d2.utils.events]: \u001b[0m eta: 21:27:26  iter: 9719  total_loss: 18.55  loss_ce: 0.2075  loss_cate: 0.1449  loss_mask: 0.7511  loss_dice: 0.9102  loss_ce_0: 0.3987  loss_cate_0: 0  loss_mask_0: 0.7505  loss_dice_0: 0.9084  loss_ce_1: 0.2559  loss_cate_1: 0  loss_mask_1: 0.7306  loss_dice_1: 0.9053  loss_ce_2: 0.2509  loss_cate_2: 0  loss_mask_2: 0.7262  loss_dice_2: 0.8674  loss_ce_3: 0.2275  loss_cate_3: 0  loss_mask_3: 0.7073  loss_dice_3: 0.8888  loss_ce_4: 0.2166  loss_cate_4: 0  loss_mask_4: 0.7392  loss_dice_4: 0.8945  loss_ce_5: 0.2638  loss_cate_5: 0  loss_mask_5: 0.7443  loss_dice_5: 0.8549  loss_ce_6: 0.2196  loss_cate_6: 0  loss_mask_6: 0.7451  loss_dice_6: 0.8767  loss_ce_7: 0.2078  loss_cate_7: 0  loss_mask_7: 0.7541  loss_dice_7: 0.8635  loss_ce_8: 0.186  loss_cate_8: 0  loss_mask_8: 0.7184  loss_dice_8: 0.8714  time: 1.1026  data_time: 0.0137  lr: 8.8997e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:49:14 d2.utils.events]: \u001b[0m eta: 21:27:13  iter: 9739  total_loss: 22.21  loss_ce: 0.2357  loss_cate: 0.1357  loss_mask: 0.8363  loss_dice: 0.9479  loss_ce_0: 0.3507  loss_cate_0: 0  loss_mask_0: 0.8336  loss_dice_0: 1.107  loss_ce_1: 0.2404  loss_cate_1: 0  loss_mask_1: 0.8265  loss_dice_1: 1.015  loss_ce_2: 0.2689  loss_cate_2: 0  loss_mask_2: 0.8318  loss_dice_2: 0.9665  loss_ce_3: 0.2529  loss_cate_3: 0  loss_mask_3: 0.8094  loss_dice_3: 0.9473  loss_ce_4: 0.227  loss_cate_4: 0  loss_mask_4: 0.8386  loss_dice_4: 0.9273  loss_ce_5: 0.1642  loss_cate_5: 0  loss_mask_5: 0.7815  loss_dice_5: 1.007  loss_ce_6: 0.2093  loss_cate_6: 0  loss_mask_6: 0.8005  loss_dice_6: 1.005  loss_ce_7: 0.2303  loss_cate_7: 0  loss_mask_7: 0.806  loss_dice_7: 0.9729  loss_ce_8: 0.2143  loss_cate_8: 0  loss_mask_8: 0.8303  loss_dice_8: 0.9937  time: 1.1026  data_time: 0.0130  lr: 8.8974e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:49:37 d2.utils.events]: \u001b[0m eta: 21:27:05  iter: 9759  total_loss: 19.32  loss_ce: 0.1441  loss_cate: 0.1208  loss_mask: 0.6903  loss_dice: 0.8606  loss_ce_0: 0.2167  loss_cate_0: 0  loss_mask_0: 0.698  loss_dice_0: 0.9226  loss_ce_1: 0.2115  loss_cate_1: 0  loss_mask_1: 0.7019  loss_dice_1: 0.8726  loss_ce_2: 0.1917  loss_cate_2: 0  loss_mask_2: 0.7118  loss_dice_2: 0.8785  loss_ce_3: 0.2201  loss_cate_3: 0  loss_mask_3: 0.6937  loss_dice_3: 0.852  loss_ce_4: 0.192  loss_cate_4: 0  loss_mask_4: 0.7009  loss_dice_4: 0.8917  loss_ce_5: 0.1545  loss_cate_5: 0  loss_mask_5: 0.7121  loss_dice_5: 0.8795  loss_ce_6: 0.1872  loss_cate_6: 0  loss_mask_6: 0.6908  loss_dice_6: 0.859  loss_ce_7: 0.1939  loss_cate_7: 0  loss_mask_7: 0.6927  loss_dice_7: 0.866  loss_ce_8: 0.1211  loss_cate_8: 0  loss_mask_8: 0.6863  loss_dice_8: 0.8533  time: 1.1026  data_time: 0.0138  lr: 8.8951e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:49:59 d2.utils.events]: \u001b[0m eta: 21:26:52  iter: 9779  total_loss: 18.62  loss_ce: 0.2599  loss_cate: 0.1402  loss_mask: 0.7803  loss_dice: 0.8557  loss_ce_0: 0.3421  loss_cate_0: 0  loss_mask_0: 0.8118  loss_dice_0: 0.8976  loss_ce_1: 0.2799  loss_cate_1: 0  loss_mask_1: 0.7863  loss_dice_1: 0.8896  loss_ce_2: 0.3023  loss_cate_2: 0  loss_mask_2: 0.8149  loss_dice_2: 0.8905  loss_ce_3: 0.2891  loss_cate_3: 0  loss_mask_3: 0.7511  loss_dice_3: 0.8666  loss_ce_4: 0.2622  loss_cate_4: 0  loss_mask_4: 0.7428  loss_dice_4: 0.8866  loss_ce_5: 0.2727  loss_cate_5: 0  loss_mask_5: 0.78  loss_dice_5: 0.8694  loss_ce_6: 0.278  loss_cate_6: 0  loss_mask_6: 0.7595  loss_dice_6: 0.8618  loss_ce_7: 0.2641  loss_cate_7: 0  loss_mask_7: 0.7976  loss_dice_7: 0.852  loss_ce_8: 0.2625  loss_cate_8: 0  loss_mask_8: 0.8047  loss_dice_8: 0.8532  time: 1.1026  data_time: 0.0160  lr: 8.8928e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:50:22 d2.utils.events]: \u001b[0m eta: 21:26:41  iter: 9799  total_loss: 22.6  loss_ce: 0.148  loss_cate: 0.1556  loss_mask: 0.8726  loss_dice: 1.095  loss_ce_0: 0.2932  loss_cate_0: 0  loss_mask_0: 0.9246  loss_dice_0: 1.124  loss_ce_1: 0.2294  loss_cate_1: 0  loss_mask_1: 0.8411  loss_dice_1: 1.117  loss_ce_2: 0.1881  loss_cate_2: 0  loss_mask_2: 0.8625  loss_dice_2: 1.04  loss_ce_3: 0.1447  loss_cate_3: 0  loss_mask_3: 0.8383  loss_dice_3: 1.091  loss_ce_4: 0.1661  loss_cate_4: 0  loss_mask_4: 0.838  loss_dice_4: 1.057  loss_ce_5: 0.1941  loss_cate_5: 0  loss_mask_5: 0.8601  loss_dice_5: 1.082  loss_ce_6: 0.2003  loss_cate_6: 0  loss_mask_6: 0.853  loss_dice_6: 1.071  loss_ce_7: 0.176  loss_cate_7: 0  loss_mask_7: 0.8344  loss_dice_7: 1.086  loss_ce_8: 0.1819  loss_cate_8: 0  loss_mask_8: 0.8695  loss_dice_8: 1.068  time: 1.1026  data_time: 0.0146  lr: 8.8905e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:50:45 d2.utils.events]: \u001b[0m eta: 21:26:16  iter: 9819  total_loss: 21.36  loss_ce: 0.189  loss_cate: 0.1505  loss_mask: 0.8573  loss_dice: 1.096  loss_ce_0: 0.3149  loss_cate_0: 0  loss_mask_0: 0.889  loss_dice_0: 1.079  loss_ce_1: 0.1719  loss_cate_1: 0  loss_mask_1: 0.8488  loss_dice_1: 1.162  loss_ce_2: 0.242  loss_cate_2: 0  loss_mask_2: 0.8419  loss_dice_2: 1.094  loss_ce_3: 0.2239  loss_cate_3: 0  loss_mask_3: 0.8586  loss_dice_3: 1.081  loss_ce_4: 0.2379  loss_cate_4: 0  loss_mask_4: 0.7926  loss_dice_4: 1.055  loss_ce_5: 0.2251  loss_cate_5: 0  loss_mask_5: 0.7948  loss_dice_5: 1.051  loss_ce_6: 0.2866  loss_cate_6: 0  loss_mask_6: 0.8034  loss_dice_6: 1.028  loss_ce_7: 0.2285  loss_cate_7: 0  loss_mask_7: 0.8592  loss_dice_7: 1.06  loss_ce_8: 0.2157  loss_cate_8: 0  loss_mask_8: 0.8408  loss_dice_8: 1.051  time: 1.1026  data_time: 0.0125  lr: 8.8883e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:51:07 d2.utils.events]: \u001b[0m eta: 21:26:09  iter: 9839  total_loss: 19.44  loss_ce: 0.1473  loss_cate: 0.1619  loss_mask: 0.8497  loss_dice: 0.8777  loss_ce_0: 0.3367  loss_cate_0: 0  loss_mask_0: 0.9199  loss_dice_0: 0.8735  loss_ce_1: 0.1821  loss_cate_1: 0  loss_mask_1: 0.8542  loss_dice_1: 0.8856  loss_ce_2: 0.1972  loss_cate_2: 0  loss_mask_2: 0.852  loss_dice_2: 0.8595  loss_ce_3: 0.114  loss_cate_3: 0  loss_mask_3: 0.841  loss_dice_3: 0.883  loss_ce_4: 0.09475  loss_cate_4: 0  loss_mask_4: 0.8724  loss_dice_4: 0.9081  loss_ce_5: 0.1333  loss_cate_5: 0  loss_mask_5: 0.8402  loss_dice_5: 0.8634  loss_ce_6: 0.1241  loss_cate_6: 0  loss_mask_6: 0.8466  loss_dice_6: 0.8585  loss_ce_7: 0.1518  loss_cate_7: 0  loss_mask_7: 0.8536  loss_dice_7: 0.8555  loss_ce_8: 0.1355  loss_cate_8: 0  loss_mask_8: 0.8702  loss_dice_8: 0.8847  time: 1.1026  data_time: 0.0140  lr: 8.886e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:51:30 d2.utils.events]: \u001b[0m eta: 21:25:45  iter: 9859  total_loss: 20.24  loss_ce: 0.199  loss_cate: 0.1519  loss_mask: 0.7861  loss_dice: 1.012  loss_ce_0: 0.3035  loss_cate_0: 0  loss_mask_0: 0.8156  loss_dice_0: 1.067  loss_ce_1: 0.1995  loss_cate_1: 0  loss_mask_1: 0.8182  loss_dice_1: 1.007  loss_ce_2: 0.285  loss_cate_2: 0  loss_mask_2: 0.82  loss_dice_2: 0.9931  loss_ce_3: 0.2414  loss_cate_3: 0  loss_mask_3: 0.7909  loss_dice_3: 0.9795  loss_ce_4: 0.2259  loss_cate_4: 0  loss_mask_4: 0.7992  loss_dice_4: 0.9764  loss_ce_5: 0.2314  loss_cate_5: 0  loss_mask_5: 0.8259  loss_dice_5: 1.001  loss_ce_6: 0.2079  loss_cate_6: 0  loss_mask_6: 0.791  loss_dice_6: 1.023  loss_ce_7: 0.2147  loss_cate_7: 0  loss_mask_7: 0.7786  loss_dice_7: 1.027  loss_ce_8: 0.2044  loss_cate_8: 0  loss_mask_8: 0.7988  loss_dice_8: 1.06  time: 1.1026  data_time: 0.0130  lr: 8.8837e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:51:52 d2.utils.events]: \u001b[0m eta: 21:25:28  iter: 9879  total_loss: 21.63  loss_ce: 0.2191  loss_cate: 0.1632  loss_mask: 0.8857  loss_dice: 0.9654  loss_ce_0: 0.3487  loss_cate_0: 0  loss_mask_0: 0.9501  loss_dice_0: 1.149  loss_ce_1: 0.3109  loss_cate_1: 0  loss_mask_1: 0.926  loss_dice_1: 1.008  loss_ce_2: 0.2736  loss_cate_2: 0  loss_mask_2: 0.9287  loss_dice_2: 1.006  loss_ce_3: 0.2507  loss_cate_3: 0  loss_mask_3: 0.9074  loss_dice_3: 0.9839  loss_ce_4: 0.2093  loss_cate_4: 0  loss_mask_4: 0.8976  loss_dice_4: 1.005  loss_ce_5: 0.2106  loss_cate_5: 0  loss_mask_5: 0.8911  loss_dice_5: 0.9652  loss_ce_6: 0.2149  loss_cate_6: 0  loss_mask_6: 0.9161  loss_dice_6: 0.9538  loss_ce_7: 0.1961  loss_cate_7: 0  loss_mask_7: 0.8716  loss_dice_7: 0.9981  loss_ce_8: 0.2048  loss_cate_8: 0  loss_mask_8: 0.8801  loss_dice_8: 0.9873  time: 1.1026  data_time: 0.0136  lr: 8.8814e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:52:15 d2.utils.events]: \u001b[0m eta: 21:25:08  iter: 9899  total_loss: 18.94  loss_ce: 0.19  loss_cate: 0.1563  loss_mask: 0.809  loss_dice: 0.8777  loss_ce_0: 0.2928  loss_cate_0: 0  loss_mask_0: 0.7775  loss_dice_0: 0.9103  loss_ce_1: 0.202  loss_cate_1: 0  loss_mask_1: 0.7937  loss_dice_1: 0.8845  loss_ce_2: 0.1787  loss_cate_2: 0  loss_mask_2: 0.8233  loss_dice_2: 0.8563  loss_ce_3: 0.2085  loss_cate_3: 0  loss_mask_3: 0.8043  loss_dice_3: 0.8716  loss_ce_4: 0.2182  loss_cate_4: 0  loss_mask_4: 0.8058  loss_dice_4: 0.8704  loss_ce_5: 0.2234  loss_cate_5: 0  loss_mask_5: 0.8259  loss_dice_5: 0.8698  loss_ce_6: 0.208  loss_cate_6: 0  loss_mask_6: 0.7747  loss_dice_6: 0.8762  loss_ce_7: 0.1975  loss_cate_7: 0  loss_mask_7: 0.768  loss_dice_7: 0.8574  loss_ce_8: 0.1897  loss_cate_8: 0  loss_mask_8: 0.7862  loss_dice_8: 0.8754  time: 1.1026  data_time: 0.0128  lr: 8.8791e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:52:37 d2.utils.events]: \u001b[0m eta: 21:24:46  iter: 9919  total_loss: 22.37  loss_ce: 0.1541  loss_cate: 0.1721  loss_mask: 0.8743  loss_dice: 1.101  loss_ce_0: 0.2739  loss_cate_0: 0  loss_mask_0: 0.9167  loss_dice_0: 1.173  loss_ce_1: 0.2184  loss_cate_1: 0  loss_mask_1: 0.8764  loss_dice_1: 1.174  loss_ce_2: 0.2096  loss_cate_2: 0  loss_mask_2: 0.8769  loss_dice_2: 1.09  loss_ce_3: 0.1935  loss_cate_3: 0  loss_mask_3: 0.8881  loss_dice_3: 1.074  loss_ce_4: 0.202  loss_cate_4: 0  loss_mask_4: 0.8835  loss_dice_4: 1.099  loss_ce_5: 0.1589  loss_cate_5: 0  loss_mask_5: 0.8792  loss_dice_5: 1.094  loss_ce_6: 0.2181  loss_cate_6: 0  loss_mask_6: 0.8689  loss_dice_6: 1.125  loss_ce_7: 0.2081  loss_cate_7: 0  loss_mask_7: 0.8617  loss_dice_7: 1.082  loss_ce_8: 0.1763  loss_cate_8: 0  loss_mask_8: 0.8341  loss_dice_8: 1.032  time: 1.1026  data_time: 0.0145  lr: 8.8769e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:52:59 d2.utils.events]: \u001b[0m eta: 21:24:25  iter: 9939  total_loss: 21.41  loss_ce: 0.2452  loss_cate: 0.1588  loss_mask: 0.8721  loss_dice: 0.9736  loss_ce_0: 0.361  loss_cate_0: 0  loss_mask_0: 0.8786  loss_dice_0: 1.02  loss_ce_1: 0.2479  loss_cate_1: 0  loss_mask_1: 0.8646  loss_dice_1: 0.9732  loss_ce_2: 0.1528  loss_cate_2: 0  loss_mask_2: 0.8702  loss_dice_2: 0.9641  loss_ce_3: 0.1347  loss_cate_3: 0  loss_mask_3: 0.8412  loss_dice_3: 0.9721  loss_ce_4: 0.1528  loss_cate_4: 0  loss_mask_4: 0.8647  loss_dice_4: 0.996  loss_ce_5: 0.2297  loss_cate_5: 0  loss_mask_5: 0.8578  loss_dice_5: 0.9821  loss_ce_6: 0.2227  loss_cate_6: 0  loss_mask_6: 0.8846  loss_dice_6: 0.9667  loss_ce_7: 0.2501  loss_cate_7: 0  loss_mask_7: 0.8591  loss_dice_7: 1.003  loss_ce_8: 0.268  loss_cate_8: 0  loss_mask_8: 0.8785  loss_dice_8: 0.9799  time: 1.1026  data_time: 0.0119  lr: 8.8746e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:53:22 d2.utils.events]: \u001b[0m eta: 21:24:02  iter: 9959  total_loss: 20.38  loss_ce: 0.2179  loss_cate: 0.1409  loss_mask: 0.8362  loss_dice: 0.9532  loss_ce_0: 0.3525  loss_cate_0: 0  loss_mask_0: 0.8439  loss_dice_0: 1.038  loss_ce_1: 0.239  loss_cate_1: 0  loss_mask_1: 0.8625  loss_dice_1: 0.9676  loss_ce_2: 0.2626  loss_cate_2: 0  loss_mask_2: 0.8052  loss_dice_2: 0.9238  loss_ce_3: 0.2327  loss_cate_3: 0  loss_mask_3: 0.8144  loss_dice_3: 0.9905  loss_ce_4: 0.2493  loss_cate_4: 0  loss_mask_4: 0.8076  loss_dice_4: 1.012  loss_ce_5: 0.2198  loss_cate_5: 0  loss_mask_5: 0.8329  loss_dice_5: 0.9798  loss_ce_6: 0.254  loss_cate_6: 0  loss_mask_6: 0.827  loss_dice_6: 0.9506  loss_ce_7: 0.2541  loss_cate_7: 0  loss_mask_7: 0.8257  loss_dice_7: 0.9531  loss_ce_8: 0.2216  loss_cate_8: 0  loss_mask_8: 0.8455  loss_dice_8: 0.9661  time: 1.1026  data_time: 0.0129  lr: 8.8723e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:53:44 d2.utils.events]: \u001b[0m eta: 21:23:23  iter: 9979  total_loss: 20.51  loss_ce: 0.2254  loss_cate: 0.177  loss_mask: 0.8187  loss_dice: 0.971  loss_ce_0: 0.3042  loss_cate_0: 0  loss_mask_0: 0.9243  loss_dice_0: 1.02  loss_ce_1: 0.2524  loss_cate_1: 0  loss_mask_1: 0.789  loss_dice_1: 0.997  loss_ce_2: 0.295  loss_cate_2: 0  loss_mask_2: 0.7681  loss_dice_2: 0.9764  loss_ce_3: 0.268  loss_cate_3: 0  loss_mask_3: 0.7673  loss_dice_3: 0.9486  loss_ce_4: 0.225  loss_cate_4: 0  loss_mask_4: 0.7626  loss_dice_4: 0.9981  loss_ce_5: 0.2546  loss_cate_5: 0  loss_mask_5: 0.8017  loss_dice_5: 0.9937  loss_ce_6: 0.2517  loss_cate_6: 0  loss_mask_6: 0.7723  loss_dice_6: 0.9758  loss_ce_7: 0.248  loss_cate_7: 0  loss_mask_7: 0.8242  loss_dice_7: 0.956  loss_ce_8: 0.2266  loss_cate_8: 0  loss_mask_8: 0.8104  loss_dice_8: 0.9747  time: 1.1026  data_time: 0.0132  lr: 8.87e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 15:54:06 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to ./output/potsdam_contrast_experiment_wrt_boundary_loss/model_0009999.pth\n",
      "\u001b[32m[10/12 15:55:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 15:55:11 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 15:55:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 15:56:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 15:56:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0021 s/iter. Inference: 0.1696 s/iter. Eval: 0.0159 s/iter. Total: 0.1875 s/iter. ETA=0:06:16\n",
      "\u001b[32m[10/12 15:56:18 d2.evaluation.evaluator]: \u001b[0mInference done 36/2016. Dataloading: 0.0028 s/iter. Inference: 0.1690 s/iter. Eval: 0.0266 s/iter. Total: 0.1987 s/iter. ETA=0:06:33\n",
      "\u001b[32m[10/12 15:56:23 d2.evaluation.evaluator]: \u001b[0mInference done 62/2016. Dataloading: 0.0030 s/iter. Inference: 0.1695 s/iter. Eval: 0.0256 s/iter. Total: 0.1984 s/iter. ETA=0:06:27\n",
      "\u001b[32m[10/12 15:56:28 d2.evaluation.evaluator]: \u001b[0mInference done 87/2016. Dataloading: 0.0030 s/iter. Inference: 0.1680 s/iter. Eval: 0.0289 s/iter. Total: 0.2001 s/iter. ETA=0:06:26\n",
      "\u001b[32m[10/12 15:56:33 d2.evaluation.evaluator]: \u001b[0mInference done 113/2016. Dataloading: 0.0029 s/iter. Inference: 0.1673 s/iter. Eval: 0.0289 s/iter. Total: 0.1995 s/iter. ETA=0:06:19\n",
      "\u001b[32m[10/12 15:56:38 d2.evaluation.evaluator]: \u001b[0mInference done 139/2016. Dataloading: 0.0029 s/iter. Inference: 0.1666 s/iter. Eval: 0.0294 s/iter. Total: 0.1992 s/iter. ETA=0:06:13\n",
      "\u001b[32m[10/12 15:56:43 d2.evaluation.evaluator]: \u001b[0mInference done 166/2016. Dataloading: 0.0028 s/iter. Inference: 0.1660 s/iter. Eval: 0.0287 s/iter. Total: 0.1977 s/iter. ETA=0:06:05\n",
      "\u001b[32m[10/12 15:56:48 d2.evaluation.evaluator]: \u001b[0mInference done 191/2016. Dataloading: 0.0027 s/iter. Inference: 0.1656 s/iter. Eval: 0.0298 s/iter. Total: 0.1984 s/iter. ETA=0:06:02\n",
      "\u001b[32m[10/12 15:56:53 d2.evaluation.evaluator]: \u001b[0mInference done 217/2016. Dataloading: 0.0027 s/iter. Inference: 0.1653 s/iter. Eval: 0.0295 s/iter. Total: 0.1977 s/iter. ETA=0:05:55\n",
      "\u001b[32m[10/12 15:56:59 d2.evaluation.evaluator]: \u001b[0mInference done 243/2016. Dataloading: 0.0026 s/iter. Inference: 0.1650 s/iter. Eval: 0.0295 s/iter. Total: 0.1974 s/iter. ETA=0:05:50\n",
      "\u001b[32m[10/12 15:57:04 d2.evaluation.evaluator]: \u001b[0mInference done 268/2016. Dataloading: 0.0026 s/iter. Inference: 0.1649 s/iter. Eval: 0.0305 s/iter. Total: 0.1983 s/iter. ETA=0:05:46\n",
      "\u001b[32m[10/12 15:57:09 d2.evaluation.evaluator]: \u001b[0mInference done 295/2016. Dataloading: 0.0026 s/iter. Inference: 0.1647 s/iter. Eval: 0.0299 s/iter. Total: 0.1975 s/iter. ETA=0:05:39\n",
      "\u001b[32m[10/12 15:57:14 d2.evaluation.evaluator]: \u001b[0mInference done 319/2016. Dataloading: 0.0026 s/iter. Inference: 0.1646 s/iter. Eval: 0.0310 s/iter. Total: 0.1984 s/iter. ETA=0:05:36\n",
      "\u001b[32m[10/12 15:57:19 d2.evaluation.evaluator]: \u001b[0mInference done 345/2016. Dataloading: 0.0026 s/iter. Inference: 0.1646 s/iter. Eval: 0.0306 s/iter. Total: 0.1980 s/iter. ETA=0:05:30\n",
      "\u001b[32m[10/12 15:57:24 d2.evaluation.evaluator]: \u001b[0mInference done 371/2016. Dataloading: 0.0026 s/iter. Inference: 0.1645 s/iter. Eval: 0.0307 s/iter. Total: 0.1980 s/iter. ETA=0:05:25\n",
      "\u001b[32m[10/12 15:57:29 d2.evaluation.evaluator]: \u001b[0mInference done 397/2016. Dataloading: 0.0025 s/iter. Inference: 0.1644 s/iter. Eval: 0.0306 s/iter. Total: 0.1977 s/iter. ETA=0:05:20\n",
      "\u001b[32m[10/12 15:57:34 d2.evaluation.evaluator]: \u001b[0mInference done 423/2016. Dataloading: 0.0025 s/iter. Inference: 0.1643 s/iter. Eval: 0.0308 s/iter. Total: 0.1978 s/iter. ETA=0:05:15\n",
      "\u001b[32m[10/12 15:57:39 d2.evaluation.evaluator]: \u001b[0mInference done 450/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0305 s/iter. Total: 0.1974 s/iter. ETA=0:05:09\n",
      "\u001b[32m[10/12 15:57:44 d2.evaluation.evaluator]: \u001b[0mInference done 475/2016. Dataloading: 0.0025 s/iter. Inference: 0.1641 s/iter. Eval: 0.0309 s/iter. Total: 0.1977 s/iter. ETA=0:05:04\n",
      "\u001b[32m[10/12 15:57:49 d2.evaluation.evaluator]: \u001b[0mInference done 502/2016. Dataloading: 0.0025 s/iter. Inference: 0.1640 s/iter. Eval: 0.0304 s/iter. Total: 0.1971 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 15:57:55 d2.evaluation.evaluator]: \u001b[0mInference done 529/2016. Dataloading: 0.0025 s/iter. Inference: 0.1640 s/iter. Eval: 0.0301 s/iter. Total: 0.1968 s/iter. ETA=0:04:52\n",
      "\u001b[32m[10/12 15:58:00 d2.evaluation.evaluator]: \u001b[0mInference done 556/2016. Dataloading: 0.0025 s/iter. Inference: 0.1640 s/iter. Eval: 0.0299 s/iter. Total: 0.1965 s/iter. ETA=0:04:46\n",
      "\u001b[32m[10/12 15:58:05 d2.evaluation.evaluator]: \u001b[0mInference done 583/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0296 s/iter. Total: 0.1961 s/iter. ETA=0:04:41\n",
      "\u001b[32m[10/12 15:58:10 d2.evaluation.evaluator]: \u001b[0mInference done 609/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0296 s/iter. Total: 0.1961 s/iter. ETA=0:04:35\n",
      "\u001b[32m[10/12 15:58:15 d2.evaluation.evaluator]: \u001b[0mInference done 635/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0295 s/iter. Total: 0.1960 s/iter. ETA=0:04:30\n",
      "\u001b[32m[10/12 15:58:20 d2.evaluation.evaluator]: \u001b[0mInference done 662/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0292 s/iter. Total: 0.1957 s/iter. ETA=0:04:25\n",
      "\u001b[32m[10/12 15:58:25 d2.evaluation.evaluator]: \u001b[0mInference done 688/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0292 s/iter. Total: 0.1957 s/iter. ETA=0:04:19\n",
      "\u001b[32m[10/12 15:58:30 d2.evaluation.evaluator]: \u001b[0mInference done 715/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0290 s/iter. Total: 0.1955 s/iter. ETA=0:04:14\n",
      "\u001b[32m[10/12 15:58:35 d2.evaluation.evaluator]: \u001b[0mInference done 742/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0287 s/iter. Total: 0.1952 s/iter. ETA=0:04:08\n",
      "\u001b[32m[10/12 15:58:40 d2.evaluation.evaluator]: \u001b[0mInference done 768/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0287 s/iter. Total: 0.1952 s/iter. ETA=0:04:03\n",
      "\u001b[32m[10/12 15:58:46 d2.evaluation.evaluator]: \u001b[0mInference done 794/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0287 s/iter. Total: 0.1952 s/iter. ETA=0:03:58\n",
      "\u001b[32m[10/12 15:58:51 d2.evaluation.evaluator]: \u001b[0mInference done 820/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0288 s/iter. Total: 0.1952 s/iter. ETA=0:03:53\n",
      "\u001b[32m[10/12 15:58:56 d2.evaluation.evaluator]: \u001b[0mInference done 847/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0286 s/iter. Total: 0.1951 s/iter. ETA=0:03:48\n",
      "\u001b[32m[10/12 15:59:01 d2.evaluation.evaluator]: \u001b[0mInference done 873/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0287 s/iter. Total: 0.1952 s/iter. ETA=0:03:43\n",
      "\u001b[32m[10/12 15:59:07 d2.evaluation.evaluator]: \u001b[0mInference done 899/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0292 s/iter. Total: 0.1958 s/iter. ETA=0:03:38\n",
      "\u001b[32m[10/12 15:59:12 d2.evaluation.evaluator]: \u001b[0mInference done 926/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0291 s/iter. Total: 0.1956 s/iter. ETA=0:03:33\n",
      "\u001b[32m[10/12 15:59:17 d2.evaluation.evaluator]: \u001b[0mInference done 952/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0291 s/iter. Total: 0.1956 s/iter. ETA=0:03:28\n",
      "\u001b[32m[10/12 15:59:22 d2.evaluation.evaluator]: \u001b[0mInference done 977/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0293 s/iter. Total: 0.1958 s/iter. ETA=0:03:23\n",
      "\u001b[32m[10/12 15:59:27 d2.evaluation.evaluator]: \u001b[0mInference done 1004/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0292 s/iter. Total: 0.1956 s/iter. ETA=0:03:17\n",
      "\u001b[32m[10/12 15:59:32 d2.evaluation.evaluator]: \u001b[0mInference done 1028/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0296 s/iter. Total: 0.1960 s/iter. ETA=0:03:13\n",
      "\u001b[32m[10/12 15:59:37 d2.evaluation.evaluator]: \u001b[0mInference done 1054/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0295 s/iter. Total: 0.1960 s/iter. ETA=0:03:08\n",
      "\u001b[32m[10/12 15:59:42 d2.evaluation.evaluator]: \u001b[0mInference done 1079/2016. Dataloading: 0.0024 s/iter. Inference: 0.1638 s/iter. Eval: 0.0297 s/iter. Total: 0.1961 s/iter. ETA=0:03:03\n",
      "\u001b[32m[10/12 15:59:47 d2.evaluation.evaluator]: \u001b[0mInference done 1106/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0295 s/iter. Total: 0.1960 s/iter. ETA=0:02:58\n",
      "\u001b[32m[10/12 15:59:52 d2.evaluation.evaluator]: \u001b[0mInference done 1130/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0297 s/iter. Total: 0.1962 s/iter. ETA=0:02:53\n",
      "\u001b[32m[10/12 15:59:57 d2.evaluation.evaluator]: \u001b[0mInference done 1157/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0297 s/iter. Total: 0.1961 s/iter. ETA=0:02:48\n",
      "\u001b[32m[10/12 16:00:03 d2.evaluation.evaluator]: \u001b[0mInference done 1182/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:02:43\n",
      "\u001b[32m[10/12 16:00:08 d2.evaluation.evaluator]: \u001b[0mInference done 1207/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:02:38\n",
      "\u001b[32m[10/12 16:00:13 d2.evaluation.evaluator]: \u001b[0mInference done 1234/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0297 s/iter. Total: 0.1962 s/iter. ETA=0:02:33\n",
      "\u001b[32m[10/12 16:00:18 d2.evaluation.evaluator]: \u001b[0mInference done 1259/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0299 s/iter. Total: 0.1964 s/iter. ETA=0:02:28\n",
      "\u001b[32m[10/12 16:00:23 d2.evaluation.evaluator]: \u001b[0mInference done 1285/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0298 s/iter. Total: 0.1964 s/iter. ETA=0:02:23\n",
      "\u001b[32m[10/12 16:00:28 d2.evaluation.evaluator]: \u001b[0mInference done 1311/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:02:18\n",
      "\u001b[32m[10/12 16:00:33 d2.evaluation.evaluator]: \u001b[0mInference done 1336/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0299 s/iter. Total: 0.1964 s/iter. ETA=0:02:13\n",
      "\u001b[32m[10/12 16:00:38 d2.evaluation.evaluator]: \u001b[0mInference done 1363/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0297 s/iter. Total: 0.1962 s/iter. ETA=0:02:08\n",
      "\u001b[32m[10/12 16:00:43 d2.evaluation.evaluator]: \u001b[0mInference done 1388/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0299 s/iter. Total: 0.1964 s/iter. ETA=0:02:03\n",
      "\u001b[32m[10/12 16:00:48 d2.evaluation.evaluator]: \u001b[0mInference done 1415/2016. Dataloading: 0.0024 s/iter. Inference: 0.1640 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:01:57\n",
      "\u001b[32m[10/12 16:00:53 d2.evaluation.evaluator]: \u001b[0mInference done 1441/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:01:52\n",
      "\u001b[32m[10/12 16:00:59 d2.evaluation.evaluator]: \u001b[0mInference done 1467/2016. Dataloading: 0.0023 s/iter. Inference: 0.1639 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:01:47\n",
      "\u001b[32m[10/12 16:01:04 d2.evaluation.evaluator]: \u001b[0mInference done 1493/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:01:42\n",
      "\u001b[32m[10/12 16:01:09 d2.evaluation.evaluator]: \u001b[0mInference done 1519/2016. Dataloading: 0.0024 s/iter. Inference: 0.1639 s/iter. Eval: 0.0297 s/iter. Total: 0.1962 s/iter. ETA=0:01:37\n",
      "\u001b[32m[10/12 16:01:14 d2.evaluation.evaluator]: \u001b[0mInference done 1545/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0297 s/iter. Total: 0.1962 s/iter. ETA=0:01:32\n",
      "\u001b[32m[10/12 16:01:19 d2.evaluation.evaluator]: \u001b[0mInference done 1571/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0297 s/iter. Total: 0.1962 s/iter. ETA=0:01:27\n",
      "\u001b[32m[10/12 16:01:24 d2.evaluation.evaluator]: \u001b[0mInference done 1598/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0296 s/iter. Total: 0.1961 s/iter. ETA=0:01:21\n",
      "\u001b[32m[10/12 16:01:29 d2.evaluation.evaluator]: \u001b[0mInference done 1624/2016. Dataloading: 0.0023 s/iter. Inference: 0.1639 s/iter. Eval: 0.0296 s/iter. Total: 0.1961 s/iter. ETA=0:01:16\n",
      "\u001b[32m[10/12 16:01:34 d2.evaluation.evaluator]: \u001b[0mInference done 1650/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0295 s/iter. Total: 0.1960 s/iter. ETA=0:01:11\n",
      "\u001b[32m[10/12 16:01:39 d2.evaluation.evaluator]: \u001b[0mInference done 1676/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0296 s/iter. Total: 0.1961 s/iter. ETA=0:01:06\n",
      "\u001b[32m[10/12 16:01:44 d2.evaluation.evaluator]: \u001b[0mInference done 1702/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0296 s/iter. Total: 0.1961 s/iter. ETA=0:01:01\n",
      "\u001b[32m[10/12 16:01:49 d2.evaluation.evaluator]: \u001b[0mInference done 1729/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0295 s/iter. Total: 0.1960 s/iter. ETA=0:00:56\n",
      "\u001b[32m[10/12 16:01:55 d2.evaluation.evaluator]: \u001b[0mInference done 1751/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0300 s/iter. Total: 0.1965 s/iter. ETA=0:00:52\n",
      "\u001b[32m[10/12 16:02:00 d2.evaluation.evaluator]: \u001b[0mInference done 1777/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0300 s/iter. Total: 0.1965 s/iter. ETA=0:00:46\n",
      "\u001b[32m[10/12 16:02:05 d2.evaluation.evaluator]: \u001b[0mInference done 1804/2016. Dataloading: 0.0023 s/iter. Inference: 0.1639 s/iter. Eval: 0.0299 s/iter. Total: 0.1963 s/iter. ETA=0:00:41\n",
      "\u001b[32m[10/12 16:02:10 d2.evaluation.evaluator]: \u001b[0mInference done 1830/2016. Dataloading: 0.0023 s/iter. Inference: 0.1639 s/iter. Eval: 0.0299 s/iter. Total: 0.1964 s/iter. ETA=0:00:36\n",
      "\u001b[32m[10/12 16:02:15 d2.evaluation.evaluator]: \u001b[0mInference done 1855/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0299 s/iter. Total: 0.1965 s/iter. ETA=0:00:31\n",
      "\u001b[32m[10/12 16:02:20 d2.evaluation.evaluator]: \u001b[0mInference done 1882/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0298 s/iter. Total: 0.1964 s/iter. ETA=0:00:26\n",
      "\u001b[32m[10/12 16:02:25 d2.evaluation.evaluator]: \u001b[0mInference done 1908/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0298 s/iter. Total: 0.1964 s/iter. ETA=0:00:21\n",
      "\u001b[32m[10/12 16:02:30 d2.evaluation.evaluator]: \u001b[0mInference done 1935/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0297 s/iter. Total: 0.1963 s/iter. ETA=0:00:15\n",
      "\u001b[32m[10/12 16:02:35 d2.evaluation.evaluator]: \u001b[0mInference done 1960/2016. Dataloading: 0.0023 s/iter. Inference: 0.1640 s/iter. Eval: 0.0298 s/iter. Total: 0.1963 s/iter. ETA=0:00:10\n",
      "\u001b[32m[10/12 16:02:40 d2.evaluation.evaluator]: \u001b[0mInference done 1987/2016. Dataloading: 0.0023 s/iter. Inference: 0.1641 s/iter. Eval: 0.0296 s/iter. Total: 0.1962 s/iter. ETA=0:00:05\n",
      "\u001b[32m[10/12 16:02:46 d2.evaluation.evaluator]: \u001b[0mInference done 2013/2016. Dataloading: 0.0023 s/iter. Inference: 0.1641 s/iter. Eval: 0.0296 s/iter. Total: 0.1962 s/iter. ETA=0:00:00\n",
      "\u001b[32m[10/12 16:02:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:34.686103 (0.196264 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 16:02:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:29 (0.164071 s / iter per device, on 1 devices)\n",
      "miou = 75.49870723968999\n",
      "OA = 89.15352518596347\n",
      "Kappa = 85.81475982108907\n",
      "F1_score = 72.00764404887909\n",
      "\u001b[32m[10/12 16:02:47 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 75.49870723968999, 'fwIoU': 80.83356928755892, 'IoU-Background': 40.57531829883351, 'IoU-Surfaces': 83.74793388418564, 'IoU-Building': 92.16726368145635, 'IoU-Low vegetation': 75.35671138539355, 'IoU-tree': 76.8943441293214, 'IoU-Car': 84.25067205894948, 'mACC': 83.68699602321568, 'pACC': 89.15352518596347, 'ACC-Background': 49.61098934988506, 'ACC-Surfaces': 93.38555090663324, 'ACC-Building': 95.77326637198193, 'ACC-Low vegetation': 86.48127508785079, 'ACC-tree': 85.93108545575191, 'ACC-Car': 90.93980896719114})])\n",
      "\u001b[32m[10/12 16:02:47 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 16:02:47 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 16:02:47 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 16:02:47 d2.evaluation.testing]: \u001b[0mcopypaste: 75.4987,80.8336,83.6870,89.1535\n",
      "\u001b[32m[10/12 16:02:47 d2.utils.events]: \u001b[0m eta: 21:22:55  iter: 9999  total_loss: 19.39  loss_ce: 0.1543  loss_cate: 0.138  loss_mask: 0.7373  loss_dice: 0.9356  loss_ce_0: 0.2391  loss_cate_0: 0  loss_mask_0: 0.7512  loss_dice_0: 0.9786  loss_ce_1: 0.1371  loss_cate_1: 0  loss_mask_1: 0.7403  loss_dice_1: 0.9409  loss_ce_2: 0.1558  loss_cate_2: 0  loss_mask_2: 0.736  loss_dice_2: 0.9714  loss_ce_3: 0.1112  loss_cate_3: 0  loss_mask_3: 0.7547  loss_dice_3: 0.9634  loss_ce_4: 0.1345  loss_cate_4: 0  loss_mask_4: 0.7481  loss_dice_4: 0.9642  loss_ce_5: 0.1442  loss_cate_5: 0  loss_mask_5: 0.7533  loss_dice_5: 0.9441  loss_ce_6: 0.1869  loss_cate_6: 0  loss_mask_6: 0.7456  loss_dice_6: 0.9309  loss_ce_7: 0.1068  loss_cate_7: 0  loss_mask_7: 0.7585  loss_dice_7: 0.9267  loss_ce_8: 0.1394  loss_cate_8: 0  loss_mask_8: 0.7496  loss_dice_8: 0.8861  time: 1.1026  data_time: 0.0124  lr: 8.8677e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:03:10 d2.utils.events]: \u001b[0m eta: 21:22:33  iter: 10019  total_loss: 22.54  loss_ce: 0.188  loss_cate: 0.1746  loss_mask: 0.8265  loss_dice: 1.041  loss_ce_0: 0.3302  loss_cate_0: 0  loss_mask_0: 0.7914  loss_dice_0: 1.057  loss_ce_1: 0.2642  loss_cate_1: 0  loss_mask_1: 0.8132  loss_dice_1: 1.055  loss_ce_2: 0.1417  loss_cate_2: 0  loss_mask_2: 0.8034  loss_dice_2: 1.076  loss_ce_3: 0.1748  loss_cate_3: 0  loss_mask_3: 0.8088  loss_dice_3: 1.063  loss_ce_4: 0.2028  loss_cate_4: 0  loss_mask_4: 0.8014  loss_dice_4: 1.05  loss_ce_5: 0.2067  loss_cate_5: 0  loss_mask_5: 0.8051  loss_dice_5: 1.069  loss_ce_6: 0.2059  loss_cate_6: 0  loss_mask_6: 0.8232  loss_dice_6: 1.055  loss_ce_7: 0.1289  loss_cate_7: 0  loss_mask_7: 0.8124  loss_dice_7: 1.052  loss_ce_8: 0.2027  loss_cate_8: 0  loss_mask_8: 0.8246  loss_dice_8: 1.132  time: 1.1026  data_time: 0.0140  lr: 8.8655e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:03:33 d2.utils.events]: \u001b[0m eta: 21:22:16  iter: 10039  total_loss: 19.29  loss_ce: 0.1134  loss_cate: 0.1556  loss_mask: 0.7586  loss_dice: 0.9465  loss_ce_0: 0.2727  loss_cate_0: 0  loss_mask_0: 0.78  loss_dice_0: 1.002  loss_ce_1: 0.08169  loss_cate_1: 0  loss_mask_1: 0.7974  loss_dice_1: 0.999  loss_ce_2: 0.164  loss_cate_2: 0  loss_mask_2: 0.7652  loss_dice_2: 0.978  loss_ce_3: 0.1237  loss_cate_3: 0  loss_mask_3: 0.7624  loss_dice_3: 0.96  loss_ce_4: 0.136  loss_cate_4: 0  loss_mask_4: 0.7582  loss_dice_4: 0.9889  loss_ce_5: 0.111  loss_cate_5: 0  loss_mask_5: 0.781  loss_dice_5: 0.9668  loss_ce_6: 0.1569  loss_cate_6: 0  loss_mask_6: 0.7541  loss_dice_6: 0.9508  loss_ce_7: 0.1593  loss_cate_7: 0  loss_mask_7: 0.7563  loss_dice_7: 0.9143  loss_ce_8: 0.1049  loss_cate_8: 0  loss_mask_8: 0.7521  loss_dice_8: 0.9493  time: 1.1026  data_time: 0.0128  lr: 8.8632e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:03:55 d2.utils.events]: \u001b[0m eta: 21:21:54  iter: 10059  total_loss: 20.28  loss_ce: 0.1792  loss_cate: 0.1706  loss_mask: 0.8196  loss_dice: 0.9482  loss_ce_0: 0.366  loss_cate_0: 0  loss_mask_0: 0.8341  loss_dice_0: 0.9323  loss_ce_1: 0.2458  loss_cate_1: 0  loss_mask_1: 0.8592  loss_dice_1: 0.9564  loss_ce_2: 0.2765  loss_cate_2: 0  loss_mask_2: 0.8471  loss_dice_2: 0.9484  loss_ce_3: 0.2105  loss_cate_3: 0  loss_mask_3: 0.8725  loss_dice_3: 0.9315  loss_ce_4: 0.232  loss_cate_4: 0  loss_mask_4: 0.8463  loss_dice_4: 0.9519  loss_ce_5: 0.2361  loss_cate_5: 0  loss_mask_5: 0.8367  loss_dice_5: 0.9439  loss_ce_6: 0.1928  loss_cate_6: 0  loss_mask_6: 0.8512  loss_dice_6: 0.9264  loss_ce_7: 0.1898  loss_cate_7: 0  loss_mask_7: 0.8672  loss_dice_7: 0.9324  loss_ce_8: 0.15  loss_cate_8: 0  loss_mask_8: 0.8639  loss_dice_8: 0.9873  time: 1.1026  data_time: 0.0129  lr: 8.8609e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:04:17 d2.utils.events]: \u001b[0m eta: 21:21:49  iter: 10079  total_loss: 21.25  loss_ce: 0.2173  loss_cate: 0.1471  loss_mask: 0.7689  loss_dice: 0.9531  loss_ce_0: 0.3554  loss_cate_0: 0  loss_mask_0: 0.8243  loss_dice_0: 1.006  loss_ce_1: 0.2794  loss_cate_1: 0  loss_mask_1: 0.7838  loss_dice_1: 1.015  loss_ce_2: 0.2467  loss_cate_2: 0  loss_mask_2: 0.8249  loss_dice_2: 0.9894  loss_ce_3: 0.2247  loss_cate_3: 0  loss_mask_3: 0.7638  loss_dice_3: 1.011  loss_ce_4: 0.2251  loss_cate_4: 0  loss_mask_4: 0.7652  loss_dice_4: 1.006  loss_ce_5: 0.2827  loss_cate_5: 0  loss_mask_5: 0.798  loss_dice_5: 1.007  loss_ce_6: 0.2633  loss_cate_6: 0  loss_mask_6: 0.7823  loss_dice_6: 0.9535  loss_ce_7: 0.2491  loss_cate_7: 0  loss_mask_7: 0.7977  loss_dice_7: 0.9774  loss_ce_8: 0.2417  loss_cate_8: 0  loss_mask_8: 0.8139  loss_dice_8: 0.987  time: 1.1026  data_time: 0.0135  lr: 8.8586e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:04:40 d2.utils.events]: \u001b[0m eta: 21:21:22  iter: 10099  total_loss: 19.18  loss_ce: 0.1446  loss_cate: 0.1537  loss_mask: 0.8364  loss_dice: 0.974  loss_ce_0: 0.3335  loss_cate_0: 0  loss_mask_0: 0.8278  loss_dice_0: 1.009  loss_ce_1: 0.2522  loss_cate_1: 0  loss_mask_1: 0.8452  loss_dice_1: 0.9937  loss_ce_2: 0.1523  loss_cate_2: 0  loss_mask_2: 0.8288  loss_dice_2: 0.9985  loss_ce_3: 0.1877  loss_cate_3: 0  loss_mask_3: 0.7862  loss_dice_3: 0.9695  loss_ce_4: 0.164  loss_cate_4: 0  loss_mask_4: 0.7906  loss_dice_4: 0.982  loss_ce_5: 0.2079  loss_cate_5: 0  loss_mask_5: 0.7785  loss_dice_5: 0.9227  loss_ce_6: 0.2362  loss_cate_6: 0  loss_mask_6: 0.7855  loss_dice_6: 0.986  loss_ce_7: 0.1506  loss_cate_7: 0  loss_mask_7: 0.8341  loss_dice_7: 0.9514  loss_ce_8: 0.1719  loss_cate_8: 0  loss_mask_8: 0.7823  loss_dice_8: 0.9477  time: 1.1026  data_time: 0.0135  lr: 8.8563e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:05:02 d2.utils.events]: \u001b[0m eta: 21:21:06  iter: 10119  total_loss: 19.53  loss_ce: 0.1803  loss_cate: 0.1367  loss_mask: 0.69  loss_dice: 0.8915  loss_ce_0: 0.3632  loss_cate_0: 0  loss_mask_0: 0.7194  loss_dice_0: 0.9653  loss_ce_1: 0.2472  loss_cate_1: 0  loss_mask_1: 0.7083  loss_dice_1: 0.9742  loss_ce_2: 0.2495  loss_cate_2: 0  loss_mask_2: 0.6941  loss_dice_2: 0.894  loss_ce_3: 0.2197  loss_cate_3: 0  loss_mask_3: 0.6975  loss_dice_3: 0.9114  loss_ce_4: 0.2183  loss_cate_4: 0  loss_mask_4: 0.6756  loss_dice_4: 0.8577  loss_ce_5: 0.2019  loss_cate_5: 0  loss_mask_5: 0.6417  loss_dice_5: 0.8819  loss_ce_6: 0.2674  loss_cate_6: 0  loss_mask_6: 0.6741  loss_dice_6: 0.8389  loss_ce_7: 0.2393  loss_cate_7: 0  loss_mask_7: 0.6854  loss_dice_7: 0.8842  loss_ce_8: 0.1903  loss_cate_8: 0  loss_mask_8: 0.6728  loss_dice_8: 0.8774  time: 1.1026  data_time: 0.0127  lr: 8.8541e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:05:24 d2.utils.events]: \u001b[0m eta: 21:20:44  iter: 10139  total_loss: 22.67  loss_ce: 0.2311  loss_cate: 0.1398  loss_mask: 0.7827  loss_dice: 0.9785  loss_ce_0: 0.3193  loss_cate_0: 0  loss_mask_0: 0.7999  loss_dice_0: 1.01  loss_ce_1: 0.2335  loss_cate_1: 0  loss_mask_1: 0.81  loss_dice_1: 1.019  loss_ce_2: 0.214  loss_cate_2: 0  loss_mask_2: 0.8375  loss_dice_2: 0.9816  loss_ce_3: 0.2486  loss_cate_3: 0  loss_mask_3: 0.7514  loss_dice_3: 0.9855  loss_ce_4: 0.2594  loss_cate_4: 0  loss_mask_4: 0.7265  loss_dice_4: 0.9798  loss_ce_5: 0.2397  loss_cate_5: 0  loss_mask_5: 0.751  loss_dice_5: 0.9768  loss_ce_6: 0.2586  loss_cate_6: 0  loss_mask_6: 0.7734  loss_dice_6: 0.9051  loss_ce_7: 0.221  loss_cate_7: 0  loss_mask_7: 0.781  loss_dice_7: 0.9662  loss_ce_8: 0.2455  loss_cate_8: 0  loss_mask_8: 0.7782  loss_dice_8: 0.9743  time: 1.1026  data_time: 0.0127  lr: 8.8518e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:05:46 d2.utils.events]: \u001b[0m eta: 21:20:24  iter: 10159  total_loss: 22.12  loss_ce: 0.2472  loss_cate: 0.1284  loss_mask: 0.7362  loss_dice: 0.9492  loss_ce_0: 0.3476  loss_cate_0: 0  loss_mask_0: 0.7192  loss_dice_0: 1.067  loss_ce_1: 0.2587  loss_cate_1: 0  loss_mask_1: 0.7168  loss_dice_1: 0.9634  loss_ce_2: 0.2562  loss_cate_2: 0  loss_mask_2: 0.7436  loss_dice_2: 0.9819  loss_ce_3: 0.2704  loss_cate_3: 0  loss_mask_3: 0.7314  loss_dice_3: 0.9431  loss_ce_4: 0.2746  loss_cate_4: 0  loss_mask_4: 0.7457  loss_dice_4: 0.9597  loss_ce_5: 0.272  loss_cate_5: 0  loss_mask_5: 0.7306  loss_dice_5: 0.9415  loss_ce_6: 0.2691  loss_cate_6: 0  loss_mask_6: 0.7336  loss_dice_6: 0.9501  loss_ce_7: 0.22  loss_cate_7: 0  loss_mask_7: 0.7321  loss_dice_7: 0.9671  loss_ce_8: 0.2175  loss_cate_8: 0  loss_mask_8: 0.7233  loss_dice_8: 0.9577  time: 1.1026  data_time: 0.0135  lr: 8.8495e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:06:09 d2.utils.events]: \u001b[0m eta: 21:20:10  iter: 10179  total_loss: 21.61  loss_ce: 0.207  loss_cate: 0.1514  loss_mask: 0.9054  loss_dice: 1.01  loss_ce_0: 0.3359  loss_cate_0: 0  loss_mask_0: 0.9019  loss_dice_0: 0.9415  loss_ce_1: 0.2081  loss_cate_1: 0  loss_mask_1: 0.9427  loss_dice_1: 1.003  loss_ce_2: 0.2152  loss_cate_2: 0  loss_mask_2: 0.9213  loss_dice_2: 1.008  loss_ce_3: 0.2171  loss_cate_3: 0  loss_mask_3: 0.9058  loss_dice_3: 1.003  loss_ce_4: 0.1936  loss_cate_4: 0  loss_mask_4: 0.9027  loss_dice_4: 0.9997  loss_ce_5: 0.2077  loss_cate_5: 0  loss_mask_5: 0.8843  loss_dice_5: 0.9474  loss_ce_6: 0.123  loss_cate_6: 0  loss_mask_6: 0.8729  loss_dice_6: 0.9712  loss_ce_7: 0.1774  loss_cate_7: 0  loss_mask_7: 0.8773  loss_dice_7: 0.9844  loss_ce_8: 0.1091  loss_cate_8: 0  loss_mask_8: 0.8833  loss_dice_8: 0.9974  time: 1.1025  data_time: 0.0144  lr: 8.8472e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:06:31 d2.utils.events]: \u001b[0m eta: 21:19:57  iter: 10199  total_loss: 21.41  loss_ce: 0.2514  loss_cate: 0.1564  loss_mask: 0.7872  loss_dice: 1.072  loss_ce_0: 0.3044  loss_cate_0: 0  loss_mask_0: 0.8201  loss_dice_0: 1.083  loss_ce_1: 0.3132  loss_cate_1: 0  loss_mask_1: 0.8367  loss_dice_1: 1.016  loss_ce_2: 0.2242  loss_cate_2: 0  loss_mask_2: 0.7836  loss_dice_2: 1.07  loss_ce_3: 0.2585  loss_cate_3: 0  loss_mask_3: 0.7936  loss_dice_3: 1.065  loss_ce_4: 0.2511  loss_cate_4: 0  loss_mask_4: 0.8083  loss_dice_4: 1.063  loss_ce_5: 0.221  loss_cate_5: 0  loss_mask_5: 0.8087  loss_dice_5: 1.095  loss_ce_6: 0.2381  loss_cate_6: 0  loss_mask_6: 0.8161  loss_dice_6: 1.059  loss_ce_7: 0.2596  loss_cate_7: 0  loss_mask_7: 0.8438  loss_dice_7: 1.064  loss_ce_8: 0.2368  loss_cate_8: 0  loss_mask_8: 0.7719  loss_dice_8: 1.004  time: 1.1025  data_time: 0.0130  lr: 8.8449e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:06:54 d2.utils.events]: \u001b[0m eta: 21:19:35  iter: 10219  total_loss: 20.73  loss_ce: 0.2513  loss_cate: 0.1631  loss_mask: 0.7506  loss_dice: 0.9747  loss_ce_0: 0.3198  loss_cate_0: 0  loss_mask_0: 0.7653  loss_dice_0: 1.018  loss_ce_1: 0.2173  loss_cate_1: 0  loss_mask_1: 0.7861  loss_dice_1: 0.9984  loss_ce_2: 0.2174  loss_cate_2: 0  loss_mask_2: 0.7478  loss_dice_2: 0.9739  loss_ce_3: 0.1834  loss_cate_3: 0  loss_mask_3: 0.7669  loss_dice_3: 0.9513  loss_ce_4: 0.1671  loss_cate_4: 0  loss_mask_4: 0.787  loss_dice_4: 0.9718  loss_ce_5: 0.2228  loss_cate_5: 0  loss_mask_5: 0.7763  loss_dice_5: 0.9619  loss_ce_6: 0.1984  loss_cate_6: 0  loss_mask_6: 0.7718  loss_dice_6: 0.9692  loss_ce_7: 0.2049  loss_cate_7: 0  loss_mask_7: 0.7557  loss_dice_7: 0.9232  loss_ce_8: 0.2137  loss_cate_8: 0  loss_mask_8: 0.7633  loss_dice_8: 0.9503  time: 1.1025  data_time: 0.0137  lr: 8.8427e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:07:17 d2.utils.events]: \u001b[0m eta: 21:18:55  iter: 10239  total_loss: 24.1  loss_ce: 0.2689  loss_cate: 0.1605  loss_mask: 1.008  loss_dice: 1.013  loss_ce_0: 0.4479  loss_cate_0: 0  loss_mask_0: 1.006  loss_dice_0: 0.9977  loss_ce_1: 0.2687  loss_cate_1: 0  loss_mask_1: 0.9876  loss_dice_1: 1.063  loss_ce_2: 0.2878  loss_cate_2: 0  loss_mask_2: 0.9457  loss_dice_2: 1.034  loss_ce_3: 0.2554  loss_cate_3: 0  loss_mask_3: 0.977  loss_dice_3: 1.033  loss_ce_4: 0.3054  loss_cate_4: 0  loss_mask_4: 0.9564  loss_dice_4: 1.032  loss_ce_5: 0.2495  loss_cate_5: 0  loss_mask_5: 1.009  loss_dice_5: 1.015  loss_ce_6: 0.2538  loss_cate_6: 0  loss_mask_6: 1.007  loss_dice_6: 1.017  loss_ce_7: 0.2542  loss_cate_7: 0  loss_mask_7: 0.9755  loss_dice_7: 1.026  loss_ce_8: 0.2441  loss_cate_8: 0  loss_mask_8: 1.005  loss_dice_8: 1.006  time: 1.1025  data_time: 0.0126  lr: 8.8404e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:07:39 d2.utils.events]: \u001b[0m eta: 21:18:34  iter: 10259  total_loss: 19.9  loss_ce: 0.1946  loss_cate: 0.1574  loss_mask: 0.7279  loss_dice: 0.9577  loss_ce_0: 0.2453  loss_cate_0: 0  loss_mask_0: 0.7897  loss_dice_0: 1.041  loss_ce_1: 0.3015  loss_cate_1: 0  loss_mask_1: 0.755  loss_dice_1: 1.015  loss_ce_2: 0.266  loss_cate_2: 0  loss_mask_2: 0.7267  loss_dice_2: 1.009  loss_ce_3: 0.2672  loss_cate_3: 0  loss_mask_3: 0.7286  loss_dice_3: 0.9709  loss_ce_4: 0.2459  loss_cate_4: 0  loss_mask_4: 0.7274  loss_dice_4: 0.9669  loss_ce_5: 0.2305  loss_cate_5: 0  loss_mask_5: 0.7187  loss_dice_5: 0.9649  loss_ce_6: 0.2292  loss_cate_6: 0  loss_mask_6: 0.7205  loss_dice_6: 0.9374  loss_ce_7: 0.2217  loss_cate_7: 0  loss_mask_7: 0.728  loss_dice_7: 0.968  loss_ce_8: 0.2329  loss_cate_8: 0  loss_mask_8: 0.7234  loss_dice_8: 0.9667  time: 1.1025  data_time: 0.0140  lr: 8.8381e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:08:01 d2.utils.events]: \u001b[0m eta: 21:18:10  iter: 10279  total_loss: 22.46  loss_ce: 0.2459  loss_cate: 0.1419  loss_mask: 0.8477  loss_dice: 0.9483  loss_ce_0: 0.3341  loss_cate_0: 0  loss_mask_0: 0.8912  loss_dice_0: 0.9753  loss_ce_1: 0.2673  loss_cate_1: 0  loss_mask_1: 0.884  loss_dice_1: 1.036  loss_ce_2: 0.2601  loss_cate_2: 0  loss_mask_2: 0.8335  loss_dice_2: 0.9434  loss_ce_3: 0.2738  loss_cate_3: 0  loss_mask_3: 0.8618  loss_dice_3: 0.9436  loss_ce_4: 0.2776  loss_cate_4: 0  loss_mask_4: 0.8135  loss_dice_4: 0.9722  loss_ce_5: 0.2729  loss_cate_5: 0  loss_mask_5: 0.8066  loss_dice_5: 0.9721  loss_ce_6: 0.2645  loss_cate_6: 0  loss_mask_6: 0.8594  loss_dice_6: 0.9916  loss_ce_7: 0.2573  loss_cate_7: 0  loss_mask_7: 0.8636  loss_dice_7: 0.9526  loss_ce_8: 0.2552  loss_cate_8: 0  loss_mask_8: 0.8697  loss_dice_8: 0.9623  time: 1.1025  data_time: 0.0128  lr: 8.8358e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:08:24 d2.utils.events]: \u001b[0m eta: 21:17:50  iter: 10299  total_loss: 20.88  loss_ce: 0.2917  loss_cate: 0.1451  loss_mask: 0.7037  loss_dice: 0.9652  loss_ce_0: 0.3128  loss_cate_0: 0  loss_mask_0: 0.7666  loss_dice_0: 0.9817  loss_ce_1: 0.2459  loss_cate_1: 0  loss_mask_1: 0.725  loss_dice_1: 0.96  loss_ce_2: 0.2946  loss_cate_2: 0  loss_mask_2: 0.6942  loss_dice_2: 0.9774  loss_ce_3: 0.2827  loss_cate_3: 0  loss_mask_3: 0.7225  loss_dice_3: 0.9472  loss_ce_4: 0.2657  loss_cate_4: 0  loss_mask_4: 0.725  loss_dice_4: 0.9707  loss_ce_5: 0.2923  loss_cate_5: 0  loss_mask_5: 0.7033  loss_dice_5: 0.9261  loss_ce_6: 0.306  loss_cate_6: 0  loss_mask_6: 0.7079  loss_dice_6: 0.9289  loss_ce_7: 0.3229  loss_cate_7: 0  loss_mask_7: 0.7402  loss_dice_7: 0.95  loss_ce_8: 0.2323  loss_cate_8: 0  loss_mask_8: 0.7073  loss_dice_8: 0.9323  time: 1.1025  data_time: 0.0136  lr: 8.8335e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:08:47 d2.utils.events]: \u001b[0m eta: 21:17:41  iter: 10319  total_loss: 21.71  loss_ce: 0.2171  loss_cate: 0.1633  loss_mask: 0.8173  loss_dice: 1.042  loss_ce_0: 0.2986  loss_cate_0: 0  loss_mask_0: 0.8369  loss_dice_0: 1.125  loss_ce_1: 0.2278  loss_cate_1: 0  loss_mask_1: 0.8244  loss_dice_1: 1.104  loss_ce_2: 0.2071  loss_cate_2: 0  loss_mask_2: 0.8009  loss_dice_2: 1.057  loss_ce_3: 0.1674  loss_cate_3: 0  loss_mask_3: 0.8321  loss_dice_3: 1.054  loss_ce_4: 0.2208  loss_cate_4: 0  loss_mask_4: 0.7988  loss_dice_4: 1.048  loss_ce_5: 0.2256  loss_cate_5: 0  loss_mask_5: 0.7911  loss_dice_5: 1.065  loss_ce_6: 0.2378  loss_cate_6: 0  loss_mask_6: 0.7948  loss_dice_6: 1.049  loss_ce_7: 0.188  loss_cate_7: 0  loss_mask_7: 0.8213  loss_dice_7: 1.062  loss_ce_8: 0.227  loss_cate_8: 0  loss_mask_8: 0.8078  loss_dice_8: 1.039  time: 1.1025  data_time: 0.0130  lr: 8.8312e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:09:09 d2.utils.events]: \u001b[0m eta: 21:17:24  iter: 10339  total_loss: 19.76  loss_ce: 0.1109  loss_cate: 0.1364  loss_mask: 0.8228  loss_dice: 1.02  loss_ce_0: 0.2153  loss_cate_0: 0  loss_mask_0: 0.8269  loss_dice_0: 0.9753  loss_ce_1: 0.2127  loss_cate_1: 0  loss_mask_1: 0.8244  loss_dice_1: 1.037  loss_ce_2: 0.1492  loss_cate_2: 0  loss_mask_2: 0.8588  loss_dice_2: 1.002  loss_ce_3: 0.1099  loss_cate_3: 0  loss_mask_3: 0.8803  loss_dice_3: 1.011  loss_ce_4: 0.09316  loss_cate_4: 0  loss_mask_4: 0.8703  loss_dice_4: 1.014  loss_ce_5: 0.1202  loss_cate_5: 0  loss_mask_5: 0.8528  loss_dice_5: 1.011  loss_ce_6: 0.108  loss_cate_6: 0  loss_mask_6: 0.8225  loss_dice_6: 1.017  loss_ce_7: 0.09108  loss_cate_7: 0  loss_mask_7: 0.8336  loss_dice_7: 1.025  loss_ce_8: 0.09743  loss_cate_8: 0  loss_mask_8: 0.8261  loss_dice_8: 1.003  time: 1.1025  data_time: 0.0137  lr: 8.829e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:09:32 d2.utils.events]: \u001b[0m eta: 21:16:57  iter: 10359  total_loss: 22.59  loss_ce: 0.2052  loss_cate: 0.1627  loss_mask: 0.8839  loss_dice: 1.086  loss_ce_0: 0.3416  loss_cate_0: 0  loss_mask_0: 0.9058  loss_dice_0: 1.121  loss_ce_1: 0.2377  loss_cate_1: 0  loss_mask_1: 0.9108  loss_dice_1: 1.045  loss_ce_2: 0.1493  loss_cate_2: 0  loss_mask_2: 0.9067  loss_dice_2: 1.099  loss_ce_3: 0.226  loss_cate_3: 0  loss_mask_3: 0.9155  loss_dice_3: 1.139  loss_ce_4: 0.1955  loss_cate_4: 0  loss_mask_4: 0.9196  loss_dice_4: 1.169  loss_ce_5: 0.2185  loss_cate_5: 0  loss_mask_5: 0.8825  loss_dice_5: 1.127  loss_ce_6: 0.2036  loss_cate_6: 0  loss_mask_6: 0.9156  loss_dice_6: 1.083  loss_ce_7: 0.1973  loss_cate_7: 0  loss_mask_7: 0.9081  loss_dice_7: 1.114  loss_ce_8: 0.205  loss_cate_8: 0  loss_mask_8: 0.9146  loss_dice_8: 1.085  time: 1.1025  data_time: 0.0142  lr: 8.8267e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:09:54 d2.utils.events]: \u001b[0m eta: 21:16:45  iter: 10379  total_loss: 18.53  loss_ce: 0.1693  loss_cate: 0.1452  loss_mask: 0.7407  loss_dice: 0.861  loss_ce_0: 0.3466  loss_cate_0: 0  loss_mask_0: 0.7496  loss_dice_0: 0.8586  loss_ce_1: 0.1779  loss_cate_1: 0  loss_mask_1: 0.7963  loss_dice_1: 0.9128  loss_ce_2: 0.2486  loss_cate_2: 0  loss_mask_2: 0.7378  loss_dice_2: 0.8701  loss_ce_3: 0.1729  loss_cate_3: 0  loss_mask_3: 0.75  loss_dice_3: 0.8436  loss_ce_4: 0.2187  loss_cate_4: 0  loss_mask_4: 0.7536  loss_dice_4: 0.8822  loss_ce_5: 0.194  loss_cate_5: 0  loss_mask_5: 0.7188  loss_dice_5: 0.857  loss_ce_6: 0.1799  loss_cate_6: 0  loss_mask_6: 0.7339  loss_dice_6: 0.8648  loss_ce_7: 0.2291  loss_cate_7: 0  loss_mask_7: 0.7676  loss_dice_7: 0.8672  loss_ce_8: 0.1668  loss_cate_8: 0  loss_mask_8: 0.7663  loss_dice_8: 0.8766  time: 1.1025  data_time: 0.0145  lr: 8.8244e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:10:16 d2.utils.events]: \u001b[0m eta: 21:16:18  iter: 10399  total_loss: 20.01  loss_ce: 0.1405  loss_cate: 0.1485  loss_mask: 0.7778  loss_dice: 0.9009  loss_ce_0: 0.2736  loss_cate_0: 0  loss_mask_0: 0.7881  loss_dice_0: 0.8791  loss_ce_1: 0.2115  loss_cate_1: 0  loss_mask_1: 0.7736  loss_dice_1: 0.8848  loss_ce_2: 0.2442  loss_cate_2: 0  loss_mask_2: 0.7738  loss_dice_2: 0.866  loss_ce_3: 0.1687  loss_cate_3: 0  loss_mask_3: 0.786  loss_dice_3: 0.8623  loss_ce_4: 0.177  loss_cate_4: 0  loss_mask_4: 0.7952  loss_dice_4: 0.9035  loss_ce_5: 0.1019  loss_cate_5: 0  loss_mask_5: 0.7795  loss_dice_5: 0.9398  loss_ce_6: 0.1354  loss_cate_6: 0  loss_mask_6: 0.7746  loss_dice_6: 0.8656  loss_ce_7: 0.1003  loss_cate_7: 0  loss_mask_7: 0.7611  loss_dice_7: 0.8792  loss_ce_8: 0.1642  loss_cate_8: 0  loss_mask_8: 0.7862  loss_dice_8: 0.8833  time: 1.1025  data_time: 0.0133  lr: 8.8221e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:10:39 d2.utils.events]: \u001b[0m eta: 21:15:51  iter: 10419  total_loss: 21.49  loss_ce: 0.1936  loss_cate: 0.1319  loss_mask: 0.9048  loss_dice: 0.9511  loss_ce_0: 0.297  loss_cate_0: 0  loss_mask_0: 0.9025  loss_dice_0: 0.9786  loss_ce_1: 0.2006  loss_cate_1: 0  loss_mask_1: 0.8988  loss_dice_1: 0.9538  loss_ce_2: 0.1983  loss_cate_2: 0  loss_mask_2: 0.8794  loss_dice_2: 0.9517  loss_ce_3: 0.1976  loss_cate_3: 0  loss_mask_3: 0.8795  loss_dice_3: 0.9525  loss_ce_4: 0.2044  loss_cate_4: 0  loss_mask_4: 0.8731  loss_dice_4: 0.9295  loss_ce_5: 0.1907  loss_cate_5: 0  loss_mask_5: 0.8748  loss_dice_5: 0.9572  loss_ce_6: 0.199  loss_cate_6: 0  loss_mask_6: 0.9015  loss_dice_6: 0.9618  loss_ce_7: 0.1778  loss_cate_7: 0  loss_mask_7: 0.9177  loss_dice_7: 0.9574  loss_ce_8: 0.1981  loss_cate_8: 0  loss_mask_8: 0.9174  loss_dice_8: 0.966  time: 1.1025  data_time: 0.0128  lr: 8.8198e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:11:01 d2.utils.events]: \u001b[0m eta: 21:15:33  iter: 10439  total_loss: 19.14  loss_ce: 0.2  loss_cate: 0.1209  loss_mask: 0.7171  loss_dice: 0.9293  loss_ce_0: 0.3296  loss_cate_0: 0  loss_mask_0: 0.7358  loss_dice_0: 0.9567  loss_ce_1: 0.2212  loss_cate_1: 0  loss_mask_1: 0.6873  loss_dice_1: 0.8982  loss_ce_2: 0.209  loss_cate_2: 0  loss_mask_2: 0.708  loss_dice_2: 0.8932  loss_ce_3: 0.1754  loss_cate_3: 0  loss_mask_3: 0.7123  loss_dice_3: 0.9821  loss_ce_4: 0.1123  loss_cate_4: 0  loss_mask_4: 0.7245  loss_dice_4: 0.9733  loss_ce_5: 0.1912  loss_cate_5: 0  loss_mask_5: 0.7219  loss_dice_5: 0.9268  loss_ce_6: 0.1812  loss_cate_6: 0  loss_mask_6: 0.7411  loss_dice_6: 0.9312  loss_ce_7: 0.1733  loss_cate_7: 0  loss_mask_7: 0.7363  loss_dice_7: 1.012  loss_ce_8: 0.1477  loss_cate_8: 0  loss_mask_8: 0.7382  loss_dice_8: 0.9271  time: 1.1025  data_time: 0.0139  lr: 8.8176e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:11:23 d2.utils.events]: \u001b[0m eta: 21:14:54  iter: 10459  total_loss: 18.76  loss_ce: 0.1794  loss_cate: 0.1253  loss_mask: 0.7269  loss_dice: 0.8752  loss_ce_0: 0.1853  loss_cate_0: 0  loss_mask_0: 0.7241  loss_dice_0: 0.9038  loss_ce_1: 0.1978  loss_cate_1: 0  loss_mask_1: 0.7121  loss_dice_1: 0.9181  loss_ce_2: 0.13  loss_cate_2: 0  loss_mask_2: 0.74  loss_dice_2: 0.8573  loss_ce_3: 0.1097  loss_cate_3: 0  loss_mask_3: 0.7348  loss_dice_3: 0.8736  loss_ce_4: 0.1508  loss_cate_4: 0  loss_mask_4: 0.7377  loss_dice_4: 0.8202  loss_ce_5: 0.1362  loss_cate_5: 0  loss_mask_5: 0.7144  loss_dice_5: 0.8119  loss_ce_6: 0.116  loss_cate_6: 0  loss_mask_6: 0.7524  loss_dice_6: 0.88  loss_ce_7: 0.1408  loss_cate_7: 0  loss_mask_7: 0.7612  loss_dice_7: 0.8599  loss_ce_8: 0.09106  loss_cate_8: 0  loss_mask_8: 0.7602  loss_dice_8: 0.8831  time: 1.1025  data_time: 0.0138  lr: 8.8153e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:11:46 d2.utils.events]: \u001b[0m eta: 21:14:32  iter: 10479  total_loss: 22.54  loss_ce: 0.155  loss_cate: 0.1618  loss_mask: 0.9284  loss_dice: 1.041  loss_ce_0: 0.2938  loss_cate_0: 0  loss_mask_0: 0.9626  loss_dice_0: 1.088  loss_ce_1: 0.2144  loss_cate_1: 0  loss_mask_1: 0.9439  loss_dice_1: 1.063  loss_ce_2: 0.2312  loss_cate_2: 0  loss_mask_2: 0.952  loss_dice_2: 1.049  loss_ce_3: 0.1379  loss_cate_3: 0  loss_mask_3: 0.9002  loss_dice_3: 1.078  loss_ce_4: 0.1525  loss_cate_4: 0  loss_mask_4: 0.9129  loss_dice_4: 1.069  loss_ce_5: 0.1642  loss_cate_5: 0  loss_mask_5: 0.9162  loss_dice_5: 1.068  loss_ce_6: 0.2062  loss_cate_6: 0  loss_mask_6: 0.9144  loss_dice_6: 1.051  loss_ce_7: 0.1762  loss_cate_7: 0  loss_mask_7: 0.9228  loss_dice_7: 1.065  loss_ce_8: 0.1574  loss_cate_8: 0  loss_mask_8: 0.9138  loss_dice_8: 1.061  time: 1.1025  data_time: 0.0158  lr: 8.813e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:12:11 d2.utils.events]: \u001b[0m eta: 21:14:02  iter: 10499  total_loss: 17.03  loss_ce: 0.1742  loss_cate: 0.1465  loss_mask: 0.7505  loss_dice: 0.762  loss_ce_0: 0.2967  loss_cate_0: 0  loss_mask_0: 0.7807  loss_dice_0: 0.8125  loss_ce_1: 0.1474  loss_cate_1: 0  loss_mask_1: 0.7746  loss_dice_1: 0.8343  loss_ce_2: 0.1162  loss_cate_2: 0  loss_mask_2: 0.7906  loss_dice_2: 0.7689  loss_ce_3: 0.08365  loss_cate_3: 0  loss_mask_3: 0.7691  loss_dice_3: 0.802  loss_ce_4: 0.08411  loss_cate_4: 0  loss_mask_4: 0.7634  loss_dice_4: 0.8037  loss_ce_5: 0.1088  loss_cate_5: 0  loss_mask_5: 0.7697  loss_dice_5: 0.7596  loss_ce_6: 0.08884  loss_cate_6: 0  loss_mask_6: 0.7653  loss_dice_6: 0.7975  loss_ce_7: 0.1884  loss_cate_7: 0  loss_mask_7: 0.751  loss_dice_7: 0.7925  loss_ce_8: 0.1181  loss_cate_8: 0  loss_mask_8: 0.7525  loss_dice_8: 0.8084  time: 1.1025  data_time: 0.0126  lr: 8.8107e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:12:33 d2.utils.events]: \u001b[0m eta: 21:13:40  iter: 10519  total_loss: 18.98  loss_ce: 0.1418  loss_cate: 0.154  loss_mask: 0.796  loss_dice: 0.896  loss_ce_0: 0.2993  loss_cate_0: 0  loss_mask_0: 0.8208  loss_dice_0: 0.9353  loss_ce_1: 0.1828  loss_cate_1: 0  loss_mask_1: 0.7548  loss_dice_1: 0.9053  loss_ce_2: 0.2053  loss_cate_2: 0  loss_mask_2: 0.7925  loss_dice_2: 0.8895  loss_ce_3: 0.1575  loss_cate_3: 0  loss_mask_3: 0.7642  loss_dice_3: 0.8891  loss_ce_4: 0.1796  loss_cate_4: 0  loss_mask_4: 0.7819  loss_dice_4: 0.8847  loss_ce_5: 0.2094  loss_cate_5: 0  loss_mask_5: 0.7748  loss_dice_5: 0.9094  loss_ce_6: 0.1955  loss_cate_6: 0  loss_mask_6: 0.7858  loss_dice_6: 0.8975  loss_ce_7: 0.1572  loss_cate_7: 0  loss_mask_7: 0.8022  loss_dice_7: 0.9176  loss_ce_8: 0.1622  loss_cate_8: 0  loss_mask_8: 0.7644  loss_dice_8: 0.9384  time: 1.1025  data_time: 0.0130  lr: 8.8084e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:12:56 d2.utils.events]: \u001b[0m eta: 21:13:11  iter: 10539  total_loss: 22.66  loss_ce: 0.1596  loss_cate: 0.1482  loss_mask: 0.9455  loss_dice: 1.086  loss_ce_0: 0.3155  loss_cate_0: 0  loss_mask_0: 0.9132  loss_dice_0: 1.168  loss_ce_1: 0.2172  loss_cate_1: 0  loss_mask_1: 0.9585  loss_dice_1: 1.192  loss_ce_2: 0.259  loss_cate_2: 0  loss_mask_2: 0.9537  loss_dice_2: 1.153  loss_ce_3: 0.1735  loss_cate_3: 0  loss_mask_3: 0.9394  loss_dice_3: 1.135  loss_ce_4: 0.1765  loss_cate_4: 0  loss_mask_4: 0.9498  loss_dice_4: 1.182  loss_ce_5: 0.2124  loss_cate_5: 0  loss_mask_5: 0.9375  loss_dice_5: 1.121  loss_ce_6: 0.2138  loss_cate_6: 0  loss_mask_6: 0.9331  loss_dice_6: 1.1  loss_ce_7: 0.2031  loss_cate_7: 0  loss_mask_7: 0.9295  loss_dice_7: 1.159  loss_ce_8: 0.2031  loss_cate_8: 0  loss_mask_8: 0.9613  loss_dice_8: 1.113  time: 1.1025  data_time: 0.0135  lr: 8.8061e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:13:18 d2.utils.events]: \u001b[0m eta: 21:12:49  iter: 10559  total_loss: 21.44  loss_ce: 0.2611  loss_cate: 0.1516  loss_mask: 0.7235  loss_dice: 0.967  loss_ce_0: 0.2656  loss_cate_0: 0  loss_mask_0: 0.7961  loss_dice_0: 1.028  loss_ce_1: 0.2445  loss_cate_1: 0  loss_mask_1: 0.7648  loss_dice_1: 1.021  loss_ce_2: 0.2612  loss_cate_2: 0  loss_mask_2: 0.7646  loss_dice_2: 1.038  loss_ce_3: 0.2559  loss_cate_3: 0  loss_mask_3: 0.7468  loss_dice_3: 0.9911  loss_ce_4: 0.2305  loss_cate_4: 0  loss_mask_4: 0.7682  loss_dice_4: 1  loss_ce_5: 0.232  loss_cate_5: 0  loss_mask_5: 0.7341  loss_dice_5: 1.01  loss_ce_6: 0.2021  loss_cate_6: 0  loss_mask_6: 0.7764  loss_dice_6: 1.001  loss_ce_7: 0.2557  loss_cate_7: 0  loss_mask_7: 0.7053  loss_dice_7: 1.009  loss_ce_8: 0.2273  loss_cate_8: 0  loss_mask_8: 0.7118  loss_dice_8: 0.9975  time: 1.1024  data_time: 0.0126  lr: 8.8039e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:13:41 d2.utils.events]: \u001b[0m eta: 21:12:21  iter: 10579  total_loss: 21.59  loss_ce: 0.2336  loss_cate: 0.1618  loss_mask: 0.8114  loss_dice: 1.05  loss_ce_0: 0.3514  loss_cate_0: 0  loss_mask_0: 0.8324  loss_dice_0: 1.039  loss_ce_1: 0.2726  loss_cate_1: 0  loss_mask_1: 0.844  loss_dice_1: 1.016  loss_ce_2: 0.2553  loss_cate_2: 0  loss_mask_2: 0.8186  loss_dice_2: 1.01  loss_ce_3: 0.2295  loss_cate_3: 0  loss_mask_3: 0.8224  loss_dice_3: 1.013  loss_ce_4: 0.2684  loss_cate_4: 0  loss_mask_4: 0.8097  loss_dice_4: 0.9928  loss_ce_5: 0.2357  loss_cate_5: 0  loss_mask_5: 0.8044  loss_dice_5: 1.022  loss_ce_6: 0.2104  loss_cate_6: 0  loss_mask_6: 0.8123  loss_dice_6: 0.9824  loss_ce_7: 0.2262  loss_cate_7: 0  loss_mask_7: 0.8069  loss_dice_7: 1.019  loss_ce_8: 0.2149  loss_cate_8: 0  loss_mask_8: 0.8137  loss_dice_8: 1.011  time: 1.1024  data_time: 0.0129  lr: 8.8016e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:14:03 d2.utils.events]: \u001b[0m eta: 21:11:56  iter: 10599  total_loss: 22.76  loss_ce: 0.2246  loss_cate: 0.1705  loss_mask: 0.8256  loss_dice: 1.084  loss_ce_0: 0.3708  loss_cate_0: 0  loss_mask_0: 0.8725  loss_dice_0: 1.098  loss_ce_1: 0.2247  loss_cate_1: 0  loss_mask_1: 0.8543  loss_dice_1: 1.071  loss_ce_2: 0.1739  loss_cate_2: 0  loss_mask_2: 0.8546  loss_dice_2: 1.099  loss_ce_3: 0.1884  loss_cate_3: 0  loss_mask_3: 0.8236  loss_dice_3: 1.08  loss_ce_4: 0.188  loss_cate_4: 0  loss_mask_4: 0.8379  loss_dice_4: 1.072  loss_ce_5: 0.2012  loss_cate_5: 0  loss_mask_5: 0.8403  loss_dice_5: 1.087  loss_ce_6: 0.1761  loss_cate_6: 0  loss_mask_6: 0.8255  loss_dice_6: 1.069  loss_ce_7: 0.1899  loss_cate_7: 0  loss_mask_7: 0.8389  loss_dice_7: 1.099  loss_ce_8: 0.1538  loss_cate_8: 0  loss_mask_8: 0.8357  loss_dice_8: 1.087  time: 1.1025  data_time: 0.0133  lr: 8.7993e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:14:25 d2.utils.events]: \u001b[0m eta: 21:11:32  iter: 10619  total_loss: 20.44  loss_ce: 0.2122  loss_cate: 0.1406  loss_mask: 0.79  loss_dice: 1.021  loss_ce_0: 0.2925  loss_cate_0: 0  loss_mask_0: 0.8131  loss_dice_0: 1.146  loss_ce_1: 0.2091  loss_cate_1: 0  loss_mask_1: 0.7891  loss_dice_1: 1.043  loss_ce_2: 0.1522  loss_cate_2: 0  loss_mask_2: 0.8164  loss_dice_2: 1.034  loss_ce_3: 0.172  loss_cate_3: 0  loss_mask_3: 0.7915  loss_dice_3: 1.019  loss_ce_4: 0.1451  loss_cate_4: 0  loss_mask_4: 0.7999  loss_dice_4: 1.031  loss_ce_5: 0.1324  loss_cate_5: 0  loss_mask_5: 0.8007  loss_dice_5: 1.058  loss_ce_6: 0.175  loss_cate_6: 0  loss_mask_6: 0.7959  loss_dice_6: 1.003  loss_ce_7: 0.2057  loss_cate_7: 0  loss_mask_7: 0.794  loss_dice_7: 1.007  loss_ce_8: 0.1602  loss_cate_8: 0  loss_mask_8: 0.8018  loss_dice_8: 1.052  time: 1.1024  data_time: 0.0129  lr: 8.797e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:14:48 d2.utils.events]: \u001b[0m eta: 21:11:17  iter: 10639  total_loss: 21.61  loss_ce: 0.2747  loss_cate: 0.1614  loss_mask: 0.7874  loss_dice: 1.053  loss_ce_0: 0.3775  loss_cate_0: 0  loss_mask_0: 0.8226  loss_dice_0: 1.044  loss_ce_1: 0.2668  loss_cate_1: 0  loss_mask_1: 0.7746  loss_dice_1: 1.105  loss_ce_2: 0.2768  loss_cate_2: 0  loss_mask_2: 0.7828  loss_dice_2: 0.9905  loss_ce_3: 0.2411  loss_cate_3: 0  loss_mask_3: 0.7861  loss_dice_3: 1.003  loss_ce_4: 0.2328  loss_cate_4: 0  loss_mask_4: 0.7874  loss_dice_4: 1.018  loss_ce_5: 0.2464  loss_cate_5: 0  loss_mask_5: 0.7911  loss_dice_5: 1.056  loss_ce_6: 0.2767  loss_cate_6: 0  loss_mask_6: 0.7885  loss_dice_6: 1.055  loss_ce_7: 0.2321  loss_cate_7: 0  loss_mask_7: 0.7869  loss_dice_7: 1.04  loss_ce_8: 0.2215  loss_cate_8: 0  loss_mask_8: 0.805  loss_dice_8: 1.052  time: 1.1024  data_time: 0.0124  lr: 8.7947e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:15:10 d2.utils.events]: \u001b[0m eta: 21:10:51  iter: 10659  total_loss: 20.08  loss_ce: 0.2438  loss_cate: 0.1774  loss_mask: 0.767  loss_dice: 0.9684  loss_ce_0: 0.3436  loss_cate_0: 0  loss_mask_0: 0.8443  loss_dice_0: 1.01  loss_ce_1: 0.3187  loss_cate_1: 0  loss_mask_1: 0.8244  loss_dice_1: 0.9884  loss_ce_2: 0.2399  loss_cate_2: 0  loss_mask_2: 0.7835  loss_dice_2: 0.9509  loss_ce_3: 0.2357  loss_cate_3: 0  loss_mask_3: 0.7747  loss_dice_3: 0.9428  loss_ce_4: 0.2183  loss_cate_4: 0  loss_mask_4: 0.779  loss_dice_4: 0.9627  loss_ce_5: 0.2492  loss_cate_5: 0  loss_mask_5: 0.7812  loss_dice_5: 0.9362  loss_ce_6: 0.2368  loss_cate_6: 0  loss_mask_6: 0.7856  loss_dice_6: 0.9307  loss_ce_7: 0.2502  loss_cate_7: 0  loss_mask_7: 0.7794  loss_dice_7: 0.9297  loss_ce_8: 0.2354  loss_cate_8: 0  loss_mask_8: 0.7698  loss_dice_8: 0.951  time: 1.1024  data_time: 0.0128  lr: 8.7925e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:15:34 d2.utils.events]: \u001b[0m eta: 21:10:23  iter: 10679  total_loss: 17.6  loss_ce: 0.1492  loss_cate: 0.12  loss_mask: 0.76  loss_dice: 0.8012  loss_ce_0: 0.2502  loss_cate_0: 0  loss_mask_0: 0.7283  loss_dice_0: 0.8462  loss_ce_1: 0.1969  loss_cate_1: 0  loss_mask_1: 0.7456  loss_dice_1: 0.8409  loss_ce_2: 0.1671  loss_cate_2: 0  loss_mask_2: 0.7165  loss_dice_2: 0.8132  loss_ce_3: 0.1385  loss_cate_3: 0  loss_mask_3: 0.7081  loss_dice_3: 0.8136  loss_ce_4: 0.1335  loss_cate_4: 0  loss_mask_4: 0.7575  loss_dice_4: 0.8202  loss_ce_5: 0.1027  loss_cate_5: 0  loss_mask_5: 0.7222  loss_dice_5: 0.8257  loss_ce_6: 0.1037  loss_cate_6: 0  loss_mask_6: 0.7402  loss_dice_6: 0.8127  loss_ce_7: 0.09822  loss_cate_7: 0  loss_mask_7: 0.7303  loss_dice_7: 0.8595  loss_ce_8: 0.09855  loss_cate_8: 0  loss_mask_8: 0.7295  loss_dice_8: 0.8382  time: 1.1024  data_time: 0.0132  lr: 8.7902e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:15:56 d2.utils.events]: \u001b[0m eta: 21:10:04  iter: 10699  total_loss: 17.69  loss_ce: 0.1948  loss_cate: 0.1281  loss_mask: 0.7069  loss_dice: 0.8829  loss_ce_0: 0.3326  loss_cate_0: 0  loss_mask_0: 0.7186  loss_dice_0: 0.9644  loss_ce_1: 0.2382  loss_cate_1: 0  loss_mask_1: 0.692  loss_dice_1: 0.8865  loss_ce_2: 0.2582  loss_cate_2: 0  loss_mask_2: 0.7256  loss_dice_2: 0.8815  loss_ce_3: 0.2424  loss_cate_3: 0  loss_mask_3: 0.6995  loss_dice_3: 0.9092  loss_ce_4: 0.2368  loss_cate_4: 0  loss_mask_4: 0.7043  loss_dice_4: 0.8756  loss_ce_5: 0.2033  loss_cate_5: 0  loss_mask_5: 0.729  loss_dice_5: 0.9136  loss_ce_6: 0.178  loss_cate_6: 0  loss_mask_6: 0.7408  loss_dice_6: 0.9233  loss_ce_7: 0.2217  loss_cate_7: 0  loss_mask_7: 0.744  loss_dice_7: 0.9322  loss_ce_8: 0.2317  loss_cate_8: 0  loss_mask_8: 0.718  loss_dice_8: 0.9011  time: 1.1024  data_time: 0.0136  lr: 8.7879e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:16:18 d2.utils.events]: \u001b[0m eta: 21:09:35  iter: 10719  total_loss: 18.37  loss_ce: 0.1863  loss_cate: 0.1541  loss_mask: 0.7756  loss_dice: 0.9021  loss_ce_0: 0.3097  loss_cate_0: 0  loss_mask_0: 0.8225  loss_dice_0: 0.9524  loss_ce_1: 0.1808  loss_cate_1: 0  loss_mask_1: 0.7736  loss_dice_1: 0.9397  loss_ce_2: 0.1231  loss_cate_2: 0  loss_mask_2: 0.7759  loss_dice_2: 0.931  loss_ce_3: 0.1097  loss_cate_3: 0  loss_mask_3: 0.7875  loss_dice_3: 0.9187  loss_ce_4: 0.1139  loss_cate_4: 0  loss_mask_4: 0.7947  loss_dice_4: 0.9385  loss_ce_5: 0.09467  loss_cate_5: 0  loss_mask_5: 0.7779  loss_dice_5: 0.927  loss_ce_6: 0.111  loss_cate_6: 0  loss_mask_6: 0.7768  loss_dice_6: 0.9095  loss_ce_7: 0.1118  loss_cate_7: 0  loss_mask_7: 0.7777  loss_dice_7: 0.8888  loss_ce_8: 0.1604  loss_cate_8: 0  loss_mask_8: 0.7801  loss_dice_8: 0.9327  time: 1.1024  data_time: 0.0133  lr: 8.7856e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:16:41 d2.utils.events]: \u001b[0m eta: 21:09:12  iter: 10739  total_loss: 19.77  loss_ce: 0.2479  loss_cate: 0.1411  loss_mask: 0.8296  loss_dice: 0.8884  loss_ce_0: 0.3316  loss_cate_0: 0  loss_mask_0: 0.8415  loss_dice_0: 0.9194  loss_ce_1: 0.2491  loss_cate_1: 0  loss_mask_1: 0.8195  loss_dice_1: 0.9363  loss_ce_2: 0.2439  loss_cate_2: 0  loss_mask_2: 0.8284  loss_dice_2: 0.9557  loss_ce_3: 0.2204  loss_cate_3: 0  loss_mask_3: 0.8425  loss_dice_3: 0.9336  loss_ce_4: 0.2195  loss_cate_4: 0  loss_mask_4: 0.8411  loss_dice_4: 0.9285  loss_ce_5: 0.2287  loss_cate_5: 0  loss_mask_5: 0.841  loss_dice_5: 0.9321  loss_ce_6: 0.2417  loss_cate_6: 0  loss_mask_6: 0.8178  loss_dice_6: 0.9413  loss_ce_7: 0.2556  loss_cate_7: 0  loss_mask_7: 0.7836  loss_dice_7: 0.8818  loss_ce_8: 0.2214  loss_cate_8: 0  loss_mask_8: 0.8031  loss_dice_8: 0.9233  time: 1.1024  data_time: 0.0128  lr: 8.7833e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:17:03 d2.utils.events]: \u001b[0m eta: 21:08:32  iter: 10759  total_loss: 18.22  loss_ce: 0.1119  loss_cate: 0.1352  loss_mask: 0.6785  loss_dice: 0.9149  loss_ce_0: 0.3068  loss_cate_0: 0  loss_mask_0: 0.6929  loss_dice_0: 0.9444  loss_ce_1: 0.168  loss_cate_1: 0  loss_mask_1: 0.6981  loss_dice_1: 0.9527  loss_ce_2: 0.2153  loss_cate_2: 0  loss_mask_2: 0.6759  loss_dice_2: 0.8879  loss_ce_3: 0.2075  loss_cate_3: 0  loss_mask_3: 0.6914  loss_dice_3: 0.8676  loss_ce_4: 0.1373  loss_cate_4: 0  loss_mask_4: 0.68  loss_dice_4: 0.8739  loss_ce_5: 0.1445  loss_cate_5: 0  loss_mask_5: 0.6883  loss_dice_5: 0.8979  loss_ce_6: 0.1415  loss_cate_6: 0  loss_mask_6: 0.6783  loss_dice_6: 0.9142  loss_ce_7: 0.1336  loss_cate_7: 0  loss_mask_7: 0.6709  loss_dice_7: 0.907  loss_ce_8: 0.1226  loss_cate_8: 0  loss_mask_8: 0.6761  loss_dice_8: 0.919  time: 1.1024  data_time: 0.0132  lr: 8.781e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:17:25 d2.utils.events]: \u001b[0m eta: 21:08:09  iter: 10779  total_loss: 21.06  loss_ce: 0.2385  loss_cate: 0.1515  loss_mask: 0.8294  loss_dice: 1.036  loss_ce_0: 0.2924  loss_cate_0: 0  loss_mask_0: 0.7833  loss_dice_0: 1.076  loss_ce_1: 0.2435  loss_cate_1: 0  loss_mask_1: 0.8333  loss_dice_1: 1.088  loss_ce_2: 0.2611  loss_cate_2: 0  loss_mask_2: 0.841  loss_dice_2: 1.01  loss_ce_3: 0.158  loss_cate_3: 0  loss_mask_3: 0.7991  loss_dice_3: 0.9938  loss_ce_4: 0.1306  loss_cate_4: 0  loss_mask_4: 0.7885  loss_dice_4: 0.9982  loss_ce_5: 0.2143  loss_cate_5: 0  loss_mask_5: 0.8203  loss_dice_5: 0.9839  loss_ce_6: 0.1515  loss_cate_6: 0  loss_mask_6: 0.7906  loss_dice_6: 0.9996  loss_ce_7: 0.2443  loss_cate_7: 0  loss_mask_7: 0.8487  loss_dice_7: 1.059  loss_ce_8: 0.2297  loss_cate_8: 0  loss_mask_8: 0.833  loss_dice_8: 1.063  time: 1.1024  data_time: 0.0142  lr: 8.7788e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:17:48 d2.utils.events]: \u001b[0m eta: 21:07:29  iter: 10799  total_loss: 21.84  loss_ce: 0.2289  loss_cate: 0.1578  loss_mask: 0.9754  loss_dice: 1.002  loss_ce_0: 0.31  loss_cate_0: 0  loss_mask_0: 0.9402  loss_dice_0: 1.211  loss_ce_1: 0.2369  loss_cate_1: 0  loss_mask_1: 0.9547  loss_dice_1: 1.018  loss_ce_2: 0.2895  loss_cate_2: 0  loss_mask_2: 0.9083  loss_dice_2: 1.023  loss_ce_3: 0.2873  loss_cate_3: 0  loss_mask_3: 0.8853  loss_dice_3: 1.014  loss_ce_4: 0.2586  loss_cate_4: 0  loss_mask_4: 0.9323  loss_dice_4: 1.011  loss_ce_5: 0.2943  loss_cate_5: 0  loss_mask_5: 0.864  loss_dice_5: 1.008  loss_ce_6: 0.2514  loss_cate_6: 0  loss_mask_6: 0.9755  loss_dice_6: 1.011  loss_ce_7: 0.2246  loss_cate_7: 0  loss_mask_7: 0.905  loss_dice_7: 1.007  loss_ce_8: 0.2411  loss_cate_8: 0  loss_mask_8: 0.946  loss_dice_8: 1.01  time: 1.1024  data_time: 0.0118  lr: 8.7765e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:18:10 d2.utils.events]: \u001b[0m eta: 21:07:05  iter: 10819  total_loss: 21.56  loss_ce: 0.2443  loss_cate: 0.1652  loss_mask: 0.8774  loss_dice: 0.9707  loss_ce_0: 0.4397  loss_cate_0: 0  loss_mask_0: 0.9211  loss_dice_0: 1.04  loss_ce_1: 0.2601  loss_cate_1: 0  loss_mask_1: 0.9007  loss_dice_1: 1.081  loss_ce_2: 0.3048  loss_cate_2: 0  loss_mask_2: 0.9084  loss_dice_2: 0.9717  loss_ce_3: 0.2607  loss_cate_3: 0  loss_mask_3: 0.8823  loss_dice_3: 0.9735  loss_ce_4: 0.2433  loss_cate_4: 0  loss_mask_4: 0.9222  loss_dice_4: 0.962  loss_ce_5: 0.2561  loss_cate_5: 0  loss_mask_5: 0.8951  loss_dice_5: 0.9837  loss_ce_6: 0.2455  loss_cate_6: 0  loss_mask_6: 0.8648  loss_dice_6: 0.9636  loss_ce_7: 0.2577  loss_cate_7: 0  loss_mask_7: 0.8813  loss_dice_7: 0.9998  loss_ce_8: 0.2665  loss_cate_8: 0  loss_mask_8: 0.8875  loss_dice_8: 1.04  time: 1.1024  data_time: 0.0118  lr: 8.7742e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:18:32 d2.utils.events]: \u001b[0m eta: 21:06:26  iter: 10839  total_loss: 22.15  loss_ce: 0.2588  loss_cate: 0.16  loss_mask: 0.8448  loss_dice: 1.028  loss_ce_0: 0.4249  loss_cate_0: 0  loss_mask_0: 0.8936  loss_dice_0: 1.116  loss_ce_1: 0.2558  loss_cate_1: 0  loss_mask_1: 0.8356  loss_dice_1: 1.099  loss_ce_2: 0.2938  loss_cate_2: 0  loss_mask_2: 0.839  loss_dice_2: 1.063  loss_ce_3: 0.2552  loss_cate_3: 0  loss_mask_3: 0.8504  loss_dice_3: 1.05  loss_ce_4: 0.2373  loss_cate_4: 0  loss_mask_4: 0.8426  loss_dice_4: 1.056  loss_ce_5: 0.2455  loss_cate_5: 0  loss_mask_5: 0.8287  loss_dice_5: 1.054  loss_ce_6: 0.2513  loss_cate_6: 0  loss_mask_6: 0.8337  loss_dice_6: 1.05  loss_ce_7: 0.2545  loss_cate_7: 0  loss_mask_7: 0.8341  loss_dice_7: 1.025  loss_ce_8: 0.242  loss_cate_8: 0  loss_mask_8: 0.8377  loss_dice_8: 1.059  time: 1.1024  data_time: 0.0130  lr: 8.7719e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:18:55 d2.utils.events]: \u001b[0m eta: 21:06:02  iter: 10859  total_loss: 22.19  loss_ce: 0.241  loss_cate: 0.1671  loss_mask: 0.9047  loss_dice: 1.073  loss_ce_0: 0.3102  loss_cate_0: 0  loss_mask_0: 0.9005  loss_dice_0: 1.166  loss_ce_1: 0.2263  loss_cate_1: 0  loss_mask_1: 0.8928  loss_dice_1: 1.096  loss_ce_2: 0.2599  loss_cate_2: 0  loss_mask_2: 0.9005  loss_dice_2: 1.074  loss_ce_3: 0.2762  loss_cate_3: 0  loss_mask_3: 0.8681  loss_dice_3: 1.072  loss_ce_4: 0.1907  loss_cate_4: 0  loss_mask_4: 0.8715  loss_dice_4: 1.078  loss_ce_5: 0.2464  loss_cate_5: 0  loss_mask_5: 0.8868  loss_dice_5: 1.086  loss_ce_6: 0.2356  loss_cate_6: 0  loss_mask_6: 0.8848  loss_dice_6: 1.11  loss_ce_7: 0.2199  loss_cate_7: 0  loss_mask_7: 0.8817  loss_dice_7: 1.114  loss_ce_8: 0.2372  loss_cate_8: 0  loss_mask_8: 0.9027  loss_dice_8: 1.04  time: 1.1024  data_time: 0.0129  lr: 8.7696e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:19:17 d2.utils.events]: \u001b[0m eta: 21:05:38  iter: 10879  total_loss: 20.32  loss_ce: 0.1365  loss_cate: 0.1675  loss_mask: 0.9517  loss_dice: 1.003  loss_ce_0: 0.2501  loss_cate_0: 0  loss_mask_0: 0.961  loss_dice_0: 0.9879  loss_ce_1: 0.1834  loss_cate_1: 0  loss_mask_1: 1.033  loss_dice_1: 0.936  loss_ce_2: 0.1376  loss_cate_2: 0  loss_mask_2: 1.03  loss_dice_2: 0.9996  loss_ce_3: 0.1811  loss_cate_3: 0  loss_mask_3: 1.019  loss_dice_3: 0.9772  loss_ce_4: 0.1152  loss_cate_4: 0  loss_mask_4: 1.025  loss_dice_4: 1.008  loss_ce_5: 0.1127  loss_cate_5: 0  loss_mask_5: 1.013  loss_dice_5: 0.9739  loss_ce_6: 0.1311  loss_cate_6: 0  loss_mask_6: 1.025  loss_dice_6: 0.9944  loss_ce_7: 0.1093  loss_cate_7: 0  loss_mask_7: 0.9961  loss_dice_7: 1.001  loss_ce_8: 0.1063  loss_cate_8: 0  loss_mask_8: 0.9932  loss_dice_8: 0.9952  time: 1.1024  data_time: 0.0137  lr: 8.7673e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:19:40 d2.utils.events]: \u001b[0m eta: 21:05:26  iter: 10899  total_loss: 19.67  loss_ce: 0.1761  loss_cate: 0.1597  loss_mask: 0.8142  loss_dice: 0.9727  loss_ce_0: 0.2831  loss_cate_0: 0  loss_mask_0: 0.8286  loss_dice_0: 1.027  loss_ce_1: 0.2287  loss_cate_1: 0  loss_mask_1: 0.7991  loss_dice_1: 0.9756  loss_ce_2: 0.2266  loss_cate_2: 0  loss_mask_2: 0.8126  loss_dice_2: 0.9771  loss_ce_3: 0.1762  loss_cate_3: 0  loss_mask_3: 0.795  loss_dice_3: 1.006  loss_ce_4: 0.2023  loss_cate_4: 0  loss_mask_4: 0.8078  loss_dice_4: 0.9678  loss_ce_5: 0.2022  loss_cate_5: 0  loss_mask_5: 0.8085  loss_dice_5: 0.961  loss_ce_6: 0.1877  loss_cate_6: 0  loss_mask_6: 0.8058  loss_dice_6: 0.9404  loss_ce_7: 0.1781  loss_cate_7: 0  loss_mask_7: 0.8137  loss_dice_7: 0.9544  loss_ce_8: 0.1874  loss_cate_8: 0  loss_mask_8: 0.8086  loss_dice_8: 0.993  time: 1.1024  data_time: 0.0137  lr: 8.7651e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:20:02 d2.utils.events]: \u001b[0m eta: 21:04:54  iter: 10919  total_loss: 22.14  loss_ce: 0.2465  loss_cate: 0.159  loss_mask: 0.8248  loss_dice: 0.995  loss_ce_0: 0.3476  loss_cate_0: 0  loss_mask_0: 0.8327  loss_dice_0: 0.9425  loss_ce_1: 0.273  loss_cate_1: 0  loss_mask_1: 0.8022  loss_dice_1: 1.001  loss_ce_2: 0.26  loss_cate_2: 0  loss_mask_2: 0.8312  loss_dice_2: 0.931  loss_ce_3: 0.2461  loss_cate_3: 0  loss_mask_3: 0.859  loss_dice_3: 0.9091  loss_ce_4: 0.2566  loss_cate_4: 0  loss_mask_4: 0.8678  loss_dice_4: 0.9193  loss_ce_5: 0.2149  loss_cate_5: 0  loss_mask_5: 0.8815  loss_dice_5: 1.001  loss_ce_6: 0.2545  loss_cate_6: 0  loss_mask_6: 0.8604  loss_dice_6: 0.9303  loss_ce_7: 0.2501  loss_cate_7: 0  loss_mask_7: 0.8409  loss_dice_7: 0.9609  loss_ce_8: 0.295  loss_cate_8: 0  loss_mask_8: 0.8312  loss_dice_8: 0.9754  time: 1.1024  data_time: 0.0120  lr: 8.7628e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:20:27 d2.utils.events]: \u001b[0m eta: 21:04:24  iter: 10939  total_loss: 19.66  loss_ce: 0.2186  loss_cate: 0.1536  loss_mask: 0.7933  loss_dice: 0.9424  loss_ce_0: 0.3318  loss_cate_0: 0  loss_mask_0: 0.8345  loss_dice_0: 0.9777  loss_ce_1: 0.2593  loss_cate_1: 0  loss_mask_1: 0.8096  loss_dice_1: 0.9559  loss_ce_2: 0.2445  loss_cate_2: 0  loss_mask_2: 0.7958  loss_dice_2: 0.9917  loss_ce_3: 0.2816  loss_cate_3: 0  loss_mask_3: 0.8015  loss_dice_3: 0.9059  loss_ce_4: 0.2141  loss_cate_4: 0  loss_mask_4: 0.805  loss_dice_4: 0.9477  loss_ce_5: 0.2242  loss_cate_5: 0  loss_mask_5: 0.7738  loss_dice_5: 0.9313  loss_ce_6: 0.2236  loss_cate_6: 0  loss_mask_6: 0.8005  loss_dice_6: 0.9258  loss_ce_7: 0.2029  loss_cate_7: 0  loss_mask_7: 0.7951  loss_dice_7: 0.976  loss_ce_8: 0.2332  loss_cate_8: 0  loss_mask_8: 0.7821  loss_dice_8: 0.9285  time: 1.1023  data_time: 0.0129  lr: 8.7605e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:20:50 d2.utils.events]: \u001b[0m eta: 21:03:55  iter: 10959  total_loss: 19.35  loss_ce: 0.2073  loss_cate: 0.1403  loss_mask: 0.7282  loss_dice: 0.9831  loss_ce_0: 0.3051  loss_cate_0: 0  loss_mask_0: 0.7612  loss_dice_0: 0.9194  loss_ce_1: 0.2092  loss_cate_1: 0  loss_mask_1: 0.689  loss_dice_1: 0.9168  loss_ce_2: 0.201  loss_cate_2: 0  loss_mask_2: 0.7379  loss_dice_2: 0.9838  loss_ce_3: 0.2072  loss_cate_3: 0  loss_mask_3: 0.7236  loss_dice_3: 0.9569  loss_ce_4: 0.2229  loss_cate_4: 0  loss_mask_4: 0.7302  loss_dice_4: 0.9531  loss_ce_5: 0.2043  loss_cate_5: 0  loss_mask_5: 0.7285  loss_dice_5: 0.934  loss_ce_6: 0.2187  loss_cate_6: 0  loss_mask_6: 0.7189  loss_dice_6: 0.9559  loss_ce_7: 0.2185  loss_cate_7: 0  loss_mask_7: 0.7198  loss_dice_7: 0.9357  loss_ce_8: 0.2291  loss_cate_8: 0  loss_mask_8: 0.7142  loss_dice_8: 0.9483  time: 1.1023  data_time: 0.0126  lr: 8.7582e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:21:12 d2.utils.events]: \u001b[0m eta: 21:03:40  iter: 10979  total_loss: 20.79  loss_ce: 0.2318  loss_cate: 0.1432  loss_mask: 0.7283  loss_dice: 0.9666  loss_ce_0: 0.359  loss_cate_0: 0  loss_mask_0: 0.7488  loss_dice_0: 1.062  loss_ce_1: 0.279  loss_cate_1: 0  loss_mask_1: 0.7335  loss_dice_1: 1.032  loss_ce_2: 0.2528  loss_cate_2: 0  loss_mask_2: 0.747  loss_dice_2: 1.025  loss_ce_3: 0.2229  loss_cate_3: 0  loss_mask_3: 0.7389  loss_dice_3: 0.9996  loss_ce_4: 0.2619  loss_cate_4: 0  loss_mask_4: 0.7462  loss_dice_4: 0.9585  loss_ce_5: 0.302  loss_cate_5: 0  loss_mask_5: 0.7457  loss_dice_5: 0.9799  loss_ce_6: 0.2133  loss_cate_6: 0  loss_mask_6: 0.7198  loss_dice_6: 0.9632  loss_ce_7: 0.2416  loss_cate_7: 0  loss_mask_7: 0.743  loss_dice_7: 0.9703  loss_ce_8: 0.2743  loss_cate_8: 0  loss_mask_8: 0.7498  loss_dice_8: 1.014  time: 1.1023  data_time: 0.0134  lr: 8.7559e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:22:40 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 16:22:40 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 16:22:40 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 16:33:47 d2.utils.events]: \u001b[0m eta: 20:59:46  iter: 11179  total_loss: 21.68  loss_ce: 0.2852  loss_cate: 0.1702  loss_mask: 0.7657  loss_dice: 0.9703  loss_ce_0: 0.3574  loss_cate_0: 0  loss_mask_0: 0.8844  loss_dice_0: 1.057  loss_ce_1: 0.3223  loss_cate_1: 0  loss_mask_1: 0.7767  loss_dice_1: 1.025  loss_ce_2: 0.2655  loss_cate_2: 0  loss_mask_2: 0.7897  loss_dice_2: 0.9829  loss_ce_3: 0.3071  loss_cate_3: 0  loss_mask_3: 0.7631  loss_dice_3: 0.9481  loss_ce_4: 0.2721  loss_cate_4: 0  loss_mask_4: 0.7583  loss_dice_4: 0.9689  loss_ce_5: 0.2594  loss_cate_5: 0  loss_mask_5: 0.7574  loss_dice_5: 0.976  loss_ce_6: 0.2553  loss_cate_6: 0  loss_mask_6: 0.7583  loss_dice_6: 0.9524  loss_ce_7: 0.2241  loss_cate_7: 0  loss_mask_7: 0.7721  loss_dice_7: 0.9615  loss_ce_8: 0.2423  loss_cate_8: 0  loss_mask_8: 0.7603  loss_dice_8: 0.9481  time: 1.1023  data_time: 0.0129  lr: 8.7331e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:34:10 d2.utils.events]: \u001b[0m eta: 20:59:27  iter: 11199  total_loss: 20.44  loss_ce: 0.2065  loss_cate: 0.1364  loss_mask: 0.7179  loss_dice: 1.001  loss_ce_0: 0.3453  loss_cate_0: 0  loss_mask_0: 0.7144  loss_dice_0: 1.132  loss_ce_1: 0.2363  loss_cate_1: 0  loss_mask_1: 0.7042  loss_dice_1: 1.084  loss_ce_2: 0.2304  loss_cate_2: 0  loss_mask_2: 0.7066  loss_dice_2: 0.9891  loss_ce_3: 0.2419  loss_cate_3: 0  loss_mask_3: 0.7288  loss_dice_3: 1.033  loss_ce_4: 0.2076  loss_cate_4: 0  loss_mask_4: 0.6817  loss_dice_4: 1.015  loss_ce_5: 0.1824  loss_cate_5: 0  loss_mask_5: 0.7072  loss_dice_5: 0.9983  loss_ce_6: 0.2068  loss_cate_6: 0  loss_mask_6: 0.7032  loss_dice_6: 0.9878  loss_ce_7: 0.2065  loss_cate_7: 0  loss_mask_7: 0.7073  loss_dice_7: 1.007  loss_ce_8: 0.1927  loss_cate_8: 0  loss_mask_8: 0.7162  loss_dice_8: 0.9993  time: 1.1023  data_time: 0.0128  lr: 8.7308e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:34:32 d2.utils.events]: \u001b[0m eta: 20:59:05  iter: 11219  total_loss: 24.09  loss_ce: 0.2684  loss_cate: 0.169  loss_mask: 0.9904  loss_dice: 1.031  loss_ce_0: 0.3471  loss_cate_0: 0  loss_mask_0: 1.001  loss_dice_0: 1.112  loss_ce_1: 0.3341  loss_cate_1: 0  loss_mask_1: 0.9715  loss_dice_1: 1.094  loss_ce_2: 0.2625  loss_cate_2: 0  loss_mask_2: 1.059  loss_dice_2: 1.098  loss_ce_3: 0.2968  loss_cate_3: 0  loss_mask_3: 1.016  loss_dice_3: 1.056  loss_ce_4: 0.2292  loss_cate_4: 0  loss_mask_4: 1.007  loss_dice_4: 1.059  loss_ce_5: 0.2788  loss_cate_5: 0  loss_mask_5: 0.9981  loss_dice_5: 1.077  loss_ce_6: 0.2824  loss_cate_6: 0  loss_mask_6: 0.9777  loss_dice_6: 1.085  loss_ce_7: 0.2722  loss_cate_7: 0  loss_mask_7: 0.9862  loss_dice_7: 1.064  loss_ce_8: 0.253  loss_cate_8: 0  loss_mask_8: 0.9785  loss_dice_8: 1.051  time: 1.1023  data_time: 0.0142  lr: 8.7285e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:34:55 d2.utils.events]: \u001b[0m eta: 20:58:45  iter: 11239  total_loss: 20.91  loss_ce: 0.2723  loss_cate: 0.1584  loss_mask: 0.7848  loss_dice: 1.044  loss_ce_0: 0.3265  loss_cate_0: 0  loss_mask_0: 0.7583  loss_dice_0: 1.13  loss_ce_1: 0.2061  loss_cate_1: 0  loss_mask_1: 0.7898  loss_dice_1: 1.081  loss_ce_2: 0.2924  loss_cate_2: 0  loss_mask_2: 0.7452  loss_dice_2: 1.076  loss_ce_3: 0.371  loss_cate_3: 0  loss_mask_3: 0.7514  loss_dice_3: 1.033  loss_ce_4: 0.2777  loss_cate_4: 0  loss_mask_4: 0.7484  loss_dice_4: 0.997  loss_ce_5: 0.2634  loss_cate_5: 0  loss_mask_5: 0.7598  loss_dice_5: 1.021  loss_ce_6: 0.2567  loss_cate_6: 0  loss_mask_6: 0.7455  loss_dice_6: 1.075  loss_ce_7: 0.2656  loss_cate_7: 0  loss_mask_7: 0.7451  loss_dice_7: 1.049  loss_ce_8: 0.3033  loss_cate_8: 0  loss_mask_8: 0.7452  loss_dice_8: 1.054  time: 1.1023  data_time: 0.0126  lr: 8.7262e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:35:17 d2.utils.events]: \u001b[0m eta: 20:58:18  iter: 11259  total_loss: 22.31  loss_ce: 0.2432  loss_cate: 0.1492  loss_mask: 0.8551  loss_dice: 1.063  loss_ce_0: 0.2752  loss_cate_0: 0  loss_mask_0: 0.8577  loss_dice_0: 1.039  loss_ce_1: 0.2461  loss_cate_1: 0  loss_mask_1: 0.8474  loss_dice_1: 1.058  loss_ce_2: 0.2523  loss_cate_2: 0  loss_mask_2: 0.8784  loss_dice_2: 1.047  loss_ce_3: 0.2412  loss_cate_3: 0  loss_mask_3: 0.8411  loss_dice_3: 1.02  loss_ce_4: 0.2594  loss_cate_4: 0  loss_mask_4: 0.8526  loss_dice_4: 1.051  loss_ce_5: 0.2507  loss_cate_5: 0  loss_mask_5: 0.8409  loss_dice_5: 1.048  loss_ce_6: 0.258  loss_cate_6: 0  loss_mask_6: 0.8504  loss_dice_6: 1.025  loss_ce_7: 0.2227  loss_cate_7: 0  loss_mask_7: 0.8577  loss_dice_7: 1.018  loss_ce_8: 0.2319  loss_cate_8: 0  loss_mask_8: 0.8525  loss_dice_8: 1.056  time: 1.1023  data_time: 0.0125  lr: 8.724e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:35:40 d2.utils.events]: \u001b[0m eta: 20:57:55  iter: 11279  total_loss: 20.71  loss_ce: 0.2193  loss_cate: 0.1596  loss_mask: 0.8226  loss_dice: 0.9754  loss_ce_0: 0.3806  loss_cate_0: 0  loss_mask_0: 0.8476  loss_dice_0: 0.9974  loss_ce_1: 0.2316  loss_cate_1: 0  loss_mask_1: 0.834  loss_dice_1: 1.023  loss_ce_2: 0.17  loss_cate_2: 0  loss_mask_2: 0.8639  loss_dice_2: 0.9777  loss_ce_3: 0.1478  loss_cate_3: 0  loss_mask_3: 0.8447  loss_dice_3: 0.9974  loss_ce_4: 0.1224  loss_cate_4: 0  loss_mask_4: 0.8477  loss_dice_4: 0.9772  loss_ce_5: 0.1935  loss_cate_5: 0  loss_mask_5: 0.8416  loss_dice_5: 0.9684  loss_ce_6: 0.1826  loss_cate_6: 0  loss_mask_6: 0.8669  loss_dice_6: 0.9533  loss_ce_7: 0.1174  loss_cate_7: 0  loss_mask_7: 0.8744  loss_dice_7: 1.011  loss_ce_8: 0.1979  loss_cate_8: 0  loss_mask_8: 0.8687  loss_dice_8: 0.9695  time: 1.1023  data_time: 0.0140  lr: 8.7217e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:36:03 d2.utils.events]: \u001b[0m eta: 20:57:17  iter: 11299  total_loss: 20.33  loss_ce: 0.2036  loss_cate: 0.1456  loss_mask: 0.8071  loss_dice: 0.8952  loss_ce_0: 0.4011  loss_cate_0: 0  loss_mask_0: 0.8173  loss_dice_0: 0.9686  loss_ce_1: 0.2386  loss_cate_1: 0  loss_mask_1: 0.829  loss_dice_1: 0.9742  loss_ce_2: 0.2129  loss_cate_2: 0  loss_mask_2: 0.8308  loss_dice_2: 1.004  loss_ce_3: 0.2013  loss_cate_3: 0  loss_mask_3: 0.8211  loss_dice_3: 0.9966  loss_ce_4: 0.2416  loss_cate_4: 0  loss_mask_4: 0.8194  loss_dice_4: 0.9333  loss_ce_5: 0.2532  loss_cate_5: 0  loss_mask_5: 0.8206  loss_dice_5: 0.9854  loss_ce_6: 0.2418  loss_cate_6: 0  loss_mask_6: 0.8085  loss_dice_6: 0.94  loss_ce_7: 0.2224  loss_cate_7: 0  loss_mask_7: 0.8143  loss_dice_7: 0.9965  loss_ce_8: 0.2085  loss_cate_8: 0  loss_mask_8: 0.8029  loss_dice_8: 0.9674  time: 1.1023  data_time: 0.0129  lr: 8.7194e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:36:25 d2.utils.events]: \u001b[0m eta: 20:56:54  iter: 11319  total_loss: 19.38  loss_ce: 0.1963  loss_cate: 0.1454  loss_mask: 0.682  loss_dice: 0.9753  loss_ce_0: 0.2606  loss_cate_0: 0  loss_mask_0: 0.6716  loss_dice_0: 1.073  loss_ce_1: 0.2405  loss_cate_1: 0  loss_mask_1: 0.6969  loss_dice_1: 1.038  loss_ce_2: 0.2482  loss_cate_2: 0  loss_mask_2: 0.6968  loss_dice_2: 0.9688  loss_ce_3: 0.2411  loss_cate_3: 0  loss_mask_3: 0.6848  loss_dice_3: 0.9831  loss_ce_4: 0.1975  loss_cate_4: 0  loss_mask_4: 0.6731  loss_dice_4: 0.9818  loss_ce_5: 0.2157  loss_cate_5: 0  loss_mask_5: 0.6725  loss_dice_5: 1.003  loss_ce_6: 0.1793  loss_cate_6: 0  loss_mask_6: 0.6838  loss_dice_6: 0.9948  loss_ce_7: 0.2282  loss_cate_7: 0  loss_mask_7: 0.6817  loss_dice_7: 0.9832  loss_ce_8: 0.2473  loss_cate_8: 0  loss_mask_8: 0.6939  loss_dice_8: 0.979  time: 1.1023  data_time: 0.0155  lr: 8.7171e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:36:48 d2.utils.events]: \u001b[0m eta: 20:56:33  iter: 11339  total_loss: 24.27  loss_ce: 0.2866  loss_cate: 0.1707  loss_mask: 0.8693  loss_dice: 1.122  loss_ce_0: 0.3958  loss_cate_0: 0  loss_mask_0: 0.8883  loss_dice_0: 1.138  loss_ce_1: 0.2889  loss_cate_1: 0  loss_mask_1: 0.8813  loss_dice_1: 1.117  loss_ce_2: 0.2746  loss_cate_2: 0  loss_mask_2: 0.9603  loss_dice_2: 1.172  loss_ce_3: 0.2804  loss_cate_3: 0  loss_mask_3: 0.8597  loss_dice_3: 1.071  loss_ce_4: 0.255  loss_cate_4: 0  loss_mask_4: 0.8879  loss_dice_4: 1.092  loss_ce_5: 0.3713  loss_cate_5: 0  loss_mask_5: 0.8871  loss_dice_5: 1.052  loss_ce_6: 0.2538  loss_cate_6: 0  loss_mask_6: 0.9104  loss_dice_6: 1.134  loss_ce_7: 0.2728  loss_cate_7: 0  loss_mask_7: 0.8883  loss_dice_7: 1.14  loss_ce_8: 0.2638  loss_cate_8: 0  loss_mask_8: 0.8718  loss_dice_8: 1.161  time: 1.1023  data_time: 0.0152  lr: 8.7148e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:37:10 d2.utils.events]: \u001b[0m eta: 20:56:27  iter: 11359  total_loss: 21.39  loss_ce: 0.2272  loss_cate: 0.1539  loss_mask: 0.7827  loss_dice: 1.013  loss_ce_0: 0.3786  loss_cate_0: 0  loss_mask_0: 0.8331  loss_dice_0: 1.066  loss_ce_1: 0.3481  loss_cate_1: 0  loss_mask_1: 0.8422  loss_dice_1: 1.015  loss_ce_2: 0.2833  loss_cate_2: 0  loss_mask_2: 0.7784  loss_dice_2: 1.054  loss_ce_3: 0.2213  loss_cate_3: 0  loss_mask_3: 0.791  loss_dice_3: 1.009  loss_ce_4: 0.2253  loss_cate_4: 0  loss_mask_4: 0.7931  loss_dice_4: 1.03  loss_ce_5: 0.2089  loss_cate_5: 0  loss_mask_5: 0.8024  loss_dice_5: 1.001  loss_ce_6: 0.2202  loss_cate_6: 0  loss_mask_6: 0.7905  loss_dice_6: 1  loss_ce_7: 0.2054  loss_cate_7: 0  loss_mask_7: 0.7859  loss_dice_7: 1.036  loss_ce_8: 0.2403  loss_cate_8: 0  loss_mask_8: 0.7788  loss_dice_8: 1.004  time: 1.1023  data_time: 0.0143  lr: 8.7125e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:37:33 d2.utils.events]: \u001b[0m eta: 20:55:48  iter: 11379  total_loss: 18.25  loss_ce: 0.1934  loss_cate: 0.1215  loss_mask: 0.7028  loss_dice: 0.8757  loss_ce_0: 0.1986  loss_cate_0: 0  loss_mask_0: 0.701  loss_dice_0: 0.9377  loss_ce_1: 0.1744  loss_cate_1: 0  loss_mask_1: 0.711  loss_dice_1: 0.9053  loss_ce_2: 0.1322  loss_cate_2: 0  loss_mask_2: 0.7347  loss_dice_2: 0.8712  loss_ce_3: 0.1554  loss_cate_3: 0  loss_mask_3: 0.7324  loss_dice_3: 0.872  loss_ce_4: 0.1229  loss_cate_4: 0  loss_mask_4: 0.6949  loss_dice_4: 0.8932  loss_ce_5: 0.1408  loss_cate_5: 0  loss_mask_5: 0.7323  loss_dice_5: 0.8633  loss_ce_6: 0.2249  loss_cate_6: 0  loss_mask_6: 0.7087  loss_dice_6: 0.9083  loss_ce_7: 0.1611  loss_cate_7: 0  loss_mask_7: 0.7208  loss_dice_7: 0.8793  loss_ce_8: 0.1792  loss_cate_8: 0  loss_mask_8: 0.7044  loss_dice_8: 0.8869  time: 1.1023  data_time: 0.0158  lr: 8.7102e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:37:55 d2.utils.events]: \u001b[0m eta: 20:55:26  iter: 11399  total_loss: 20.91  loss_ce: 0.1961  loss_cate: 0.1411  loss_mask: 0.7537  loss_dice: 1.027  loss_ce_0: 0.3076  loss_cate_0: 0  loss_mask_0: 0.7479  loss_dice_0: 1.036  loss_ce_1: 0.2425  loss_cate_1: 0  loss_mask_1: 0.7766  loss_dice_1: 1.051  loss_ce_2: 0.2133  loss_cate_2: 0  loss_mask_2: 0.772  loss_dice_2: 1.009  loss_ce_3: 0.2381  loss_cate_3: 0  loss_mask_3: 0.7723  loss_dice_3: 0.987  loss_ce_4: 0.2544  loss_cate_4: 0  loss_mask_4: 0.7564  loss_dice_4: 0.9928  loss_ce_5: 0.2335  loss_cate_5: 0  loss_mask_5: 0.7675  loss_dice_5: 1.035  loss_ce_6: 0.2209  loss_cate_6: 0  loss_mask_6: 0.7529  loss_dice_6: 1.025  loss_ce_7: 0.2392  loss_cate_7: 0  loss_mask_7: 0.7514  loss_dice_7: 1.022  loss_ce_8: 0.2345  loss_cate_8: 0  loss_mask_8: 0.7553  loss_dice_8: 1.018  time: 1.1023  data_time: 0.0138  lr: 8.708e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:38:18 d2.utils.events]: \u001b[0m eta: 20:55:04  iter: 11419  total_loss: 21.49  loss_ce: 0.2044  loss_cate: 0.1459  loss_mask: 0.921  loss_dice: 0.979  loss_ce_0: 0.2678  loss_cate_0: 0  loss_mask_0: 0.9147  loss_dice_0: 1.058  loss_ce_1: 0.2472  loss_cate_1: 0  loss_mask_1: 0.8788  loss_dice_1: 1.015  loss_ce_2: 0.1805  loss_cate_2: 0  loss_mask_2: 0.8561  loss_dice_2: 1.017  loss_ce_3: 0.2147  loss_cate_3: 0  loss_mask_3: 0.8452  loss_dice_3: 0.9947  loss_ce_4: 0.1896  loss_cate_4: 0  loss_mask_4: 0.8496  loss_dice_4: 1.038  loss_ce_5: 0.1972  loss_cate_5: 0  loss_mask_5: 0.893  loss_dice_5: 1.024  loss_ce_6: 0.1803  loss_cate_6: 0  loss_mask_6: 0.904  loss_dice_6: 0.9822  loss_ce_7: 0.2191  loss_cate_7: 0  loss_mask_7: 0.8977  loss_dice_7: 0.9983  loss_ce_8: 0.1996  loss_cate_8: 0  loss_mask_8: 0.9049  loss_dice_8: 0.9795  time: 1.1023  data_time: 0.0137  lr: 8.7057e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:38:41 d2.utils.events]: \u001b[0m eta: 20:54:49  iter: 11439  total_loss: 19.01  loss_ce: 0.2334  loss_cate: 0.1369  loss_mask: 0.7588  loss_dice: 1.001  loss_ce_0: 0.2801  loss_cate_0: 0  loss_mask_0: 0.7251  loss_dice_0: 1.035  loss_ce_1: 0.2663  loss_cate_1: 0  loss_mask_1: 0.715  loss_dice_1: 0.9972  loss_ce_2: 0.266  loss_cate_2: 0  loss_mask_2: 0.7208  loss_dice_2: 0.9746  loss_ce_3: 0.2643  loss_cate_3: 0  loss_mask_3: 0.7117  loss_dice_3: 0.9702  loss_ce_4: 0.2215  loss_cate_4: 0  loss_mask_4: 0.7078  loss_dice_4: 0.9986  loss_ce_5: 0.2369  loss_cate_5: 0  loss_mask_5: 0.7442  loss_dice_5: 0.9772  loss_ce_6: 0.2754  loss_cate_6: 0  loss_mask_6: 0.7491  loss_dice_6: 0.9845  loss_ce_7: 0.2209  loss_cate_7: 0  loss_mask_7: 0.7606  loss_dice_7: 0.9433  loss_ce_8: 0.2296  loss_cate_8: 0  loss_mask_8: 0.7512  loss_dice_8: 0.996  time: 1.1022  data_time: 0.0132  lr: 8.7034e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:39:03 d2.utils.events]: \u001b[0m eta: 20:54:40  iter: 11459  total_loss: 21.58  loss_ce: 0.1951  loss_cate: 0.1604  loss_mask: 0.8888  loss_dice: 0.9889  loss_ce_0: 0.3737  loss_cate_0: 0  loss_mask_0: 0.9485  loss_dice_0: 1.056  loss_ce_1: 0.2016  loss_cate_1: 0  loss_mask_1: 0.8668  loss_dice_1: 1.03  loss_ce_2: 0.2079  loss_cate_2: 0  loss_mask_2: 0.8533  loss_dice_2: 1.022  loss_ce_3: 0.2077  loss_cate_3: 0  loss_mask_3: 0.8748  loss_dice_3: 1.009  loss_ce_4: 0.1813  loss_cate_4: 0  loss_mask_4: 0.85  loss_dice_4: 1.026  loss_ce_5: 0.2113  loss_cate_5: 0  loss_mask_5: 0.8543  loss_dice_5: 1.004  loss_ce_6: 0.2119  loss_cate_6: 0  loss_mask_6: 0.8657  loss_dice_6: 0.9802  loss_ce_7: 0.2246  loss_cate_7: 0  loss_mask_7: 0.8695  loss_dice_7: 1.017  loss_ce_8: 0.2011  loss_cate_8: 0  loss_mask_8: 0.8878  loss_dice_8: 1.046  time: 1.1023  data_time: 0.0153  lr: 8.7011e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:39:48 d2.utils.events]: \u001b[0m eta: 20:54:02  iter: 11499  total_loss: 23.1  loss_ce: 0.2582  loss_cate: 0.1694  loss_mask: 0.9274  loss_dice: 1.009  loss_ce_0: 0.3927  loss_cate_0: 0  loss_mask_0: 1.05  loss_dice_0: 1.03  loss_ce_1: 0.3795  loss_cate_1: 0  loss_mask_1: 0.9969  loss_dice_1: 1.037  loss_ce_2: 0.2632  loss_cate_2: 0  loss_mask_2: 1.002  loss_dice_2: 1.018  loss_ce_3: 0.2822  loss_cate_3: 0  loss_mask_3: 0.9319  loss_dice_3: 0.9864  loss_ce_4: 0.2643  loss_cate_4: 0  loss_mask_4: 0.9035  loss_dice_4: 0.9974  loss_ce_5: 0.2664  loss_cate_5: 0  loss_mask_5: 0.9358  loss_dice_5: 1.015  loss_ce_6: 0.2513  loss_cate_6: 0  loss_mask_6: 0.9751  loss_dice_6: 0.9539  loss_ce_7: 0.2465  loss_cate_7: 0  loss_mask_7: 0.9767  loss_dice_7: 0.9725  loss_ce_8: 0.2566  loss_cate_8: 0  loss_mask_8: 0.9366  loss_dice_8: 0.9703  time: 1.1023  data_time: 0.0139  lr: 8.6965e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:40:10 d2.utils.events]: \u001b[0m eta: 20:53:40  iter: 11519  total_loss: 22.22  loss_ce: 0.2129  loss_cate: 0.1527  loss_mask: 0.769  loss_dice: 1.022  loss_ce_0: 0.2861  loss_cate_0: 0  loss_mask_0: 0.8259  loss_dice_0: 1.112  loss_ce_1: 0.2577  loss_cate_1: 0  loss_mask_1: 0.8006  loss_dice_1: 1.099  loss_ce_2: 0.245  loss_cate_2: 0  loss_mask_2: 0.7559  loss_dice_2: 1.061  loss_ce_3: 0.2577  loss_cate_3: 0  loss_mask_3: 0.7703  loss_dice_3: 1.101  loss_ce_4: 0.2276  loss_cate_4: 0  loss_mask_4: 0.7575  loss_dice_4: 1.059  loss_ce_5: 0.2569  loss_cate_5: 0  loss_mask_5: 0.758  loss_dice_5: 1.099  loss_ce_6: 0.2772  loss_cate_6: 0  loss_mask_6: 0.7514  loss_dice_6: 1.09  loss_ce_7: 0.2305  loss_cate_7: 0  loss_mask_7: 0.7551  loss_dice_7: 1.08  loss_ce_8: 0.2369  loss_cate_8: 0  loss_mask_8: 0.7597  loss_dice_8: 1.066  time: 1.1022  data_time: 0.0133  lr: 8.6942e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:40:32 d2.utils.events]: \u001b[0m eta: 20:53:17  iter: 11539  total_loss: 17.76  loss_ce: 0.1982  loss_cate: 0.1376  loss_mask: 0.6773  loss_dice: 0.8711  loss_ce_0: 0.3092  loss_cate_0: 0  loss_mask_0: 0.6979  loss_dice_0: 0.9427  loss_ce_1: 0.2551  loss_cate_1: 0  loss_mask_1: 0.6741  loss_dice_1: 0.8852  loss_ce_2: 0.2722  loss_cate_2: 0  loss_mask_2: 0.6678  loss_dice_2: 0.8818  loss_ce_3: 0.2393  loss_cate_3: 0  loss_mask_3: 0.6869  loss_dice_3: 0.8894  loss_ce_4: 0.2321  loss_cate_4: 0  loss_mask_4: 0.6697  loss_dice_4: 0.8862  loss_ce_5: 0.2417  loss_cate_5: 0  loss_mask_5: 0.6694  loss_dice_5: 0.855  loss_ce_6: 0.2167  loss_cate_6: 0  loss_mask_6: 0.6752  loss_dice_6: 0.8598  loss_ce_7: 0.2148  loss_cate_7: 0  loss_mask_7: 0.6749  loss_dice_7: 0.8657  loss_ce_8: 0.2023  loss_cate_8: 0  loss_mask_8: 0.6758  loss_dice_8: 0.8898  time: 1.1022  data_time: 0.0126  lr: 8.692e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:40:55 d2.utils.events]: \u001b[0m eta: 20:52:59  iter: 11559  total_loss: 21.48  loss_ce: 0.2284  loss_cate: 0.1746  loss_mask: 0.7312  loss_dice: 0.954  loss_ce_0: 0.4274  loss_cate_0: 0  loss_mask_0: 0.7625  loss_dice_0: 0.98  loss_ce_1: 0.3441  loss_cate_1: 0  loss_mask_1: 0.7257  loss_dice_1: 0.9942  loss_ce_2: 0.247  loss_cate_2: 0  loss_mask_2: 0.6914  loss_dice_2: 0.9764  loss_ce_3: 0.2786  loss_cate_3: 0  loss_mask_3: 0.6832  loss_dice_3: 0.963  loss_ce_4: 0.2911  loss_cate_4: 0  loss_mask_4: 0.7328  loss_dice_4: 0.9189  loss_ce_5: 0.2489  loss_cate_5: 0  loss_mask_5: 0.7336  loss_dice_5: 0.9299  loss_ce_6: 0.2329  loss_cate_6: 0  loss_mask_6: 0.7216  loss_dice_6: 0.9603  loss_ce_7: 0.2208  loss_cate_7: 0  loss_mask_7: 0.7248  loss_dice_7: 0.9737  loss_ce_8: 0.2458  loss_cate_8: 0  loss_mask_8: 0.7267  loss_dice_8: 0.9563  time: 1.1022  data_time: 0.0127  lr: 8.6897e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:41:18 d2.utils.events]: \u001b[0m eta: 20:52:37  iter: 11579  total_loss: 18.6  loss_ce: 0.1853  loss_cate: 0.1512  loss_mask: 0.7942  loss_dice: 0.8903  loss_ce_0: 0.3034  loss_cate_0: 0  loss_mask_0: 0.8022  loss_dice_0: 0.8855  loss_ce_1: 0.2072  loss_cate_1: 0  loss_mask_1: 0.763  loss_dice_1: 0.9076  loss_ce_2: 0.1489  loss_cate_2: 0  loss_mask_2: 0.7635  loss_dice_2: 0.9043  loss_ce_3: 0.1627  loss_cate_3: 0  loss_mask_3: 0.7747  loss_dice_3: 0.9006  loss_ce_4: 0.1465  loss_cate_4: 0  loss_mask_4: 0.7637  loss_dice_4: 0.9043  loss_ce_5: 0.1501  loss_cate_5: 0  loss_mask_5: 0.772  loss_dice_5: 0.8966  loss_ce_6: 0.1332  loss_cate_6: 0  loss_mask_6: 0.7703  loss_dice_6: 0.8972  loss_ce_7: 0.117  loss_cate_7: 0  loss_mask_7: 0.7836  loss_dice_7: 0.9082  loss_ce_8: 0.158  loss_cate_8: 0  loss_mask_8: 0.7792  loss_dice_8: 0.8649  time: 1.1022  data_time: 0.0133  lr: 8.6874e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:41:40 d2.utils.events]: \u001b[0m eta: 20:52:10  iter: 11599  total_loss: 19.3  loss_ce: 0.2279  loss_cate: 0.1553  loss_mask: 0.7861  loss_dice: 0.9185  loss_ce_0: 0.315  loss_cate_0: 0  loss_mask_0: 0.7688  loss_dice_0: 0.9922  loss_ce_1: 0.2956  loss_cate_1: 0  loss_mask_1: 0.7874  loss_dice_1: 0.8768  loss_ce_2: 0.284  loss_cate_2: 0  loss_mask_2: 0.7395  loss_dice_2: 0.8513  loss_ce_3: 0.2467  loss_cate_3: 0  loss_mask_3: 0.7798  loss_dice_3: 0.8546  loss_ce_4: 0.2738  loss_cate_4: 0  loss_mask_4: 0.7613  loss_dice_4: 0.8817  loss_ce_5: 0.2866  loss_cate_5: 0  loss_mask_5: 0.8165  loss_dice_5: 0.885  loss_ce_6: 0.2741  loss_cate_6: 0  loss_mask_6: 0.8105  loss_dice_6: 0.9195  loss_ce_7: 0.2489  loss_cate_7: 0  loss_mask_7: 0.767  loss_dice_7: 0.8606  loss_ce_8: 0.2487  loss_cate_8: 0  loss_mask_8: 0.7737  loss_dice_8: 0.8929  time: 1.1022  data_time: 0.0130  lr: 8.6851e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:42:03 d2.utils.events]: \u001b[0m eta: 20:51:46  iter: 11619  total_loss: 23.16  loss_ce: 0.2149  loss_cate: 0.1676  loss_mask: 0.8569  loss_dice: 1.097  loss_ce_0: 0.2609  loss_cate_0: 0  loss_mask_0: 0.9614  loss_dice_0: 1.163  loss_ce_1: 0.2188  loss_cate_1: 0  loss_mask_1: 0.9026  loss_dice_1: 1.067  loss_ce_2: 0.2626  loss_cate_2: 0  loss_mask_2: 0.8939  loss_dice_2: 1.071  loss_ce_3: 0.2497  loss_cate_3: 0  loss_mask_3: 0.8923  loss_dice_3: 1.112  loss_ce_4: 0.2552  loss_cate_4: 0  loss_mask_4: 0.8893  loss_dice_4: 1.098  loss_ce_5: 0.1861  loss_cate_5: 0  loss_mask_5: 0.8774  loss_dice_5: 1.051  loss_ce_6: 0.2286  loss_cate_6: 0  loss_mask_6: 0.8709  loss_dice_6: 1.094  loss_ce_7: 0.1469  loss_cate_7: 0  loss_mask_7: 0.8753  loss_dice_7: 1.117  loss_ce_8: 0.1659  loss_cate_8: 0  loss_mask_8: 0.8931  loss_dice_8: 1.119  time: 1.1022  data_time: 0.0130  lr: 8.6828e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:42:25 d2.utils.events]: \u001b[0m eta: 20:51:11  iter: 11639  total_loss: 20.57  loss_ce: 0.1812  loss_cate: 0.1669  loss_mask: 0.7832  loss_dice: 0.8723  loss_ce_0: 0.3661  loss_cate_0: 0  loss_mask_0: 0.8017  loss_dice_0: 0.9411  loss_ce_1: 0.227  loss_cate_1: 0  loss_mask_1: 0.8003  loss_dice_1: 0.9401  loss_ce_2: 0.2725  loss_cate_2: 0  loss_mask_2: 0.7577  loss_dice_2: 0.9451  loss_ce_3: 0.2199  loss_cate_3: 0  loss_mask_3: 0.7578  loss_dice_3: 0.9139  loss_ce_4: 0.2155  loss_cate_4: 0  loss_mask_4: 0.7258  loss_dice_4: 0.9005  loss_ce_5: 0.176  loss_cate_5: 0  loss_mask_5: 0.7152  loss_dice_5: 0.8954  loss_ce_6: 0.2058  loss_cate_6: 0  loss_mask_6: 0.7489  loss_dice_6: 0.8685  loss_ce_7: 0.135  loss_cate_7: 0  loss_mask_7: 0.7185  loss_dice_7: 0.8649  loss_ce_8: 0.1809  loss_cate_8: 0  loss_mask_8: 0.7185  loss_dice_8: 0.8911  time: 1.1022  data_time: 0.0124  lr: 8.6805e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:42:48 d2.utils.events]: \u001b[0m eta: 20:50:53  iter: 11659  total_loss: 19.94  loss_ce: 0.2178  loss_cate: 0.1208  loss_mask: 0.7261  loss_dice: 0.9989  loss_ce_0: 0.2767  loss_cate_0: 0  loss_mask_0: 0.7372  loss_dice_0: 1.051  loss_ce_1: 0.2389  loss_cate_1: 0  loss_mask_1: 0.7138  loss_dice_1: 1.017  loss_ce_2: 0.1782  loss_cate_2: 0  loss_mask_2: 0.7141  loss_dice_2: 1.054  loss_ce_3: 0.2144  loss_cate_3: 0  loss_mask_3: 0.7242  loss_dice_3: 1.008  loss_ce_4: 0.1905  loss_cate_4: 0  loss_mask_4: 0.7222  loss_dice_4: 1.027  loss_ce_5: 0.1834  loss_cate_5: 0  loss_mask_5: 0.7091  loss_dice_5: 1.009  loss_ce_6: 0.1962  loss_cate_6: 0  loss_mask_6: 0.7164  loss_dice_6: 0.9907  loss_ce_7: 0.1972  loss_cate_7: 0  loss_mask_7: 0.7208  loss_dice_7: 1.018  loss_ce_8: 0.224  loss_cate_8: 0  loss_mask_8: 0.7218  loss_dice_8: 1.023  time: 1.1022  data_time: 0.0146  lr: 8.6783e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:43:10 d2.utils.events]: \u001b[0m eta: 20:50:22  iter: 11679  total_loss: 18.24  loss_ce: 0.1702  loss_cate: 0.1308  loss_mask: 0.6943  loss_dice: 0.9454  loss_ce_0: 0.339  loss_cate_0: 0  loss_mask_0: 0.7227  loss_dice_0: 0.9469  loss_ce_1: 0.2447  loss_cate_1: 0  loss_mask_1: 0.7174  loss_dice_1: 0.8957  loss_ce_2: 0.2301  loss_cate_2: 0  loss_mask_2: 0.6893  loss_dice_2: 0.8777  loss_ce_3: 0.2033  loss_cate_3: 0  loss_mask_3: 0.6857  loss_dice_3: 0.9514  loss_ce_4: 0.1845  loss_cate_4: 0  loss_mask_4: 0.6902  loss_dice_4: 0.9235  loss_ce_5: 0.1514  loss_cate_5: 0  loss_mask_5: 0.6999  loss_dice_5: 0.883  loss_ce_6: 0.1621  loss_cate_6: 0  loss_mask_6: 0.7239  loss_dice_6: 0.8876  loss_ce_7: 0.1764  loss_cate_7: 0  loss_mask_7: 0.7278  loss_dice_7: 0.8686  loss_ce_8: 0.1658  loss_cate_8: 0  loss_mask_8: 0.7043  loss_dice_8: 0.8648  time: 1.1022  data_time: 0.0133  lr: 8.676e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:43:32 d2.utils.events]: \u001b[0m eta: 20:49:57  iter: 11699  total_loss: 19.06  loss_ce: 0.1598  loss_cate: 0.1496  loss_mask: 0.7279  loss_dice: 0.9486  loss_ce_0: 0.2997  loss_cate_0: 0  loss_mask_0: 0.7295  loss_dice_0: 0.9487  loss_ce_1: 0.193  loss_cate_1: 0  loss_mask_1: 0.7295  loss_dice_1: 0.9731  loss_ce_2: 0.1927  loss_cate_2: 0  loss_mask_2: 0.7232  loss_dice_2: 0.9529  loss_ce_3: 0.1575  loss_cate_3: 0  loss_mask_3: 0.7286  loss_dice_3: 0.9446  loss_ce_4: 0.188  loss_cate_4: 0  loss_mask_4: 0.7236  loss_dice_4: 0.9318  loss_ce_5: 0.1579  loss_cate_5: 0  loss_mask_5: 0.7186  loss_dice_5: 0.9439  loss_ce_6: 0.1486  loss_cate_6: 0  loss_mask_6: 0.7256  loss_dice_6: 0.94  loss_ce_7: 0.1445  loss_cate_7: 0  loss_mask_7: 0.7262  loss_dice_7: 0.9452  loss_ce_8: 0.148  loss_cate_8: 0  loss_mask_8: 0.7307  loss_dice_8: 0.9636  time: 1.1022  data_time: 0.0129  lr: 8.6737e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:43:55 d2.utils.events]: \u001b[0m eta: 20:49:27  iter: 11719  total_loss: 21.6  loss_ce: 0.2549  loss_cate: 0.1418  loss_mask: 0.7837  loss_dice: 0.9299  loss_ce_0: 0.3766  loss_cate_0: 0  loss_mask_0: 0.7956  loss_dice_0: 0.9645  loss_ce_1: 0.2791  loss_cate_1: 0  loss_mask_1: 0.8382  loss_dice_1: 0.932  loss_ce_2: 0.279  loss_cate_2: 0  loss_mask_2: 0.7671  loss_dice_2: 0.9066  loss_ce_3: 0.2851  loss_cate_3: 0  loss_mask_3: 0.8143  loss_dice_3: 0.9395  loss_ce_4: 0.2732  loss_cate_4: 0  loss_mask_4: 0.8547  loss_dice_4: 0.987  loss_ce_5: 0.2551  loss_cate_5: 0  loss_mask_5: 0.7801  loss_dice_5: 0.9115  loss_ce_6: 0.2577  loss_cate_6: 0  loss_mask_6: 0.8493  loss_dice_6: 0.9205  loss_ce_7: 0.2649  loss_cate_7: 0  loss_mask_7: 0.7464  loss_dice_7: 0.9335  loss_ce_8: 0.2394  loss_cate_8: 0  loss_mask_8: 0.7695  loss_dice_8: 0.9168  time: 1.1022  data_time: 0.0133  lr: 8.6714e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:44:17 d2.utils.events]: \u001b[0m eta: 20:49:02  iter: 11739  total_loss: 21.2  loss_ce: 0.152  loss_cate: 0.1453  loss_mask: 0.8254  loss_dice: 1.043  loss_ce_0: 0.2924  loss_cate_0: 0  loss_mask_0: 0.8311  loss_dice_0: 1.088  loss_ce_1: 0.2464  loss_cate_1: 0  loss_mask_1: 0.8312  loss_dice_1: 1.029  loss_ce_2: 0.248  loss_cate_2: 0  loss_mask_2: 0.8064  loss_dice_2: 1.029  loss_ce_3: 0.1759  loss_cate_3: 0  loss_mask_3: 0.792  loss_dice_3: 1.004  loss_ce_4: 0.184  loss_cate_4: 0  loss_mask_4: 0.8174  loss_dice_4: 0.9838  loss_ce_5: 0.2886  loss_cate_5: 0  loss_mask_5: 0.786  loss_dice_5: 1.027  loss_ce_6: 0.205  loss_cate_6: 0  loss_mask_6: 0.8119  loss_dice_6: 1.002  loss_ce_7: 0.1597  loss_cate_7: 0  loss_mask_7: 0.8259  loss_dice_7: 1.017  loss_ce_8: 0.1694  loss_cate_8: 0  loss_mask_8: 0.8202  loss_dice_8: 1.053  time: 1.1022  data_time: 0.0161  lr: 8.6691e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:44:39 d2.utils.events]: \u001b[0m eta: 20:48:39  iter: 11759  total_loss: 19.24  loss_ce: 0.1697  loss_cate: 0.135  loss_mask: 0.7365  loss_dice: 0.9478  loss_ce_0: 0.2824  loss_cate_0: 0  loss_mask_0: 0.7719  loss_dice_0: 1.052  loss_ce_1: 0.1862  loss_cate_1: 0  loss_mask_1: 0.7468  loss_dice_1: 0.9617  loss_ce_2: 0.1681  loss_cate_2: 0  loss_mask_2: 0.744  loss_dice_2: 0.9566  loss_ce_3: 0.1645  loss_cate_3: 0  loss_mask_3: 0.7583  loss_dice_3: 0.9841  loss_ce_4: 0.1581  loss_cate_4: 0  loss_mask_4: 0.7371  loss_dice_4: 0.9785  loss_ce_5: 0.1419  loss_cate_5: 0  loss_mask_5: 0.7502  loss_dice_5: 0.9375  loss_ce_6: 0.1601  loss_cate_6: 0  loss_mask_6: 0.7423  loss_dice_6: 0.9754  loss_ce_7: 0.1622  loss_cate_7: 0  loss_mask_7: 0.7387  loss_dice_7: 0.9629  loss_ce_8: 0.1887  loss_cate_8: 0  loss_mask_8: 0.7229  loss_dice_8: 0.9548  time: 1.1022  data_time: 0.0127  lr: 8.6668e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:45:03 d2.utils.events]: \u001b[0m eta: 20:48:00  iter: 11779  total_loss: 21.96  loss_ce: 0.2755  loss_cate: 0.1624  loss_mask: 0.7632  loss_dice: 0.9872  loss_ce_0: 0.3563  loss_cate_0: 0  loss_mask_0: 0.7822  loss_dice_0: 1.084  loss_ce_1: 0.3785  loss_cate_1: 0  loss_mask_1: 0.8431  loss_dice_1: 0.997  loss_ce_2: 0.3548  loss_cate_2: 0  loss_mask_2: 0.7806  loss_dice_2: 1.028  loss_ce_3: 0.3843  loss_cate_3: 0  loss_mask_3: 0.7758  loss_dice_3: 0.9866  loss_ce_4: 0.3428  loss_cate_4: 0  loss_mask_4: 0.7718  loss_dice_4: 0.9792  loss_ce_5: 0.2911  loss_cate_5: 0  loss_mask_5: 0.7653  loss_dice_5: 0.9851  loss_ce_6: 0.3043  loss_cate_6: 0  loss_mask_6: 0.7607  loss_dice_6: 0.9884  loss_ce_7: 0.3355  loss_cate_7: 0  loss_mask_7: 0.7588  loss_dice_7: 1.011  loss_ce_8: 0.3139  loss_cate_8: 0  loss_mask_8: 0.7648  loss_dice_8: 0.9831  time: 1.1022  data_time: 0.0131  lr: 8.6645e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:45:25 d2.utils.events]: \u001b[0m eta: 20:47:40  iter: 11799  total_loss: 19.77  loss_ce: 0.1499  loss_cate: 0.1539  loss_mask: 0.7402  loss_dice: 0.9709  loss_ce_0: 0.2527  loss_cate_0: 0  loss_mask_0: 0.7277  loss_dice_0: 1.052  loss_ce_1: 0.1943  loss_cate_1: 0  loss_mask_1: 0.7403  loss_dice_1: 1.014  loss_ce_2: 0.1928  loss_cate_2: 0  loss_mask_2: 0.7108  loss_dice_2: 1.003  loss_ce_3: 0.2275  loss_cate_3: 0  loss_mask_3: 0.7524  loss_dice_3: 0.9617  loss_ce_4: 0.2472  loss_cate_4: 0  loss_mask_4: 0.7576  loss_dice_4: 0.9853  loss_ce_5: 0.2602  loss_cate_5: 0  loss_mask_5: 0.7408  loss_dice_5: 0.9487  loss_ce_6: 0.2125  loss_cate_6: 0  loss_mask_6: 0.7303  loss_dice_6: 0.9874  loss_ce_7: 0.2112  loss_cate_7: 0  loss_mask_7: 0.706  loss_dice_7: 0.9572  loss_ce_8: 0.207  loss_cate_8: 0  loss_mask_8: 0.7057  loss_dice_8: 0.9869  time: 1.1022  data_time: 0.0128  lr: 8.6622e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:45:48 d2.utils.events]: \u001b[0m eta: 20:47:22  iter: 11819  total_loss: 19.09  loss_ce: 0.1949  loss_cate: 0.1328  loss_mask: 0.6889  loss_dice: 0.8642  loss_ce_0: 0.3456  loss_cate_0: 0  loss_mask_0: 0.7106  loss_dice_0: 0.9132  loss_ce_1: 0.2528  loss_cate_1: 0  loss_mask_1: 0.7372  loss_dice_1: 0.9323  loss_ce_2: 0.2932  loss_cate_2: 0  loss_mask_2: 0.6885  loss_dice_2: 0.874  loss_ce_3: 0.2759  loss_cate_3: 0  loss_mask_3: 0.6758  loss_dice_3: 0.8769  loss_ce_4: 0.2724  loss_cate_4: 0  loss_mask_4: 0.6858  loss_dice_4: 0.879  loss_ce_5: 0.26  loss_cate_5: 0  loss_mask_5: 0.6763  loss_dice_5: 0.8782  loss_ce_6: 0.2576  loss_cate_6: 0  loss_mask_6: 0.6734  loss_dice_6: 0.8803  loss_ce_7: 0.2371  loss_cate_7: 0  loss_mask_7: 0.6814  loss_dice_7: 0.863  loss_ce_8: 0.2566  loss_cate_8: 0  loss_mask_8: 0.6963  loss_dice_8: 0.8648  time: 1.1022  data_time: 0.0126  lr: 8.66e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:46:10 d2.utils.events]: \u001b[0m eta: 20:47:19  iter: 11839  total_loss: 18.31  loss_ce: 0.1678  loss_cate: 0.1585  loss_mask: 0.8564  loss_dice: 0.8916  loss_ce_0: 0.2955  loss_cate_0: 0  loss_mask_0: 0.8651  loss_dice_0: 0.898  loss_ce_1: 0.1832  loss_cate_1: 0  loss_mask_1: 0.8744  loss_dice_1: 0.9218  loss_ce_2: 0.1408  loss_cate_2: 0  loss_mask_2: 0.8659  loss_dice_2: 0.9233  loss_ce_3: 0.1745  loss_cate_3: 0  loss_mask_3: 0.8399  loss_dice_3: 0.8849  loss_ce_4: 0.1932  loss_cate_4: 0  loss_mask_4: 0.8662  loss_dice_4: 0.8995  loss_ce_5: 0.2119  loss_cate_5: 0  loss_mask_5: 0.8549  loss_dice_5: 0.8712  loss_ce_6: 0.1921  loss_cate_6: 0  loss_mask_6: 0.8459  loss_dice_6: 0.9354  loss_ce_7: 0.1297  loss_cate_7: 0  loss_mask_7: 0.8631  loss_dice_7: 0.851  loss_ce_8: 0.1588  loss_cate_8: 0  loss_mask_8: 0.854  loss_dice_8: 0.9003  time: 1.1022  data_time: 0.0132  lr: 8.6577e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:46:33 d2.utils.events]: \u001b[0m eta: 20:46:54  iter: 11859  total_loss: 20.97  loss_ce: 0.1685  loss_cate: 0.1327  loss_mask: 0.8461  loss_dice: 0.9241  loss_ce_0: 0.3039  loss_cate_0: 0  loss_mask_0: 0.8418  loss_dice_0: 1.006  loss_ce_1: 0.267  loss_cate_1: 0  loss_mask_1: 0.8452  loss_dice_1: 0.9332  loss_ce_2: 0.1835  loss_cate_2: 0  loss_mask_2: 0.8494  loss_dice_2: 0.9144  loss_ce_3: 0.1966  loss_cate_3: 0  loss_mask_3: 0.8402  loss_dice_3: 0.9084  loss_ce_4: 0.1746  loss_cate_4: 0  loss_mask_4: 0.8439  loss_dice_4: 0.9199  loss_ce_5: 0.186  loss_cate_5: 0  loss_mask_5: 0.8347  loss_dice_5: 0.9358  loss_ce_6: 0.1631  loss_cate_6: 0  loss_mask_6: 0.8577  loss_dice_6: 0.9529  loss_ce_7: 0.217  loss_cate_7: 0  loss_mask_7: 0.8267  loss_dice_7: 0.931  loss_ce_8: 0.2049  loss_cate_8: 0  loss_mask_8: 0.831  loss_dice_8: 0.9305  time: 1.1021  data_time: 0.0130  lr: 8.6554e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:46:55 d2.utils.events]: \u001b[0m eta: 20:46:37  iter: 11879  total_loss: 20.03  loss_ce: 0.2055  loss_cate: 0.1561  loss_mask: 0.711  loss_dice: 1.015  loss_ce_0: 0.3136  loss_cate_0: 0  loss_mask_0: 0.7676  loss_dice_0: 1.059  loss_ce_1: 0.238  loss_cate_1: 0  loss_mask_1: 0.7073  loss_dice_1: 1.037  loss_ce_2: 0.2561  loss_cate_2: 0  loss_mask_2: 0.696  loss_dice_2: 0.9827  loss_ce_3: 0.2319  loss_cate_3: 0  loss_mask_3: 0.7134  loss_dice_3: 0.9949  loss_ce_4: 0.2467  loss_cate_4: 0  loss_mask_4: 0.7131  loss_dice_4: 1.009  loss_ce_5: 0.1802  loss_cate_5: 0  loss_mask_5: 0.713  loss_dice_5: 1.032  loss_ce_6: 0.2056  loss_cate_6: 0  loss_mask_6: 0.6989  loss_dice_6: 1.017  loss_ce_7: 0.1795  loss_cate_7: 0  loss_mask_7: 0.7052  loss_dice_7: 1.039  loss_ce_8: 0.2137  loss_cate_8: 0  loss_mask_8: 0.7103  loss_dice_8: 1.015  time: 1.1021  data_time: 0.0133  lr: 8.6531e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:47:18 d2.utils.events]: \u001b[0m eta: 20:45:50  iter: 11899  total_loss: 21.96  loss_ce: 0.2733  loss_cate: 0.1614  loss_mask: 0.8134  loss_dice: 0.9669  loss_ce_0: 0.356  loss_cate_0: 0  loss_mask_0: 0.789  loss_dice_0: 1.027  loss_ce_1: 0.2937  loss_cate_1: 0  loss_mask_1: 0.7893  loss_dice_1: 1.012  loss_ce_2: 0.2943  loss_cate_2: 0  loss_mask_2: 0.7881  loss_dice_2: 0.9964  loss_ce_3: 0.3177  loss_cate_3: 0  loss_mask_3: 0.7736  loss_dice_3: 0.969  loss_ce_4: 0.3087  loss_cate_4: 0  loss_mask_4: 0.7841  loss_dice_4: 0.9791  loss_ce_5: 0.2747  loss_cate_5: 0  loss_mask_5: 0.8009  loss_dice_5: 1.008  loss_ce_6: 0.2842  loss_cate_6: 0  loss_mask_6: 0.797  loss_dice_6: 0.9701  loss_ce_7: 0.2909  loss_cate_7: 0  loss_mask_7: 0.7583  loss_dice_7: 0.9657  loss_ce_8: 0.2866  loss_cate_8: 0  loss_mask_8: 0.7809  loss_dice_8: 0.9986  time: 1.1021  data_time: 0.0128  lr: 8.6508e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:47:41 d2.utils.events]: \u001b[0m eta: 20:45:55  iter: 11919  total_loss: 21.01  loss_ce: 0.1833  loss_cate: 0.1491  loss_mask: 0.727  loss_dice: 1.001  loss_ce_0: 0.3351  loss_cate_0: 0  loss_mask_0: 0.7712  loss_dice_0: 1.059  loss_ce_1: 0.2331  loss_cate_1: 0  loss_mask_1: 0.7159  loss_dice_1: 0.9953  loss_ce_2: 0.2101  loss_cate_2: 0  loss_mask_2: 0.7347  loss_dice_2: 0.9694  loss_ce_3: 0.2122  loss_cate_3: 0  loss_mask_3: 0.7302  loss_dice_3: 1.007  loss_ce_4: 0.2036  loss_cate_4: 0  loss_mask_4: 0.7183  loss_dice_4: 0.9971  loss_ce_5: 0.2004  loss_cate_5: 0  loss_mask_5: 0.7358  loss_dice_5: 0.9947  loss_ce_6: 0.1919  loss_cate_6: 0  loss_mask_6: 0.7414  loss_dice_6: 1.027  loss_ce_7: 0.199  loss_cate_7: 0  loss_mask_7: 0.7241  loss_dice_7: 1.001  loss_ce_8: 0.2142  loss_cate_8: 0  loss_mask_8: 0.7363  loss_dice_8: 0.9882  time: 1.1021  data_time: 0.0130  lr: 8.6485e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:48:04 d2.utils.events]: \u001b[0m eta: 20:45:51  iter: 11939  total_loss: 18.52  loss_ce: 0.2087  loss_cate: 0.1441  loss_mask: 0.68  loss_dice: 0.9178  loss_ce_0: 0.3694  loss_cate_0: 0  loss_mask_0: 0.7181  loss_dice_0: 0.9154  loss_ce_1: 0.2037  loss_cate_1: 0  loss_mask_1: 0.7352  loss_dice_1: 0.9127  loss_ce_2: 0.2223  loss_cate_2: 0  loss_mask_2: 0.7623  loss_dice_2: 0.9208  loss_ce_3: 0.2113  loss_cate_3: 0  loss_mask_3: 0.7707  loss_dice_3: 0.8954  loss_ce_4: 0.1903  loss_cate_4: 0  loss_mask_4: 0.7275  loss_dice_4: 0.929  loss_ce_5: 0.1534  loss_cate_5: 0  loss_mask_5: 0.7319  loss_dice_5: 0.9269  loss_ce_6: 0.1596  loss_cate_6: 0  loss_mask_6: 0.7449  loss_dice_6: 0.8965  loss_ce_7: 0.1933  loss_cate_7: 0  loss_mask_7: 0.7698  loss_dice_7: 0.9152  loss_ce_8: 0.2004  loss_cate_8: 0  loss_mask_8: 0.691  loss_dice_8: 0.9302  time: 1.1021  data_time: 0.0139  lr: 8.6462e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:48:26 d2.utils.events]: \u001b[0m eta: 20:45:31  iter: 11959  total_loss: 22.25  loss_ce: 0.203  loss_cate: 0.1572  loss_mask: 0.7863  loss_dice: 0.8872  loss_ce_0: 0.3273  loss_cate_0: 0  loss_mask_0: 0.8553  loss_dice_0: 0.9964  loss_ce_1: 0.2527  loss_cate_1: 0  loss_mask_1: 0.8671  loss_dice_1: 1.016  loss_ce_2: 0.2086  loss_cate_2: 0  loss_mask_2: 0.8145  loss_dice_2: 0.9513  loss_ce_3: 0.2023  loss_cate_3: 0  loss_mask_3: 0.8011  loss_dice_3: 0.8777  loss_ce_4: 0.2135  loss_cate_4: 0  loss_mask_4: 0.8036  loss_dice_4: 0.9194  loss_ce_5: 0.2409  loss_cate_5: 0  loss_mask_5: 0.7924  loss_dice_5: 0.9151  loss_ce_6: 0.217  loss_cate_6: 0  loss_mask_6: 0.7732  loss_dice_6: 0.9027  loss_ce_7: 0.2008  loss_cate_7: 0  loss_mask_7: 0.7875  loss_dice_7: 0.9229  loss_ce_8: 0.2481  loss_cate_8: 0  loss_mask_8: 0.789  loss_dice_8: 0.9072  time: 1.1021  data_time: 0.0138  lr: 8.644e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:48:48 d2.utils.events]: \u001b[0m eta: 20:45:03  iter: 11979  total_loss: 20.32  loss_ce: 0.2298  loss_cate: 0.1447  loss_mask: 0.795  loss_dice: 0.9425  loss_ce_0: 0.3284  loss_cate_0: 0  loss_mask_0: 0.7908  loss_dice_0: 0.9913  loss_ce_1: 0.3127  loss_cate_1: 0  loss_mask_1: 0.723  loss_dice_1: 0.9645  loss_ce_2: 0.2579  loss_cate_2: 0  loss_mask_2: 0.8063  loss_dice_2: 0.9842  loss_ce_3: 0.224  loss_cate_3: 0  loss_mask_3: 0.8155  loss_dice_3: 0.9962  loss_ce_4: 0.2581  loss_cate_4: 0  loss_mask_4: 0.7609  loss_dice_4: 0.9356  loss_ce_5: 0.2143  loss_cate_5: 0  loss_mask_5: 0.7692  loss_dice_5: 0.9633  loss_ce_6: 0.1993  loss_cate_6: 0  loss_mask_6: 0.8085  loss_dice_6: 0.964  loss_ce_7: 0.2207  loss_cate_7: 0  loss_mask_7: 0.7875  loss_dice_7: 0.9827  loss_ce_8: 0.232  loss_cate_8: 0  loss_mask_8: 0.8151  loss_dice_8: 0.9499  time: 1.1021  data_time: 0.0127  lr: 8.6417e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:50:05 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 16:50:05 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 16:50:05 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 16:51:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 16:51:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0015 s/iter. Inference: 0.1645 s/iter. Eval: 0.0146 s/iter. Total: 0.1806 s/iter. ETA=0:06:02\n",
      "\u001b[32m[10/12 16:51:23 d2.evaluation.evaluator]: \u001b[0mInference done 39/2016. Dataloading: 0.0022 s/iter. Inference: 0.1627 s/iter. Eval: 0.0182 s/iter. Total: 0.1832 s/iter. ETA=0:06:02\n",
      "\u001b[32m[10/12 16:51:28 d2.evaluation.evaluator]: \u001b[0mInference done 65/2016. Dataloading: 0.0022 s/iter. Inference: 0.1620 s/iter. Eval: 0.0238 s/iter. Total: 0.1882 s/iter. ETA=0:06:07\n",
      "\u001b[32m[10/12 16:51:33 d2.evaluation.evaluator]: \u001b[0mInference done 93/2016. Dataloading: 0.0023 s/iter. Inference: 0.1617 s/iter. Eval: 0.0228 s/iter. Total: 0.1869 s/iter. ETA=0:05:59\n",
      "\u001b[32m[10/12 16:51:38 d2.evaluation.evaluator]: \u001b[0mInference done 120/2016. Dataloading: 0.0022 s/iter. Inference: 0.1618 s/iter. Eval: 0.0257 s/iter. Total: 0.1899 s/iter. ETA=0:05:59\n",
      "\u001b[32m[10/12 16:51:43 d2.evaluation.evaluator]: \u001b[0mInference done 146/2016. Dataloading: 0.0023 s/iter. Inference: 0.1629 s/iter. Eval: 0.0263 s/iter. Total: 0.1916 s/iter. ETA=0:05:58\n",
      "\u001b[32m[10/12 16:51:48 d2.evaluation.evaluator]: \u001b[0mInference done 173/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0254 s/iter. Total: 0.1907 s/iter. ETA=0:05:51\n",
      "\u001b[32m[10/12 16:51:54 d2.evaluation.evaluator]: \u001b[0mInference done 200/2016. Dataloading: 0.0023 s/iter. Inference: 0.1627 s/iter. Eval: 0.0254 s/iter. Total: 0.1905 s/iter. ETA=0:05:45\n",
      "\u001b[32m[10/12 16:51:59 d2.evaluation.evaluator]: \u001b[0mInference done 227/2016. Dataloading: 0.0023 s/iter. Inference: 0.1628 s/iter. Eval: 0.0248 s/iter. Total: 0.1900 s/iter. ETA=0:05:39\n",
      "\u001b[32m[10/12 16:52:04 d2.evaluation.evaluator]: \u001b[0mInference done 253/2016. Dataloading: 0.0031 s/iter. Inference: 0.1628 s/iter. Eval: 0.0247 s/iter. Total: 0.1907 s/iter. ETA=0:05:36\n",
      "\u001b[32m[10/12 16:52:09 d2.evaluation.evaluator]: \u001b[0mInference done 279/2016. Dataloading: 0.0030 s/iter. Inference: 0.1626 s/iter. Eval: 0.0252 s/iter. Total: 0.1910 s/iter. ETA=0:05:31\n",
      "\u001b[32m[10/12 16:52:14 d2.evaluation.evaluator]: \u001b[0mInference done 306/2016. Dataloading: 0.0030 s/iter. Inference: 0.1626 s/iter. Eval: 0.0250 s/iter. Total: 0.1908 s/iter. ETA=0:05:26\n",
      "\u001b[32m[10/12 16:52:19 d2.evaluation.evaluator]: \u001b[0mInference done 331/2016. Dataloading: 0.0030 s/iter. Inference: 0.1629 s/iter. Eval: 0.0255 s/iter. Total: 0.1915 s/iter. ETA=0:05:22\n",
      "\u001b[32m[10/12 16:52:24 d2.evaluation.evaluator]: \u001b[0mInference done 358/2016. Dataloading: 0.0030 s/iter. Inference: 0.1631 s/iter. Eval: 0.0251 s/iter. Total: 0.1914 s/iter. ETA=0:05:17\n",
      "\u001b[32m[10/12 16:52:29 d2.evaluation.evaluator]: \u001b[0mInference done 384/2016. Dataloading: 0.0029 s/iter. Inference: 0.1633 s/iter. Eval: 0.0254 s/iter. Total: 0.1917 s/iter. ETA=0:05:12\n",
      "\u001b[32m[10/12 16:52:34 d2.evaluation.evaluator]: \u001b[0mInference done 410/2016. Dataloading: 0.0029 s/iter. Inference: 0.1634 s/iter. Eval: 0.0254 s/iter. Total: 0.1918 s/iter. ETA=0:05:08\n",
      "\u001b[32m[10/12 16:52:39 d2.evaluation.evaluator]: \u001b[0mInference done 437/2016. Dataloading: 0.0029 s/iter. Inference: 0.1633 s/iter. Eval: 0.0254 s/iter. Total: 0.1917 s/iter. ETA=0:05:02\n",
      "\u001b[32m[10/12 16:52:44 d2.evaluation.evaluator]: \u001b[0mInference done 462/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0259 s/iter. Total: 0.1922 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 16:52:49 d2.evaluation.evaluator]: \u001b[0mInference done 489/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0257 s/iter. Total: 0.1920 s/iter. ETA=0:04:53\n",
      "\u001b[32m[10/12 16:52:54 d2.evaluation.evaluator]: \u001b[0mInference done 515/2016. Dataloading: 0.0028 s/iter. Inference: 0.1633 s/iter. Eval: 0.0259 s/iter. Total: 0.1921 s/iter. ETA=0:04:48\n",
      "\u001b[32m[10/12 16:52:59 d2.evaluation.evaluator]: \u001b[0mInference done 541/2016. Dataloading: 0.0027 s/iter. Inference: 0.1633 s/iter. Eval: 0.0259 s/iter. Total: 0.1921 s/iter. ETA=0:04:43\n",
      "\u001b[32m[10/12 16:53:04 d2.evaluation.evaluator]: \u001b[0mInference done 567/2016. Dataloading: 0.0027 s/iter. Inference: 0.1634 s/iter. Eval: 0.0258 s/iter. Total: 0.1921 s/iter. ETA=0:04:38\n",
      "\u001b[32m[10/12 16:53:09 d2.evaluation.evaluator]: \u001b[0mInference done 592/2016. Dataloading: 0.0027 s/iter. Inference: 0.1634 s/iter. Eval: 0.0262 s/iter. Total: 0.1925 s/iter. ETA=0:04:34\n",
      "\u001b[32m[10/12 16:53:14 d2.evaluation.evaluator]: \u001b[0mInference done 618/2016. Dataloading: 0.0027 s/iter. Inference: 0.1634 s/iter. Eval: 0.0263 s/iter. Total: 0.1926 s/iter. ETA=0:04:29\n",
      "\u001b[32m[10/12 16:53:20 d2.evaluation.evaluator]: \u001b[0mInference done 644/2016. Dataloading: 0.0027 s/iter. Inference: 0.1634 s/iter. Eval: 0.0265 s/iter. Total: 0.1928 s/iter. ETA=0:04:24\n",
      "\u001b[32m[10/12 16:53:25 d2.evaluation.evaluator]: \u001b[0mInference done 672/2016. Dataloading: 0.0027 s/iter. Inference: 0.1634 s/iter. Eval: 0.0262 s/iter. Total: 0.1924 s/iter. ETA=0:04:18\n",
      "\u001b[32m[10/12 16:53:30 d2.evaluation.evaluator]: \u001b[0mInference done 698/2016. Dataloading: 0.0027 s/iter. Inference: 0.1634 s/iter. Eval: 0.0263 s/iter. Total: 0.1925 s/iter. ETA=0:04:13\n",
      "\u001b[32m[10/12 16:53:35 d2.evaluation.evaluator]: \u001b[0mInference done 725/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0263 s/iter. Total: 0.1925 s/iter. ETA=0:04:08\n",
      "\u001b[32m[10/12 16:53:40 d2.evaluation.evaluator]: \u001b[0mInference done 751/2016. Dataloading: 0.0026 s/iter. Inference: 0.1634 s/iter. Eval: 0.0264 s/iter. Total: 0.1925 s/iter. ETA=0:04:03\n",
      "\u001b[32m[10/12 16:53:45 d2.evaluation.evaluator]: \u001b[0mInference done 777/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0263 s/iter. Total: 0.1926 s/iter. ETA=0:03:58\n",
      "\u001b[32m[10/12 16:53:50 d2.evaluation.evaluator]: \u001b[0mInference done 804/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:03:53\n",
      "\u001b[32m[10/12 16:53:55 d2.evaluation.evaluator]: \u001b[0mInference done 830/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0262 s/iter. Total: 0.1925 s/iter. ETA=0:03:48\n",
      "\u001b[32m[10/12 16:54:00 d2.evaluation.evaluator]: \u001b[0mInference done 857/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0262 s/iter. Total: 0.1924 s/iter. ETA=0:03:42\n",
      "\u001b[32m[10/12 16:54:05 d2.evaluation.evaluator]: \u001b[0mInference done 884/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:03:37\n",
      "\u001b[32m[10/12 16:54:11 d2.evaluation.evaluator]: \u001b[0mInference done 911/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0260 s/iter. Total: 0.1922 s/iter. ETA=0:03:32\n",
      "\u001b[32m[10/12 16:54:16 d2.evaluation.evaluator]: \u001b[0mInference done 938/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0260 s/iter. Total: 0.1922 s/iter. ETA=0:03:27\n",
      "\u001b[32m[10/12 16:54:21 d2.evaluation.evaluator]: \u001b[0mInference done 964/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:03:22\n",
      "\u001b[32m[10/12 16:54:26 d2.evaluation.evaluator]: \u001b[0mInference done 991/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0259 s/iter. Total: 0.1921 s/iter. ETA=0:03:16\n",
      "\u001b[32m[10/12 16:54:31 d2.evaluation.evaluator]: \u001b[0mInference done 1016/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1924 s/iter. ETA=0:03:12\n",
      "\u001b[32m[10/12 16:54:36 d2.evaluation.evaluator]: \u001b[0mInference done 1043/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:03:07\n",
      "\u001b[32m[10/12 16:54:41 d2.evaluation.evaluator]: \u001b[0mInference done 1069/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0263 s/iter. Total: 0.1925 s/iter. ETA=0:03:02\n",
      "\u001b[32m[10/12 16:54:46 d2.evaluation.evaluator]: \u001b[0mInference done 1097/2016. Dataloading: 0.0026 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:02:56\n",
      "\u001b[32m[10/12 16:54:51 d2.evaluation.evaluator]: \u001b[0mInference done 1123/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:02:51\n",
      "\u001b[32m[10/12 16:54:56 d2.evaluation.evaluator]: \u001b[0mInference done 1149/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0261 s/iter. Total: 0.1923 s/iter. ETA=0:02:46\n",
      "\u001b[32m[10/12 16:55:02 d2.evaluation.evaluator]: \u001b[0mInference done 1177/2016. Dataloading: 0.0025 s/iter. Inference: 0.1635 s/iter. Eval: 0.0259 s/iter. Total: 0.1921 s/iter. ETA=0:02:41\n",
      "\u001b[32m[10/12 16:55:07 d2.evaluation.evaluator]: \u001b[0mInference done 1200/2016. Dataloading: 0.0030 s/iter. Inference: 0.1635 s/iter. Eval: 0.0260 s/iter. Total: 0.1926 s/iter. ETA=0:02:37\n",
      "\u001b[32m[10/12 16:55:12 d2.evaluation.evaluator]: \u001b[0mInference done 1227/2016. Dataloading: 0.0030 s/iter. Inference: 0.1635 s/iter. Eval: 0.0259 s/iter. Total: 0.1926 s/iter. ETA=0:02:31\n",
      "\u001b[32m[10/12 16:55:17 d2.evaluation.evaluator]: \u001b[0mInference done 1253/2016. Dataloading: 0.0030 s/iter. Inference: 0.1636 s/iter. Eval: 0.0259 s/iter. Total: 0.1926 s/iter. ETA=0:02:26\n",
      "\u001b[32m[10/12 16:55:22 d2.evaluation.evaluator]: \u001b[0mInference done 1278/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0261 s/iter. Total: 0.1928 s/iter. ETA=0:02:22\n",
      "\u001b[32m[10/12 16:55:27 d2.evaluation.evaluator]: \u001b[0mInference done 1305/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0260 s/iter. Total: 0.1927 s/iter. ETA=0:02:17\n",
      "\u001b[32m[10/12 16:55:32 d2.evaluation.evaluator]: \u001b[0mInference done 1330/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0262 s/iter. Total: 0.1929 s/iter. ETA=0:02:12\n",
      "\u001b[32m[10/12 16:55:37 d2.evaluation.evaluator]: \u001b[0mInference done 1357/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0262 s/iter. Total: 0.1929 s/iter. ETA=0:02:07\n",
      "\u001b[32m[10/12 16:55:42 d2.evaluation.evaluator]: \u001b[0mInference done 1382/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0264 s/iter. Total: 0.1931 s/iter. ETA=0:02:02\n",
      "\u001b[32m[10/12 16:55:47 d2.evaluation.evaluator]: \u001b[0mInference done 1409/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0264 s/iter. Total: 0.1931 s/iter. ETA=0:01:57\n",
      "\u001b[32m[10/12 16:55:53 d2.evaluation.evaluator]: \u001b[0mInference done 1434/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0266 s/iter. Total: 0.1933 s/iter. ETA=0:01:52\n",
      "\u001b[32m[10/12 16:55:58 d2.evaluation.evaluator]: \u001b[0mInference done 1461/2016. Dataloading: 0.0029 s/iter. Inference: 0.1636 s/iter. Eval: 0.0265 s/iter. Total: 0.1932 s/iter. ETA=0:01:47\n",
      "\u001b[32m[10/12 16:56:03 d2.evaluation.evaluator]: \u001b[0mInference done 1486/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1933 s/iter. ETA=0:01:42\n",
      "\u001b[32m[10/12 16:56:08 d2.evaluation.evaluator]: \u001b[0mInference done 1513/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0266 s/iter. Total: 0.1932 s/iter. ETA=0:01:37\n",
      "\u001b[32m[10/12 16:56:13 d2.evaluation.evaluator]: \u001b[0mInference done 1539/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1932 s/iter. ETA=0:01:32\n",
      "\u001b[32m[10/12 16:56:18 d2.evaluation.evaluator]: \u001b[0mInference done 1565/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1933 s/iter. ETA=0:01:27\n",
      "\u001b[32m[10/12 16:56:23 d2.evaluation.evaluator]: \u001b[0mInference done 1591/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1933 s/iter. ETA=0:01:22\n",
      "\u001b[32m[10/12 16:56:28 d2.evaluation.evaluator]: \u001b[0mInference done 1617/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1933 s/iter. ETA=0:01:17\n",
      "\u001b[32m[10/12 16:56:34 d2.evaluation.evaluator]: \u001b[0mInference done 1644/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0268 s/iter. Total: 0.1936 s/iter. ETA=0:01:12\n",
      "\u001b[32m[10/12 16:56:39 d2.evaluation.evaluator]: \u001b[0mInference done 1671/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1935 s/iter. ETA=0:01:06\n",
      "\u001b[32m[10/12 16:56:44 d2.evaluation.evaluator]: \u001b[0mInference done 1697/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0268 s/iter. Total: 0.1935 s/iter. ETA=0:01:01\n",
      "\u001b[32m[10/12 16:56:49 d2.evaluation.evaluator]: \u001b[0mInference done 1724/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:56\n",
      "\u001b[32m[10/12 16:56:54 d2.evaluation.evaluator]: \u001b[0mInference done 1750/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:51\n",
      "\u001b[32m[10/12 16:56:59 d2.evaluation.evaluator]: \u001b[0mInference done 1776/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:46\n",
      "\u001b[32m[10/12 16:57:04 d2.evaluation.evaluator]: \u001b[0mInference done 1802/2016. Dataloading: 0.0028 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:41\n",
      "\u001b[32m[10/12 16:57:09 d2.evaluation.evaluator]: \u001b[0mInference done 1829/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:36\n",
      "\u001b[32m[10/12 16:57:14 d2.evaluation.evaluator]: \u001b[0mInference done 1854/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0268 s/iter. Total: 0.1935 s/iter. ETA=0:00:31\n",
      "\u001b[32m[10/12 16:57:19 d2.evaluation.evaluator]: \u001b[0mInference done 1881/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0268 s/iter. Total: 0.1935 s/iter. ETA=0:00:26\n",
      "\u001b[32m[10/12 16:57:24 d2.evaluation.evaluator]: \u001b[0mInference done 1907/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0268 s/iter. Total: 0.1935 s/iter. ETA=0:00:21\n",
      "\u001b[32m[10/12 16:57:30 d2.evaluation.evaluator]: \u001b[0mInference done 1934/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:15\n",
      "\u001b[32m[10/12 16:57:35 d2.evaluation.evaluator]: \u001b[0mInference done 1960/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:10\n",
      "\u001b[32m[10/12 16:57:40 d2.evaluation.evaluator]: \u001b[0mInference done 1986/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0267 s/iter. Total: 0.1934 s/iter. ETA=0:00:05\n",
      "\u001b[32m[10/12 16:57:45 d2.evaluation.evaluator]: \u001b[0mInference done 2011/2016. Dataloading: 0.0027 s/iter. Inference: 0.1636 s/iter. Eval: 0.0268 s/iter. Total: 0.1935 s/iter. ETA=0:00:00\n",
      "\u001b[32m[10/12 16:57:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:29.217191 (0.193544 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 16:57:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:29 (0.163625 s / iter per device, on 1 devices)\n",
      "miou = 75.18048611734905\n",
      "OA = 88.91101734978812\n",
      "Kappa = 85.52151502190078\n",
      "F1_score = 71.58382314207485\n",
      "\u001b[32m[10/12 16:57:46 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 75.18048611734905, 'fwIoU': 80.63164399244039, 'IoU-Background': 39.929466039310434, 'IoU-Surfaces': 84.35486851906377, 'IoU-Building': 92.05671879724815, 'IoU-Low vegetation': 74.79765205205486, 'IoU-tree': 75.61239193518229, 'IoU-Car': 84.33181936123479, 'mACC': 83.75936240658018, 'pACC': 88.91101734978812, 'ACC-Background': 51.50004007340122, 'ACC-Surfaces': 92.576970077642, 'ACC-Building': 94.99933954615163, 'ACC-Low vegetation': 89.40460177398926, 'ACC-tree': 82.96010819163145, 'ACC-Car': 91.1151147766655})])\n",
      "\u001b[32m[10/12 16:57:46 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 16:57:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 16:57:46 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 16:57:46 d2.evaluation.testing]: \u001b[0mcopypaste: 75.1805,80.6316,83.7594,88.9110\n",
      "\u001b[32m[10/12 16:57:46 d2.utils.events]: \u001b[0m eta: 20:44:33  iter: 11999  total_loss: 21.81  loss_ce: 0.1075  loss_cate: 0.1568  loss_mask: 0.86  loss_dice: 1.025  loss_ce_0: 0.3452  loss_cate_0: 0  loss_mask_0: 0.8389  loss_dice_0: 1.116  loss_ce_1: 0.2071  loss_cate_1: 0  loss_mask_1: 0.8745  loss_dice_1: 1.107  loss_ce_2: 0.1944  loss_cate_2: 0  loss_mask_2: 0.8585  loss_dice_2: 1.027  loss_ce_3: 0.1598  loss_cate_3: 0  loss_mask_3: 0.8561  loss_dice_3: 1.01  loss_ce_4: 0.1383  loss_cate_4: 0  loss_mask_4: 0.8871  loss_dice_4: 1.044  loss_ce_5: 0.1273  loss_cate_5: 0  loss_mask_5: 0.8227  loss_dice_5: 1.064  loss_ce_6: 0.1278  loss_cate_6: 0  loss_mask_6: 0.8134  loss_dice_6: 1.039  loss_ce_7: 0.1192  loss_cate_7: 0  loss_mask_7: 0.832  loss_dice_7: 1.077  loss_ce_8: 0.121  loss_cate_8: 0  loss_mask_8: 0.8434  loss_dice_8: 0.9992  time: 1.1021  data_time: 0.0123  lr: 8.6394e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:58:08 d2.utils.events]: \u001b[0m eta: 20:44:21  iter: 12019  total_loss: 22.01  loss_ce: 0.3266  loss_cate: 0.1484  loss_mask: 0.8951  loss_dice: 0.9933  loss_ce_0: 0.3811  loss_cate_0: 0  loss_mask_0: 0.8638  loss_dice_0: 1.061  loss_ce_1: 0.2676  loss_cate_1: 0  loss_mask_1: 0.8331  loss_dice_1: 1.003  loss_ce_2: 0.2843  loss_cate_2: 0  loss_mask_2: 0.8775  loss_dice_2: 0.9912  loss_ce_3: 0.28  loss_cate_3: 0  loss_mask_3: 0.8448  loss_dice_3: 0.9373  loss_ce_4: 0.2692  loss_cate_4: 0  loss_mask_4: 0.852  loss_dice_4: 0.9986  loss_ce_5: 0.2996  loss_cate_5: 0  loss_mask_5: 0.8494  loss_dice_5: 0.9699  loss_ce_6: 0.3214  loss_cate_6: 0  loss_mask_6: 0.9115  loss_dice_6: 0.9852  loss_ce_7: 0.2642  loss_cate_7: 0  loss_mask_7: 0.9166  loss_dice_7: 1.011  loss_ce_8: 0.2611  loss_cate_8: 0  loss_mask_8: 0.9042  loss_dice_8: 1.018  time: 1.1021  data_time: 0.0141  lr: 8.6371e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:58:31 d2.utils.events]: \u001b[0m eta: 20:44:00  iter: 12039  total_loss: 19.45  loss_ce: 0.2034  loss_cate: 0.1397  loss_mask: 0.7627  loss_dice: 0.9158  loss_ce_0: 0.2695  loss_cate_0: 0  loss_mask_0: 0.7626  loss_dice_0: 0.9883  loss_ce_1: 0.2102  loss_cate_1: 0  loss_mask_1: 0.7869  loss_dice_1: 0.9118  loss_ce_2: 0.1938  loss_cate_2: 0  loss_mask_2: 0.7795  loss_dice_2: 0.8987  loss_ce_3: 0.2244  loss_cate_3: 0  loss_mask_3: 0.7724  loss_dice_3: 0.9474  loss_ce_4: 0.2046  loss_cate_4: 0  loss_mask_4: 0.7708  loss_dice_4: 0.9149  loss_ce_5: 0.1905  loss_cate_5: 0  loss_mask_5: 0.7559  loss_dice_5: 0.9455  loss_ce_6: 0.1882  loss_cate_6: 0  loss_mask_6: 0.8112  loss_dice_6: 0.939  loss_ce_7: 0.1985  loss_cate_7: 0  loss_mask_7: 0.7906  loss_dice_7: 0.9392  loss_ce_8: 0.2114  loss_cate_8: 0  loss_mask_8: 0.7738  loss_dice_8: 0.9494  time: 1.1021  data_time: 0.0143  lr: 8.6348e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:58:53 d2.utils.events]: \u001b[0m eta: 20:43:38  iter: 12059  total_loss: 22.62  loss_ce: 0.3003  loss_cate: 0.1506  loss_mask: 0.8789  loss_dice: 1.02  loss_ce_0: 0.3623  loss_cate_0: 0  loss_mask_0: 0.9175  loss_dice_0: 1.049  loss_ce_1: 0.2754  loss_cate_1: 0  loss_mask_1: 0.8594  loss_dice_1: 1.001  loss_ce_2: 0.261  loss_cate_2: 0  loss_mask_2: 0.8585  loss_dice_2: 0.9714  loss_ce_3: 0.2978  loss_cate_3: 0  loss_mask_3: 0.8489  loss_dice_3: 0.9893  loss_ce_4: 0.2704  loss_cate_4: 0  loss_mask_4: 0.8408  loss_dice_4: 0.9778  loss_ce_5: 0.2413  loss_cate_5: 0  loss_mask_5: 0.8508  loss_dice_5: 1.033  loss_ce_6: 0.2909  loss_cate_6: 0  loss_mask_6: 0.8399  loss_dice_6: 0.9807  loss_ce_7: 0.267  loss_cate_7: 0  loss_mask_7: 0.8673  loss_dice_7: 1.01  loss_ce_8: 0.2974  loss_cate_8: 0  loss_mask_8: 0.8664  loss_dice_8: 1.013  time: 1.1021  data_time: 0.0130  lr: 8.6325e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:59:16 d2.utils.events]: \u001b[0m eta: 20:43:26  iter: 12079  total_loss: 18.72  loss_ce: 0.232  loss_cate: 0.1426  loss_mask: 0.7025  loss_dice: 0.8373  loss_ce_0: 0.359  loss_cate_0: 0  loss_mask_0: 0.7246  loss_dice_0: 0.8738  loss_ce_1: 0.2732  loss_cate_1: 0  loss_mask_1: 0.7135  loss_dice_1: 0.9142  loss_ce_2: 0.2355  loss_cate_2: 0  loss_mask_2: 0.7064  loss_dice_2: 0.8672  loss_ce_3: 0.23  loss_cate_3: 0  loss_mask_3: 0.7169  loss_dice_3: 0.8241  loss_ce_4: 0.2562  loss_cate_4: 0  loss_mask_4: 0.718  loss_dice_4: 0.8352  loss_ce_5: 0.2531  loss_cate_5: 0  loss_mask_5: 0.7025  loss_dice_5: 0.8493  loss_ce_6: 0.2335  loss_cate_6: 0  loss_mask_6: 0.687  loss_dice_6: 0.828  loss_ce_7: 0.242  loss_cate_7: 0  loss_mask_7: 0.7023  loss_dice_7: 0.8385  loss_ce_8: 0.2396  loss_cate_8: 0  loss_mask_8: 0.7044  loss_dice_8: 0.8014  time: 1.1021  data_time: 0.0136  lr: 8.6302e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 16:59:39 d2.utils.events]: \u001b[0m eta: 20:43:13  iter: 12099  total_loss: 20.83  loss_ce: 0.214  loss_cate: 0.154  loss_mask: 0.7549  loss_dice: 1.007  loss_ce_0: 0.2956  loss_cate_0: 0  loss_mask_0: 0.7649  loss_dice_0: 1.115  loss_ce_1: 0.2134  loss_cate_1: 0  loss_mask_1: 0.755  loss_dice_1: 1.031  loss_ce_2: 0.2261  loss_cate_2: 0  loss_mask_2: 0.7536  loss_dice_2: 1.032  loss_ce_3: 0.2401  loss_cate_3: 0  loss_mask_3: 0.7554  loss_dice_3: 0.9906  loss_ce_4: 0.21  loss_cate_4: 0  loss_mask_4: 0.7572  loss_dice_4: 1.02  loss_ce_5: 0.1972  loss_cate_5: 0  loss_mask_5: 0.7527  loss_dice_5: 1.012  loss_ce_6: 0.2213  loss_cate_6: 0  loss_mask_6: 0.7513  loss_dice_6: 1.004  loss_ce_7: 0.2432  loss_cate_7: 0  loss_mask_7: 0.7481  loss_dice_7: 1.005  loss_ce_8: 0.1989  loss_cate_8: 0  loss_mask_8: 0.7612  loss_dice_8: 1.011  time: 1.1021  data_time: 0.0147  lr: 8.6279e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:00:04 d2.utils.events]: \u001b[0m eta: 20:42:59  iter: 12119  total_loss: 20.44  loss_ce: 0.1558  loss_cate: 0.1359  loss_mask: 0.8074  loss_dice: 0.916  loss_ce_0: 0.3197  loss_cate_0: 0  loss_mask_0: 0.8468  loss_dice_0: 0.9659  loss_ce_1: 0.2072  loss_cate_1: 0  loss_mask_1: 0.8582  loss_dice_1: 0.9766  loss_ce_2: 0.201  loss_cate_2: 0  loss_mask_2: 0.8211  loss_dice_2: 0.9724  loss_ce_3: 0.1483  loss_cate_3: 0  loss_mask_3: 0.8123  loss_dice_3: 0.8895  loss_ce_4: 0.1418  loss_cate_4: 0  loss_mask_4: 0.7937  loss_dice_4: 0.9462  loss_ce_5: 0.1314  loss_cate_5: 0  loss_mask_5: 0.7853  loss_dice_5: 0.9289  loss_ce_6: 0.2058  loss_cate_6: 0  loss_mask_6: 0.8039  loss_dice_6: 0.9439  loss_ce_7: 0.1304  loss_cate_7: 0  loss_mask_7: 0.7941  loss_dice_7: 0.948  loss_ce_8: 0.1783  loss_cate_8: 0  loss_mask_8: 0.809  loss_dice_8: 0.9004  time: 1.1021  data_time: 0.0135  lr: 8.6257e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:00:26 d2.utils.events]: \u001b[0m eta: 20:42:43  iter: 12139  total_loss: 20.24  loss_ce: 0.2889  loss_cate: 0.141  loss_mask: 0.6994  loss_dice: 0.8846  loss_ce_0: 0.3744  loss_cate_0: 0  loss_mask_0: 0.7089  loss_dice_0: 0.9928  loss_ce_1: 0.3102  loss_cate_1: 0  loss_mask_1: 0.7485  loss_dice_1: 0.9956  loss_ce_2: 0.291  loss_cate_2: 0  loss_mask_2: 0.7522  loss_dice_2: 0.9763  loss_ce_3: 0.2461  loss_cate_3: 0  loss_mask_3: 0.7024  loss_dice_3: 0.9395  loss_ce_4: 0.2723  loss_cate_4: 0  loss_mask_4: 0.721  loss_dice_4: 0.9575  loss_ce_5: 0.2632  loss_cate_5: 0  loss_mask_5: 0.7298  loss_dice_5: 0.9402  loss_ce_6: 0.2527  loss_cate_6: 0  loss_mask_6: 0.6901  loss_dice_6: 0.8843  loss_ce_7: 0.2882  loss_cate_7: 0  loss_mask_7: 0.7008  loss_dice_7: 0.898  loss_ce_8: 0.2866  loss_cate_8: 0  loss_mask_8: 0.7048  loss_dice_8: 0.8818  time: 1.1021  data_time: 0.0128  lr: 8.6234e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:00:48 d2.utils.events]: \u001b[0m eta: 20:42:22  iter: 12159  total_loss: 18.64  loss_ce: 0.1527  loss_cate: 0.1202  loss_mask: 0.6517  loss_dice: 0.8903  loss_ce_0: 0.3031  loss_cate_0: 0  loss_mask_0: 0.6515  loss_dice_0: 0.9928  loss_ce_1: 0.1937  loss_cate_1: 0  loss_mask_1: 0.676  loss_dice_1: 0.9598  loss_ce_2: 0.1889  loss_cate_2: 0  loss_mask_2: 0.6604  loss_dice_2: 0.9295  loss_ce_3: 0.1811  loss_cate_3: 0  loss_mask_3: 0.6517  loss_dice_3: 0.9356  loss_ce_4: 0.1993  loss_cate_4: 0  loss_mask_4: 0.667  loss_dice_4: 0.9113  loss_ce_5: 0.1627  loss_cate_5: 0  loss_mask_5: 0.6642  loss_dice_5: 0.9209  loss_ce_6: 0.1579  loss_cate_6: 0  loss_mask_6: 0.6542  loss_dice_6: 0.932  loss_ce_7: 0.1444  loss_cate_7: 0  loss_mask_7: 0.6473  loss_dice_7: 0.9024  loss_ce_8: 0.1536  loss_cate_8: 0  loss_mask_8: 0.6377  loss_dice_8: 0.9313  time: 1.1021  data_time: 0.0132  lr: 8.6211e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:01:11 d2.utils.events]: \u001b[0m eta: 20:41:52  iter: 12179  total_loss: 20.86  loss_ce: 0.1441  loss_cate: 0.152  loss_mask: 0.7767  loss_dice: 0.9438  loss_ce_0: 0.2441  loss_cate_0: 0  loss_mask_0: 0.8202  loss_dice_0: 1.016  loss_ce_1: 0.2002  loss_cate_1: 0  loss_mask_1: 0.841  loss_dice_1: 0.9478  loss_ce_2: 0.1736  loss_cate_2: 0  loss_mask_2: 0.853  loss_dice_2: 0.9303  loss_ce_3: 0.1718  loss_cate_3: 0  loss_mask_3: 0.818  loss_dice_3: 0.9901  loss_ce_4: 0.2008  loss_cate_4: 0  loss_mask_4: 0.8515  loss_dice_4: 0.9469  loss_ce_5: 0.1599  loss_cate_5: 0  loss_mask_5: 0.8129  loss_dice_5: 0.9648  loss_ce_6: 0.1455  loss_cate_6: 0  loss_mask_6: 0.7885  loss_dice_6: 0.9803  loss_ce_7: 0.1996  loss_cate_7: 0  loss_mask_7: 0.8064  loss_dice_7: 0.9822  loss_ce_8: 0.1491  loss_cate_8: 0  loss_mask_8: 0.803  loss_dice_8: 0.9672  time: 1.1021  data_time: 0.0125  lr: 8.6188e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:01:33 d2.utils.events]: \u001b[0m eta: 20:41:35  iter: 12199  total_loss: 18.42  loss_ce: 0.1666  loss_cate: 0.1436  loss_mask: 0.7807  loss_dice: 0.8361  loss_ce_0: 0.3637  loss_cate_0: 0  loss_mask_0: 0.8254  loss_dice_0: 0.8998  loss_ce_1: 0.155  loss_cate_1: 0  loss_mask_1: 0.7976  loss_dice_1: 0.9011  loss_ce_2: 0.1417  loss_cate_2: 0  loss_mask_2: 0.7923  loss_dice_2: 0.9585  loss_ce_3: 0.1642  loss_cate_3: 0  loss_mask_3: 0.7963  loss_dice_3: 0.8763  loss_ce_4: 0.221  loss_cate_4: 0  loss_mask_4: 0.7959  loss_dice_4: 0.8686  loss_ce_5: 0.1985  loss_cate_5: 0  loss_mask_5: 0.7976  loss_dice_5: 0.8627  loss_ce_6: 0.1662  loss_cate_6: 0  loss_mask_6: 0.7829  loss_dice_6: 0.8986  loss_ce_7: 0.1273  loss_cate_7: 0  loss_mask_7: 0.7705  loss_dice_7: 0.9378  loss_ce_8: 0.1458  loss_cate_8: 0  loss_mask_8: 0.7778  loss_dice_8: 0.9025  time: 1.1021  data_time: 0.0127  lr: 8.6165e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:01:56 d2.utils.events]: \u001b[0m eta: 20:41:17  iter: 12219  total_loss: 20.73  loss_ce: 0.2076  loss_cate: 0.1557  loss_mask: 0.8523  loss_dice: 0.9419  loss_ce_0: 0.3128  loss_cate_0: 0  loss_mask_0: 0.8868  loss_dice_0: 0.9675  loss_ce_1: 0.2177  loss_cate_1: 0  loss_mask_1: 0.8544  loss_dice_1: 0.9469  loss_ce_2: 0.218  loss_cate_2: 0  loss_mask_2: 0.8394  loss_dice_2: 0.9439  loss_ce_3: 0.2267  loss_cate_3: 0  loss_mask_3: 0.8253  loss_dice_3: 0.9403  loss_ce_4: 0.2415  loss_cate_4: 0  loss_mask_4: 0.8403  loss_dice_4: 0.946  loss_ce_5: 0.2019  loss_cate_5: 0  loss_mask_5: 0.8604  loss_dice_5: 0.9364  loss_ce_6: 0.231  loss_cate_6: 0  loss_mask_6: 0.851  loss_dice_6: 0.9264  loss_ce_7: 0.2138  loss_cate_7: 0  loss_mask_7: 0.8528  loss_dice_7: 0.9268  loss_ce_8: 0.2079  loss_cate_8: 0  loss_mask_8: 0.8689  loss_dice_8: 0.9324  time: 1.1021  data_time: 0.0143  lr: 8.6142e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:02:18 d2.utils.events]: \u001b[0m eta: 20:40:57  iter: 12239  total_loss: 19.12  loss_ce: 0.1159  loss_cate: 0.1427  loss_mask: 0.8179  loss_dice: 0.879  loss_ce_0: 0.2608  loss_cate_0: 0  loss_mask_0: 0.8453  loss_dice_0: 0.8853  loss_ce_1: 0.09425  loss_cate_1: 0  loss_mask_1: 0.8126  loss_dice_1: 0.8839  loss_ce_2: 0.1065  loss_cate_2: 0  loss_mask_2: 0.8167  loss_dice_2: 0.9133  loss_ce_3: 0.0959  loss_cate_3: 0  loss_mask_3: 0.822  loss_dice_3: 0.8723  loss_ce_4: 0.1065  loss_cate_4: 0  loss_mask_4: 0.82  loss_dice_4: 0.8842  loss_ce_5: 0.1039  loss_cate_5: 0  loss_mask_5: 0.8195  loss_dice_5: 0.8712  loss_ce_6: 0.09755  loss_cate_6: 0  loss_mask_6: 0.8064  loss_dice_6: 0.9329  loss_ce_7: 0.09701  loss_cate_7: 0  loss_mask_7: 0.7946  loss_dice_7: 0.9007  loss_ce_8: 0.1074  loss_cate_8: 0  loss_mask_8: 0.8179  loss_dice_8: 0.8579  time: 1.1021  data_time: 0.0127  lr: 8.6119e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:02:41 d2.utils.events]: \u001b[0m eta: 20:40:40  iter: 12259  total_loss: 18.41  loss_ce: 0.1512  loss_cate: 0.1349  loss_mask: 0.7605  loss_dice: 0.9204  loss_ce_0: 0.3049  loss_cate_0: 0  loss_mask_0: 0.7943  loss_dice_0: 0.99  loss_ce_1: 0.1919  loss_cate_1: 0  loss_mask_1: 0.7664  loss_dice_1: 0.9394  loss_ce_2: 0.1974  loss_cate_2: 0  loss_mask_2: 0.7873  loss_dice_2: 0.9179  loss_ce_3: 0.09857  loss_cate_3: 0  loss_mask_3: 0.7741  loss_dice_3: 0.9462  loss_ce_4: 0.114  loss_cate_4: 0  loss_mask_4: 0.759  loss_dice_4: 0.9384  loss_ce_5: 0.1234  loss_cate_5: 0  loss_mask_5: 0.7651  loss_dice_5: 0.9253  loss_ce_6: 0.1516  loss_cate_6: 0  loss_mask_6: 0.7689  loss_dice_6: 0.9121  loss_ce_7: 0.1426  loss_cate_7: 0  loss_mask_7: 0.7694  loss_dice_7: 0.9121  loss_ce_8: 0.1566  loss_cate_8: 0  loss_mask_8: 0.7771  loss_dice_8: 0.9114  time: 1.1021  data_time: 0.0129  lr: 8.6096e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:03:03 d2.utils.events]: \u001b[0m eta: 20:40:25  iter: 12279  total_loss: 19.65  loss_ce: 0.1213  loss_cate: 0.1329  loss_mask: 0.7995  loss_dice: 0.9637  loss_ce_0: 0.3215  loss_cate_0: 0  loss_mask_0: 0.8164  loss_dice_0: 0.9801  loss_ce_1: 0.2445  loss_cate_1: 0  loss_mask_1: 0.7943  loss_dice_1: 0.9351  loss_ce_2: 0.22  loss_cate_2: 0  loss_mask_2: 0.8007  loss_dice_2: 0.9229  loss_ce_3: 0.1796  loss_cate_3: 0  loss_mask_3: 0.795  loss_dice_3: 0.9228  loss_ce_4: 0.1349  loss_cate_4: 0  loss_mask_4: 0.8214  loss_dice_4: 0.9329  loss_ce_5: 0.1863  loss_cate_5: 0  loss_mask_5: 0.8218  loss_dice_5: 0.9068  loss_ce_6: 0.1591  loss_cate_6: 0  loss_mask_6: 0.7914  loss_dice_6: 0.894  loss_ce_7: 0.1733  loss_cate_7: 0  loss_mask_7: 0.812  loss_dice_7: 0.9287  loss_ce_8: 0.1621  loss_cate_8: 0  loss_mask_8: 0.8031  loss_dice_8: 0.9422  time: 1.1021  data_time: 0.0142  lr: 8.6074e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:03:26 d2.utils.events]: \u001b[0m eta: 20:40:05  iter: 12299  total_loss: 21.02  loss_ce: 0.1816  loss_cate: 0.1649  loss_mask: 0.753  loss_dice: 1.052  loss_ce_0: 0.3228  loss_cate_0: 0  loss_mask_0: 0.7463  loss_dice_0: 1.123  loss_ce_1: 0.2243  loss_cate_1: 0  loss_mask_1: 0.75  loss_dice_1: 1.076  loss_ce_2: 0.2783  loss_cate_2: 0  loss_mask_2: 0.749  loss_dice_2: 1.034  loss_ce_3: 0.2423  loss_cate_3: 0  loss_mask_3: 0.7615  loss_dice_3: 1.02  loss_ce_4: 0.2269  loss_cate_4: 0  loss_mask_4: 0.7738  loss_dice_4: 1.025  loss_ce_5: 0.1967  loss_cate_5: 0  loss_mask_5: 0.759  loss_dice_5: 1.067  loss_ce_6: 0.2123  loss_cate_6: 0  loss_mask_6: 0.7685  loss_dice_6: 1.024  loss_ce_7: 0.1984  loss_cate_7: 0  loss_mask_7: 0.7757  loss_dice_7: 1.064  loss_ce_8: 0.1761  loss_cate_8: 0  loss_mask_8: 0.7653  loss_dice_8: 1.068  time: 1.1021  data_time: 0.0124  lr: 8.6051e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:03:48 d2.utils.events]: \u001b[0m eta: 20:39:47  iter: 12319  total_loss: 19.51  loss_ce: 0.2109  loss_cate: 0.1633  loss_mask: 0.8481  loss_dice: 0.9174  loss_ce_0: 0.2671  loss_cate_0: 0  loss_mask_0: 0.8223  loss_dice_0: 0.9936  loss_ce_1: 0.1912  loss_cate_1: 0  loss_mask_1: 0.8657  loss_dice_1: 0.938  loss_ce_2: 0.1826  loss_cate_2: 0  loss_mask_2: 0.847  loss_dice_2: 0.8963  loss_ce_3: 0.194  loss_cate_3: 0  loss_mask_3: 0.8112  loss_dice_3: 0.9372  loss_ce_4: 0.1771  loss_cate_4: 0  loss_mask_4: 0.8192  loss_dice_4: 0.9566  loss_ce_5: 0.1621  loss_cate_5: 0  loss_mask_5: 0.8168  loss_dice_5: 0.9272  loss_ce_6: 0.1949  loss_cate_6: 0  loss_mask_6: 0.7668  loss_dice_6: 0.9747  loss_ce_7: 0.1755  loss_cate_7: 0  loss_mask_7: 0.7519  loss_dice_7: 0.965  loss_ce_8: 0.2254  loss_cate_8: 0  loss_mask_8: 0.8034  loss_dice_8: 0.927  time: 1.1021  data_time: 0.0138  lr: 8.6028e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:04:11 d2.utils.events]: \u001b[0m eta: 20:39:25  iter: 12339  total_loss: 19.54  loss_ce: 0.3107  loss_cate: 0.1554  loss_mask: 0.7413  loss_dice: 0.9243  loss_ce_0: 0.4636  loss_cate_0: 0  loss_mask_0: 0.7498  loss_dice_0: 0.9221  loss_ce_1: 0.2808  loss_cate_1: 0  loss_mask_1: 0.7636  loss_dice_1: 0.9137  loss_ce_2: 0.2969  loss_cate_2: 0  loss_mask_2: 0.7704  loss_dice_2: 0.9125  loss_ce_3: 0.2531  loss_cate_3: 0  loss_mask_3: 0.7636  loss_dice_3: 0.9502  loss_ce_4: 0.2766  loss_cate_4: 0  loss_mask_4: 0.7667  loss_dice_4: 0.9579  loss_ce_5: 0.2631  loss_cate_5: 0  loss_mask_5: 0.7532  loss_dice_5: 0.9258  loss_ce_6: 0.2922  loss_cate_6: 0  loss_mask_6: 0.7514  loss_dice_6: 0.8763  loss_ce_7: 0.2565  loss_cate_7: 0  loss_mask_7: 0.7621  loss_dice_7: 0.9372  loss_ce_8: 0.2777  loss_cate_8: 0  loss_mask_8: 0.7308  loss_dice_8: 0.9618  time: 1.1021  data_time: 0.0151  lr: 8.6005e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:04:33 d2.utils.events]: \u001b[0m eta: 20:39:07  iter: 12359  total_loss: 20.84  loss_ce: 0.2309  loss_cate: 0.1557  loss_mask: 0.7824  loss_dice: 1.023  loss_ce_0: 0.4142  loss_cate_0: 0  loss_mask_0: 0.8209  loss_dice_0: 1.042  loss_ce_1: 0.2546  loss_cate_1: 0  loss_mask_1: 0.7817  loss_dice_1: 1.039  loss_ce_2: 0.2364  loss_cate_2: 0  loss_mask_2: 0.7868  loss_dice_2: 0.9988  loss_ce_3: 0.2217  loss_cate_3: 0  loss_mask_3: 0.8004  loss_dice_3: 1.04  loss_ce_4: 0.2313  loss_cate_4: 0  loss_mask_4: 0.778  loss_dice_4: 1.034  loss_ce_5: 0.2259  loss_cate_5: 0  loss_mask_5: 0.7941  loss_dice_5: 1.036  loss_ce_6: 0.1961  loss_cate_6: 0  loss_mask_6: 0.7985  loss_dice_6: 0.9973  loss_ce_7: 0.2107  loss_cate_7: 0  loss_mask_7: 0.7928  loss_dice_7: 1.033  loss_ce_8: 0.2103  loss_cate_8: 0  loss_mask_8: 0.7657  loss_dice_8: 1.015  time: 1.1021  data_time: 0.0141  lr: 8.5982e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:04:56 d2.utils.events]: \u001b[0m eta: 20:38:57  iter: 12379  total_loss: 20.88  loss_ce: 0.1688  loss_cate: 0.1395  loss_mask: 0.8229  loss_dice: 0.9219  loss_ce_0: 0.2913  loss_cate_0: 0  loss_mask_0: 0.826  loss_dice_0: 1.086  loss_ce_1: 0.2048  loss_cate_1: 0  loss_mask_1: 0.8082  loss_dice_1: 1.057  loss_ce_2: 0.2079  loss_cate_2: 0  loss_mask_2: 0.8533  loss_dice_2: 1.006  loss_ce_3: 0.1836  loss_cate_3: 0  loss_mask_3: 0.8113  loss_dice_3: 0.9747  loss_ce_4: 0.1731  loss_cate_4: 0  loss_mask_4: 0.8093  loss_dice_4: 0.9012  loss_ce_5: 0.1604  loss_cate_5: 0  loss_mask_5: 0.8118  loss_dice_5: 0.9067  loss_ce_6: 0.1717  loss_cate_6: 0  loss_mask_6: 0.7941  loss_dice_6: 0.8698  loss_ce_7: 0.1751  loss_cate_7: 0  loss_mask_7: 0.8232  loss_dice_7: 0.9002  loss_ce_8: 0.1362  loss_cate_8: 0  loss_mask_8: 0.8009  loss_dice_8: 0.9031  time: 1.1021  data_time: 0.0124  lr: 8.5959e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:05:19 d2.utils.events]: \u001b[0m eta: 20:38:51  iter: 12399  total_loss: 20.89  loss_ce: 0.2309  loss_cate: 0.1312  loss_mask: 0.7027  loss_dice: 0.9688  loss_ce_0: 0.3383  loss_cate_0: 0  loss_mask_0: 0.7153  loss_dice_0: 1.06  loss_ce_1: 0.231  loss_cate_1: 0  loss_mask_1: 0.7318  loss_dice_1: 0.9668  loss_ce_2: 0.1957  loss_cate_2: 0  loss_mask_2: 0.7179  loss_dice_2: 0.9385  loss_ce_3: 0.2123  loss_cate_3: 0  loss_mask_3: 0.7193  loss_dice_3: 0.9575  loss_ce_4: 0.2147  loss_cate_4: 0  loss_mask_4: 0.7325  loss_dice_4: 0.9343  loss_ce_5: 0.1997  loss_cate_5: 0  loss_mask_5: 0.7277  loss_dice_5: 0.9741  loss_ce_6: 0.2162  loss_cate_6: 0  loss_mask_6: 0.7078  loss_dice_6: 0.9526  loss_ce_7: 0.2115  loss_cate_7: 0  loss_mask_7: 0.7095  loss_dice_7: 0.9451  loss_ce_8: 0.2452  loss_cate_8: 0  loss_mask_8: 0.7028  loss_dice_8: 0.9807  time: 1.1021  data_time: 0.0127  lr: 8.5936e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:05:41 d2.utils.events]: \u001b[0m eta: 20:38:37  iter: 12419  total_loss: 20.14  loss_ce: 0.09111  loss_cate: 0.1441  loss_mask: 0.7782  loss_dice: 0.9976  loss_ce_0: 0.3255  loss_cate_0: 0  loss_mask_0: 0.8022  loss_dice_0: 1.011  loss_ce_1: 0.1228  loss_cate_1: 0  loss_mask_1: 0.8131  loss_dice_1: 1.016  loss_ce_2: 0.1187  loss_cate_2: 0  loss_mask_2: 0.7855  loss_dice_2: 0.9651  loss_ce_3: 0.1203  loss_cate_3: 0  loss_mask_3: 0.755  loss_dice_3: 0.9591  loss_ce_4: 0.1362  loss_cate_4: 0  loss_mask_4: 0.772  loss_dice_4: 0.9682  loss_ce_5: 0.1174  loss_cate_5: 0  loss_mask_5: 0.797  loss_dice_5: 0.9872  loss_ce_6: 0.08238  loss_cate_6: 0  loss_mask_6: 0.793  loss_dice_6: 0.9644  loss_ce_7: 0.07976  loss_cate_7: 0  loss_mask_7: 0.7897  loss_dice_7: 1.004  loss_ce_8: 0.09899  loss_cate_8: 0  loss_mask_8: 0.7759  loss_dice_8: 0.9904  time: 1.1022  data_time: 0.0138  lr: 8.5913e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:06:04 d2.utils.events]: \u001b[0m eta: 20:38:23  iter: 12439  total_loss: 18.7  loss_ce: 0.1103  loss_cate: 0.141  loss_mask: 0.7799  loss_dice: 0.9155  loss_ce_0: 0.2419  loss_cate_0: 0  loss_mask_0: 0.7849  loss_dice_0: 0.9784  loss_ce_1: 0.1723  loss_cate_1: 0  loss_mask_1: 0.7463  loss_dice_1: 0.9277  loss_ce_2: 0.09833  loss_cate_2: 0  loss_mask_2: 0.7862  loss_dice_2: 0.9171  loss_ce_3: 0.1103  loss_cate_3: 0  loss_mask_3: 0.7491  loss_dice_3: 0.8568  loss_ce_4: 0.1092  loss_cate_4: 0  loss_mask_4: 0.7616  loss_dice_4: 0.8919  loss_ce_5: 0.1167  loss_cate_5: 0  loss_mask_5: 0.7237  loss_dice_5: 0.8799  loss_ce_6: 0.104  loss_cate_6: 0  loss_mask_6: 0.7685  loss_dice_6: 0.9146  loss_ce_7: 0.1028  loss_cate_7: 0  loss_mask_7: 0.7637  loss_dice_7: 0.9172  loss_ce_8: 0.09224  loss_cate_8: 0  loss_mask_8: 0.7749  loss_dice_8: 0.927  time: 1.1022  data_time: 0.0139  lr: 8.5891e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:06:26 d2.utils.events]: \u001b[0m eta: 20:38:01  iter: 12459  total_loss: 18.96  loss_ce: 0.1649  loss_cate: 0.1203  loss_mask: 0.7768  loss_dice: 0.9762  loss_ce_0: 0.3848  loss_cate_0: 0  loss_mask_0: 0.7923  loss_dice_0: 0.9814  loss_ce_1: 0.1957  loss_cate_1: 0  loss_mask_1: 0.7853  loss_dice_1: 0.9718  loss_ce_2: 0.2418  loss_cate_2: 0  loss_mask_2: 0.8092  loss_dice_2: 1.004  loss_ce_3: 0.2331  loss_cate_3: 0  loss_mask_3: 0.7791  loss_dice_3: 0.954  loss_ce_4: 0.2254  loss_cate_4: 0  loss_mask_4: 0.7295  loss_dice_4: 0.9571  loss_ce_5: 0.228  loss_cate_5: 0  loss_mask_5: 0.7786  loss_dice_5: 0.9099  loss_ce_6: 0.1951  loss_cate_6: 0  loss_mask_6: 0.7834  loss_dice_6: 0.9631  loss_ce_7: 0.2176  loss_cate_7: 0  loss_mask_7: 0.7779  loss_dice_7: 0.9646  loss_ce_8: 0.219  loss_cate_8: 0  loss_mask_8: 0.7232  loss_dice_8: 0.9742  time: 1.1022  data_time: 0.0131  lr: 8.5868e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:06:48 d2.utils.events]: \u001b[0m eta: 20:37:41  iter: 12479  total_loss: 18.96  loss_ce: 0.1188  loss_cate: 0.1526  loss_mask: 0.7045  loss_dice: 0.8909  loss_ce_0: 0.2372  loss_cate_0: 0  loss_mask_0: 0.7794  loss_dice_0: 0.9719  loss_ce_1: 0.1652  loss_cate_1: 0  loss_mask_1: 0.7546  loss_dice_1: 0.9015  loss_ce_2: 0.1369  loss_cate_2: 0  loss_mask_2: 0.7299  loss_dice_2: 0.9308  loss_ce_3: 0.1078  loss_cate_3: 0  loss_mask_3: 0.7285  loss_dice_3: 0.9042  loss_ce_4: 0.1119  loss_cate_4: 0  loss_mask_4: 0.7342  loss_dice_4: 0.9119  loss_ce_5: 0.1379  loss_cate_5: 0  loss_mask_5: 0.7341  loss_dice_5: 0.8935  loss_ce_6: 0.1289  loss_cate_6: 0  loss_mask_6: 0.7234  loss_dice_6: 0.914  loss_ce_7: 0.1057  loss_cate_7: 0  loss_mask_7: 0.7366  loss_dice_7: 0.9214  loss_ce_8: 0.1045  loss_cate_8: 0  loss_mask_8: 0.7111  loss_dice_8: 0.9048  time: 1.1022  data_time: 0.0126  lr: 8.5845e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:07:11 d2.utils.events]: \u001b[0m eta: 20:37:08  iter: 12499  total_loss: 20.15  loss_ce: 0.1799  loss_cate: 0.1684  loss_mask: 0.8593  loss_dice: 0.9275  loss_ce_0: 0.3536  loss_cate_0: 0  loss_mask_0: 0.9101  loss_dice_0: 1.008  loss_ce_1: 0.1958  loss_cate_1: 0  loss_mask_1: 0.8837  loss_dice_1: 0.9829  loss_ce_2: 0.2223  loss_cate_2: 0  loss_mask_2: 0.8697  loss_dice_2: 0.9172  loss_ce_3: 0.1839  loss_cate_3: 0  loss_mask_3: 0.8697  loss_dice_3: 0.9248  loss_ce_4: 0.1862  loss_cate_4: 0  loss_mask_4: 0.8775  loss_dice_4: 0.9503  loss_ce_5: 0.1548  loss_cate_5: 0  loss_mask_5: 0.875  loss_dice_5: 0.9415  loss_ce_6: 0.1676  loss_cate_6: 0  loss_mask_6: 0.8679  loss_dice_6: 0.9325  loss_ce_7: 0.1746  loss_cate_7: 0  loss_mask_7: 0.8742  loss_dice_7: 0.9438  loss_ce_8: 0.1632  loss_cate_8: 0  loss_mask_8: 0.8785  loss_dice_8: 0.9454  time: 1.1021  data_time: 0.0125  lr: 8.5822e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:07:34 d2.utils.events]: \u001b[0m eta: 20:36:46  iter: 12519  total_loss: 23.99  loss_ce: 0.2076  loss_cate: 0.1567  loss_mask: 0.908  loss_dice: 1.157  loss_ce_0: 0.3821  loss_cate_0: 0  loss_mask_0: 0.986  loss_dice_0: 1.13  loss_ce_1: 0.2518  loss_cate_1: 0  loss_mask_1: 0.9502  loss_dice_1: 1.145  loss_ce_2: 0.2415  loss_cate_2: 0  loss_mask_2: 0.9114  loss_dice_2: 1.203  loss_ce_3: 0.2794  loss_cate_3: 0  loss_mask_3: 0.923  loss_dice_3: 1.152  loss_ce_4: 0.2294  loss_cate_4: 0  loss_mask_4: 0.9338  loss_dice_4: 1.149  loss_ce_5: 0.2136  loss_cate_5: 0  loss_mask_5: 0.9302  loss_dice_5: 1.142  loss_ce_6: 0.2231  loss_cate_6: 0  loss_mask_6: 0.9212  loss_dice_6: 1.152  loss_ce_7: 0.1584  loss_cate_7: 0  loss_mask_7: 0.8965  loss_dice_7: 1.114  loss_ce_8: 0.2302  loss_cate_8: 0  loss_mask_8: 0.9114  loss_dice_8: 1.116  time: 1.1021  data_time: 0.0135  lr: 8.5799e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:07:56 d2.utils.events]: \u001b[0m eta: 20:36:21  iter: 12539  total_loss: 19.69  loss_ce: 0.2559  loss_cate: 0.1561  loss_mask: 0.6991  loss_dice: 0.9466  loss_ce_0: 0.3452  loss_cate_0: 0  loss_mask_0: 0.7221  loss_dice_0: 1.004  loss_ce_1: 0.2092  loss_cate_1: 0  loss_mask_1: 0.698  loss_dice_1: 1.024  loss_ce_2: 0.2272  loss_cate_2: 0  loss_mask_2: 0.6992  loss_dice_2: 1.036  loss_ce_3: 0.2417  loss_cate_3: 0  loss_mask_3: 0.693  loss_dice_3: 0.9973  loss_ce_4: 0.2492  loss_cate_4: 0  loss_mask_4: 0.6912  loss_dice_4: 0.988  loss_ce_5: 0.2675  loss_cate_5: 0  loss_mask_5: 0.7011  loss_dice_5: 1.004  loss_ce_6: 0.2241  loss_cate_6: 0  loss_mask_6: 0.6961  loss_dice_6: 0.9829  loss_ce_7: 0.2817  loss_cate_7: 0  loss_mask_7: 0.6997  loss_dice_7: 0.9637  loss_ce_8: 0.2819  loss_cate_8: 0  loss_mask_8: 0.7018  loss_dice_8: 0.9686  time: 1.1021  data_time: 0.0132  lr: 8.5776e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:08:18 d2.utils.events]: \u001b[0m eta: 20:35:57  iter: 12559  total_loss: 20.38  loss_ce: 0.2339  loss_cate: 0.1555  loss_mask: 0.8409  loss_dice: 0.9712  loss_ce_0: 0.3634  loss_cate_0: 0  loss_mask_0: 0.8157  loss_dice_0: 1.009  loss_ce_1: 0.2313  loss_cate_1: 0  loss_mask_1: 0.8481  loss_dice_1: 1.006  loss_ce_2: 0.2283  loss_cate_2: 0  loss_mask_2: 0.8731  loss_dice_2: 1.027  loss_ce_3: 0.231  loss_cate_3: 0  loss_mask_3: 0.806  loss_dice_3: 0.9484  loss_ce_4: 0.2437  loss_cate_4: 0  loss_mask_4: 0.8382  loss_dice_4: 0.9292  loss_ce_5: 0.2063  loss_cate_5: 0  loss_mask_5: 0.8383  loss_dice_5: 0.9719  loss_ce_6: 0.2253  loss_cate_6: 0  loss_mask_6: 0.8316  loss_dice_6: 0.9775  loss_ce_7: 0.2458  loss_cate_7: 0  loss_mask_7: 0.8198  loss_dice_7: 0.981  loss_ce_8: 0.2432  loss_cate_8: 0  loss_mask_8: 0.8253  loss_dice_8: 0.9469  time: 1.1021  data_time: 0.0135  lr: 8.5753e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:08:41 d2.utils.events]: \u001b[0m eta: 20:35:35  iter: 12579  total_loss: 20.1  loss_ce: 0.2395  loss_cate: 0.1618  loss_mask: 0.7181  loss_dice: 0.974  loss_ce_0: 0.3284  loss_cate_0: 0  loss_mask_0: 0.7697  loss_dice_0: 1.054  loss_ce_1: 0.2749  loss_cate_1: 0  loss_mask_1: 0.7287  loss_dice_1: 0.9818  loss_ce_2: 0.272  loss_cate_2: 0  loss_mask_2: 0.7387  loss_dice_2: 0.9335  loss_ce_3: 0.2665  loss_cate_3: 0  loss_mask_3: 0.7758  loss_dice_3: 0.957  loss_ce_4: 0.2312  loss_cate_4: 0  loss_mask_4: 0.7697  loss_dice_4: 0.9932  loss_ce_5: 0.2747  loss_cate_5: 0  loss_mask_5: 0.7582  loss_dice_5: 1.002  loss_ce_6: 0.2609  loss_cate_6: 0  loss_mask_6: 0.7404  loss_dice_6: 0.9659  loss_ce_7: 0.2392  loss_cate_7: 0  loss_mask_7: 0.7641  loss_dice_7: 0.9605  loss_ce_8: 0.1977  loss_cate_8: 0  loss_mask_8: 0.7679  loss_dice_8: 0.9625  time: 1.1021  data_time: 0.0152  lr: 8.573e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:09:04 d2.utils.events]: \u001b[0m eta: 20:35:13  iter: 12599  total_loss: 20.04  loss_ce: 0.1863  loss_cate: 0.1465  loss_mask: 0.7186  loss_dice: 0.9548  loss_ce_0: 0.3351  loss_cate_0: 0  loss_mask_0: 0.7302  loss_dice_0: 1.001  loss_ce_1: 0.244  loss_cate_1: 0  loss_mask_1: 0.7413  loss_dice_1: 0.9928  loss_ce_2: 0.2274  loss_cate_2: 0  loss_mask_2: 0.7369  loss_dice_2: 0.9427  loss_ce_3: 0.2347  loss_cate_3: 0  loss_mask_3: 0.7318  loss_dice_3: 0.9405  loss_ce_4: 0.2218  loss_cate_4: 0  loss_mask_4: 0.7923  loss_dice_4: 0.9581  loss_ce_5: 0.1906  loss_cate_5: 0  loss_mask_5: 0.7116  loss_dice_5: 0.97  loss_ce_6: 0.1752  loss_cate_6: 0  loss_mask_6: 0.7051  loss_dice_6: 0.9335  loss_ce_7: 0.224  loss_cate_7: 0  loss_mask_7: 0.7113  loss_dice_7: 0.9473  loss_ce_8: 0.2366  loss_cate_8: 0  loss_mask_8: 0.7155  loss_dice_8: 0.9313  time: 1.1021  data_time: 0.0143  lr: 8.5707e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:09:26 d2.utils.events]: \u001b[0m eta: 20:34:49  iter: 12619  total_loss: 21.12  loss_ce: 0.2945  loss_cate: 0.1468  loss_mask: 0.7573  loss_dice: 1.048  loss_ce_0: 0.3373  loss_cate_0: 0  loss_mask_0: 0.8043  loss_dice_0: 1.056  loss_ce_1: 0.23  loss_cate_1: 0  loss_mask_1: 0.7668  loss_dice_1: 1.05  loss_ce_2: 0.2101  loss_cate_2: 0  loss_mask_2: 0.7646  loss_dice_2: 1.046  loss_ce_3: 0.2134  loss_cate_3: 0  loss_mask_3: 0.78  loss_dice_3: 1.019  loss_ce_4: 0.1995  loss_cate_4: 0  loss_mask_4: 0.7546  loss_dice_4: 1.046  loss_ce_5: 0.2016  loss_cate_5: 0  loss_mask_5: 0.7392  loss_dice_5: 1.049  loss_ce_6: 0.1399  loss_cate_6: 0  loss_mask_6: 0.743  loss_dice_6: 1.046  loss_ce_7: 0.2074  loss_cate_7: 0  loss_mask_7: 0.7375  loss_dice_7: 1.038  loss_ce_8: 0.1907  loss_cate_8: 0  loss_mask_8: 0.7298  loss_dice_8: 0.999  time: 1.1021  data_time: 0.0130  lr: 8.5685e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:09:48 d2.utils.events]: \u001b[0m eta: 20:34:22  iter: 12639  total_loss: 19.11  loss_ce: 0.2465  loss_cate: 0.1319  loss_mask: 0.6639  loss_dice: 0.9121  loss_ce_0: 0.3281  loss_cate_0: 0  loss_mask_0: 0.6976  loss_dice_0: 1.023  loss_ce_1: 0.2332  loss_cate_1: 0  loss_mask_1: 0.7037  loss_dice_1: 0.9659  loss_ce_2: 0.1154  loss_cate_2: 0  loss_mask_2: 0.6586  loss_dice_2: 0.9303  loss_ce_3: 0.157  loss_cate_3: 0  loss_mask_3: 0.6684  loss_dice_3: 0.9444  loss_ce_4: 0.1813  loss_cate_4: 0  loss_mask_4: 0.6599  loss_dice_4: 0.9413  loss_ce_5: 0.2756  loss_cate_5: 0  loss_mask_5: 0.6381  loss_dice_5: 0.943  loss_ce_6: 0.1664  loss_cate_6: 0  loss_mask_6: 0.6549  loss_dice_6: 0.9601  loss_ce_7: 0.2836  loss_cate_7: 0  loss_mask_7: 0.6601  loss_dice_7: 0.9463  loss_ce_8: 0.2291  loss_cate_8: 0  loss_mask_8: 0.6618  loss_dice_8: 0.9118  time: 1.1021  data_time: 0.0128  lr: 8.5662e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:10:11 d2.utils.events]: \u001b[0m eta: 20:33:59  iter: 12659  total_loss: 19.17  loss_ce: 0.224  loss_cate: 0.1398  loss_mask: 0.7409  loss_dice: 0.8812  loss_ce_0: 0.318  loss_cate_0: 0  loss_mask_0: 0.7115  loss_dice_0: 0.9662  loss_ce_1: 0.3168  loss_cate_1: 0  loss_mask_1: 0.7491  loss_dice_1: 0.9013  loss_ce_2: 0.287  loss_cate_2: 0  loss_mask_2: 0.7246  loss_dice_2: 0.8971  loss_ce_3: 0.2551  loss_cate_3: 0  loss_mask_3: 0.7302  loss_dice_3: 0.9173  loss_ce_4: 0.2295  loss_cate_4: 0  loss_mask_4: 0.7279  loss_dice_4: 0.9122  loss_ce_5: 0.1885  loss_cate_5: 0  loss_mask_5: 0.747  loss_dice_5: 0.8936  loss_ce_6: 0.1908  loss_cate_6: 0  loss_mask_6: 0.732  loss_dice_6: 0.8951  loss_ce_7: 0.1995  loss_cate_7: 0  loss_mask_7: 0.7644  loss_dice_7: 0.8909  loss_ce_8: 0.2081  loss_cate_8: 0  loss_mask_8: 0.7346  loss_dice_8: 0.9069  time: 1.1021  data_time: 0.0136  lr: 8.5639e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:10:34 d2.utils.events]: \u001b[0m eta: 20:33:45  iter: 12679  total_loss: 20.34  loss_ce: 0.1331  loss_cate: 0.1382  loss_mask: 0.7966  loss_dice: 1.039  loss_ce_0: 0.327  loss_cate_0: 0  loss_mask_0: 0.8448  loss_dice_0: 1.051  loss_ce_1: 0.2117  loss_cate_1: 0  loss_mask_1: 0.7945  loss_dice_1: 1.034  loss_ce_2: 0.2109  loss_cate_2: 0  loss_mask_2: 0.7663  loss_dice_2: 0.9897  loss_ce_3: 0.1412  loss_cate_3: 0  loss_mask_3: 0.785  loss_dice_3: 1.023  loss_ce_4: 0.1274  loss_cate_4: 0  loss_mask_4: 0.7804  loss_dice_4: 1.007  loss_ce_5: 0.1549  loss_cate_5: 0  loss_mask_5: 0.7892  loss_dice_5: 1.043  loss_ce_6: 0.1228  loss_cate_6: 0  loss_mask_6: 0.7818  loss_dice_6: 1.004  loss_ce_7: 0.156  loss_cate_7: 0  loss_mask_7: 0.7795  loss_dice_7: 1.02  loss_ce_8: 0.1311  loss_cate_8: 0  loss_mask_8: 0.7866  loss_dice_8: 1.037  time: 1.1021  data_time: 0.0143  lr: 8.5616e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:10:56 d2.utils.events]: \u001b[0m eta: 20:33:16  iter: 12699  total_loss: 22.49  loss_ce: 0.1231  loss_cate: 0.1257  loss_mask: 0.8728  loss_dice: 1.004  loss_ce_0: 0.2617  loss_cate_0: 0  loss_mask_0: 0.9123  loss_dice_0: 1.015  loss_ce_1: 0.189  loss_cate_1: 0  loss_mask_1: 0.9181  loss_dice_1: 1.06  loss_ce_2: 0.2348  loss_cate_2: 0  loss_mask_2: 0.8835  loss_dice_2: 0.9717  loss_ce_3: 0.1745  loss_cate_3: 0  loss_mask_3: 0.9326  loss_dice_3: 1.004  loss_ce_4: 0.1224  loss_cate_4: 0  loss_mask_4: 0.8775  loss_dice_4: 1.009  loss_ce_5: 0.1508  loss_cate_5: 0  loss_mask_5: 0.8848  loss_dice_5: 0.9972  loss_ce_6: 0.1258  loss_cate_6: 0  loss_mask_6: 0.8708  loss_dice_6: 0.9983  loss_ce_7: 0.09271  loss_cate_7: 0  loss_mask_7: 0.871  loss_dice_7: 1.032  loss_ce_8: 0.1544  loss_cate_8: 0  loss_mask_8: 0.8982  loss_dice_8: 1.009  time: 1.1021  data_time: 0.0121  lr: 8.5593e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:11:18 d2.utils.events]: \u001b[0m eta: 20:32:51  iter: 12719  total_loss: 18.62  loss_ce: 0.1203  loss_cate: 0.1434  loss_mask: 0.7764  loss_dice: 0.8675  loss_ce_0: 0.2169  loss_cate_0: 0  loss_mask_0: 0.8207  loss_dice_0: 0.8791  loss_ce_1: 0.1633  loss_cate_1: 0  loss_mask_1: 0.8032  loss_dice_1: 0.8971  loss_ce_2: 0.1199  loss_cate_2: 0  loss_mask_2: 0.8146  loss_dice_2: 0.8985  loss_ce_3: 0.1297  loss_cate_3: 0  loss_mask_3: 0.8063  loss_dice_3: 0.8862  loss_ce_4: 0.1118  loss_cate_4: 0  loss_mask_4: 0.8009  loss_dice_4: 0.8842  loss_ce_5: 0.1117  loss_cate_5: 0  loss_mask_5: 0.7592  loss_dice_5: 0.8775  loss_ce_6: 0.1163  loss_cate_6: 0  loss_mask_6: 0.7823  loss_dice_6: 0.8856  loss_ce_7: 0.1038  loss_cate_7: 0  loss_mask_7: 0.7765  loss_dice_7: 0.8819  loss_ce_8: 0.1144  loss_cate_8: 0  loss_mask_8: 0.7688  loss_dice_8: 0.8918  time: 1.1021  data_time: 0.0131  lr: 8.557e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:11:42 d2.utils.events]: \u001b[0m eta: 20:32:02  iter: 12739  total_loss: 21.02  loss_ce: 0.2587  loss_cate: 0.1509  loss_mask: 0.8011  loss_dice: 0.9577  loss_ce_0: 0.3936  loss_cate_0: 0  loss_mask_0: 0.8361  loss_dice_0: 0.9916  loss_ce_1: 0.2367  loss_cate_1: 0  loss_mask_1: 0.8287  loss_dice_1: 0.9949  loss_ce_2: 0.2429  loss_cate_2: 0  loss_mask_2: 0.7785  loss_dice_2: 0.934  loss_ce_3: 0.2395  loss_cate_3: 0  loss_mask_3: 0.8222  loss_dice_3: 0.9923  loss_ce_4: 0.2508  loss_cate_4: 0  loss_mask_4: 0.8228  loss_dice_4: 0.9377  loss_ce_5: 0.211  loss_cate_5: 0  loss_mask_5: 0.804  loss_dice_5: 1.003  loss_ce_6: 0.2407  loss_cate_6: 0  loss_mask_6: 0.785  loss_dice_6: 0.9999  loss_ce_7: 0.2249  loss_cate_7: 0  loss_mask_7: 0.7994  loss_dice_7: 1.002  loss_ce_8: 0.2172  loss_cate_8: 0  loss_mask_8: 0.8087  loss_dice_8: 1.013  time: 1.1021  data_time: 0.0128  lr: 8.5547e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:12:05 d2.utils.events]: \u001b[0m eta: 20:31:40  iter: 12759  total_loss: 18.17  loss_ce: 0.2515  loss_cate: 0.1381  loss_mask: 0.6372  loss_dice: 0.9079  loss_ce_0: 0.3779  loss_cate_0: 0  loss_mask_0: 0.6554  loss_dice_0: 0.9703  loss_ce_1: 0.2618  loss_cate_1: 0  loss_mask_1: 0.6471  loss_dice_1: 0.9141  loss_ce_2: 0.2502  loss_cate_2: 0  loss_mask_2: 0.6105  loss_dice_2: 0.9184  loss_ce_3: 0.1986  loss_cate_3: 0  loss_mask_3: 0.6375  loss_dice_3: 0.9033  loss_ce_4: 0.3016  loss_cate_4: 0  loss_mask_4: 0.6165  loss_dice_4: 0.8894  loss_ce_5: 0.2389  loss_cate_5: 0  loss_mask_5: 0.6117  loss_dice_5: 0.9123  loss_ce_6: 0.2583  loss_cate_6: 0  loss_mask_6: 0.6192  loss_dice_6: 0.9034  loss_ce_7: 0.2655  loss_cate_7: 0  loss_mask_7: 0.6295  loss_dice_7: 0.9312  loss_ce_8: 0.2255  loss_cate_8: 0  loss_mask_8: 0.6354  loss_dice_8: 0.8968  time: 1.1021  data_time: 0.0128  lr: 8.5524e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:12:27 d2.utils.events]: \u001b[0m eta: 20:31:14  iter: 12779  total_loss: 20.82  loss_ce: 0.2314  loss_cate: 0.1526  loss_mask: 0.7922  loss_dice: 0.9199  loss_ce_0: 0.3202  loss_cate_0: 0  loss_mask_0: 0.8175  loss_dice_0: 1.024  loss_ce_1: 0.2051  loss_cate_1: 0  loss_mask_1: 0.8145  loss_dice_1: 0.9503  loss_ce_2: 0.2873  loss_cate_2: 0  loss_mask_2: 0.806  loss_dice_2: 0.9863  loss_ce_3: 0.2502  loss_cate_3: 0  loss_mask_3: 0.7782  loss_dice_3: 0.9745  loss_ce_4: 0.2234  loss_cate_4: 0  loss_mask_4: 0.7856  loss_dice_4: 0.9741  loss_ce_5: 0.2726  loss_cate_5: 0  loss_mask_5: 0.7832  loss_dice_5: 0.9851  loss_ce_6: 0.2253  loss_cate_6: 0  loss_mask_6: 0.7982  loss_dice_6: 0.989  loss_ce_7: 0.2157  loss_cate_7: 0  loss_mask_7: 0.8059  loss_dice_7: 0.9501  loss_ce_8: 0.2334  loss_cate_8: 0  loss_mask_8: 0.7995  loss_dice_8: 0.9191  time: 1.1021  data_time: 0.0133  lr: 8.5501e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:12:49 d2.utils.events]: \u001b[0m eta: 20:30:54  iter: 12799  total_loss: 19.76  loss_ce: 0.1962  loss_cate: 0.1519  loss_mask: 0.7877  loss_dice: 0.9518  loss_ce_0: 0.3987  loss_cate_0: 0  loss_mask_0: 0.8954  loss_dice_0: 0.9488  loss_ce_1: 0.2318  loss_cate_1: 0  loss_mask_1: 0.8796  loss_dice_1: 0.9758  loss_ce_2: 0.2434  loss_cate_2: 0  loss_mask_2: 0.8642  loss_dice_2: 0.9501  loss_ce_3: 0.2261  loss_cate_3: 0  loss_mask_3: 0.8517  loss_dice_3: 0.9872  loss_ce_4: 0.2629  loss_cate_4: 0  loss_mask_4: 0.8664  loss_dice_4: 0.9613  loss_ce_5: 0.2007  loss_cate_5: 0  loss_mask_5: 0.8645  loss_dice_5: 0.9508  loss_ce_6: 0.2304  loss_cate_6: 0  loss_mask_6: 0.7921  loss_dice_6: 0.9476  loss_ce_7: 0.2068  loss_cate_7: 0  loss_mask_7: 0.7906  loss_dice_7: 0.9755  loss_ce_8: 0.195  loss_cate_8: 0  loss_mask_8: 0.7909  loss_dice_8: 0.9683  time: 1.1020  data_time: 0.0133  lr: 8.5479e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:13:11 d2.utils.events]: \u001b[0m eta: 20:30:34  iter: 12819  total_loss: 20.66  loss_ce: 0.1294  loss_cate: 0.1452  loss_mask: 0.8189  loss_dice: 0.9535  loss_ce_0: 0.2774  loss_cate_0: 0  loss_mask_0: 0.9078  loss_dice_0: 1.038  loss_ce_1: 0.2286  loss_cate_1: 0  loss_mask_1: 0.8394  loss_dice_1: 0.9737  loss_ce_2: 0.1754  loss_cate_2: 0  loss_mask_2: 0.8129  loss_dice_2: 0.9398  loss_ce_3: 0.2082  loss_cate_3: 0  loss_mask_3: 0.8115  loss_dice_3: 0.9567  loss_ce_4: 0.1776  loss_cate_4: 0  loss_mask_4: 0.8317  loss_dice_4: 1.012  loss_ce_5: 0.1955  loss_cate_5: 0  loss_mask_5: 0.8247  loss_dice_5: 0.9837  loss_ce_6: 0.1852  loss_cate_6: 0  loss_mask_6: 0.8221  loss_dice_6: 0.9605  loss_ce_7: 0.182  loss_cate_7: 0  loss_mask_7: 0.829  loss_dice_7: 0.984  loss_ce_8: 0.1619  loss_cate_8: 0  loss_mask_8: 0.831  loss_dice_8: 0.914  time: 1.1020  data_time: 0.0132  lr: 8.5456e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:13:33 d2.utils.events]: \u001b[0m eta: 20:30:12  iter: 12839  total_loss: 19.81  loss_ce: 0.1771  loss_cate: 0.1327  loss_mask: 0.805  loss_dice: 1.003  loss_ce_0: 0.2861  loss_cate_0: 0  loss_mask_0: 0.8004  loss_dice_0: 1.032  loss_ce_1: 0.2174  loss_cate_1: 0  loss_mask_1: 0.8292  loss_dice_1: 1.02  loss_ce_2: 0.1654  loss_cate_2: 0  loss_mask_2: 0.8062  loss_dice_2: 1.043  loss_ce_3: 0.1737  loss_cate_3: 0  loss_mask_3: 0.8034  loss_dice_3: 1.038  loss_ce_4: 0.1397  loss_cate_4: 0  loss_mask_4: 0.8322  loss_dice_4: 1.023  loss_ce_5: 0.1731  loss_cate_5: 0  loss_mask_5: 0.8125  loss_dice_5: 1.007  loss_ce_6: 0.1782  loss_cate_6: 0  loss_mask_6: 0.7994  loss_dice_6: 1.015  loss_ce_7: 0.1718  loss_cate_7: 0  loss_mask_7: 0.8031  loss_dice_7: 1.039  loss_ce_8: 0.2012  loss_cate_8: 0  loss_mask_8: 0.7967  loss_dice_8: 1.009  time: 1.1020  data_time: 0.0138  lr: 8.5433e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:13:56 d2.utils.events]: \u001b[0m eta: 20:29:50  iter: 12859  total_loss: 20.48  loss_ce: 0.1714  loss_cate: 0.1453  loss_mask: 0.793  loss_dice: 0.9843  loss_ce_0: 0.3172  loss_cate_0: 0  loss_mask_0: 0.7741  loss_dice_0: 1.071  loss_ce_1: 0.2334  loss_cate_1: 0  loss_mask_1: 0.7568  loss_dice_1: 0.9854  loss_ce_2: 0.2176  loss_cate_2: 0  loss_mask_2: 0.7712  loss_dice_2: 1.007  loss_ce_3: 0.1994  loss_cate_3: 0  loss_mask_3: 0.7543  loss_dice_3: 1.003  loss_ce_4: 0.1898  loss_cate_4: 0  loss_mask_4: 0.7735  loss_dice_4: 0.942  loss_ce_5: 0.2557  loss_cate_5: 0  loss_mask_5: 0.7794  loss_dice_5: 0.9434  loss_ce_6: 0.1736  loss_cate_6: 0  loss_mask_6: 0.7736  loss_dice_6: 0.9735  loss_ce_7: 0.2107  loss_cate_7: 0  loss_mask_7: 0.7833  loss_dice_7: 0.9789  loss_ce_8: 0.1639  loss_cate_8: 0  loss_mask_8: 0.7835  loss_dice_8: 0.9558  time: 1.1020  data_time: 0.0133  lr: 8.541e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:14:19 d2.utils.events]: \u001b[0m eta: 20:29:27  iter: 12879  total_loss: 20.98  loss_ce: 0.2754  loss_cate: 0.1461  loss_mask: 0.6992  loss_dice: 1.028  loss_ce_0: 0.3941  loss_cate_0: 0  loss_mask_0: 0.7434  loss_dice_0: 1.097  loss_ce_1: 0.2735  loss_cate_1: 0  loss_mask_1: 0.7053  loss_dice_1: 1.096  loss_ce_2: 0.28  loss_cate_2: 0  loss_mask_2: 0.6923  loss_dice_2: 1.014  loss_ce_3: 0.2935  loss_cate_3: 0  loss_mask_3: 0.7195  loss_dice_3: 0.985  loss_ce_4: 0.285  loss_cate_4: 0  loss_mask_4: 0.6985  loss_dice_4: 1.022  loss_ce_5: 0.2933  loss_cate_5: 0  loss_mask_5: 0.7144  loss_dice_5: 0.9979  loss_ce_6: 0.276  loss_cate_6: 0  loss_mask_6: 0.7038  loss_dice_6: 0.9752  loss_ce_7: 0.2318  loss_cate_7: 0  loss_mask_7: 0.7095  loss_dice_7: 0.9999  loss_ce_8: 0.2088  loss_cate_8: 0  loss_mask_8: 0.7249  loss_dice_8: 1.054  time: 1.1020  data_time: 0.0132  lr: 8.5387e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:14:41 d2.utils.events]: \u001b[0m eta: 20:29:06  iter: 12899  total_loss: 22.11  loss_ce: 0.3045  loss_cate: 0.1631  loss_mask: 0.821  loss_dice: 0.9751  loss_ce_0: 0.4182  loss_cate_0: 0  loss_mask_0: 0.8998  loss_dice_0: 1.052  loss_ce_1: 0.2873  loss_cate_1: 0  loss_mask_1: 0.869  loss_dice_1: 1.067  loss_ce_2: 0.2774  loss_cate_2: 0  loss_mask_2: 0.8552  loss_dice_2: 1.001  loss_ce_3: 0.2716  loss_cate_3: 0  loss_mask_3: 0.8584  loss_dice_3: 0.9749  loss_ce_4: 0.2931  loss_cate_4: 0  loss_mask_4: 0.8278  loss_dice_4: 0.978  loss_ce_5: 0.2812  loss_cate_5: 0  loss_mask_5: 0.8347  loss_dice_5: 0.9856  loss_ce_6: 0.2544  loss_cate_6: 0  loss_mask_6: 0.8368  loss_dice_6: 0.9628  loss_ce_7: 0.269  loss_cate_7: 0  loss_mask_7: 0.8215  loss_dice_7: 0.9996  loss_ce_8: 0.285  loss_cate_8: 0  loss_mask_8: 0.8426  loss_dice_8: 0.9922  time: 1.1020  data_time: 0.0131  lr: 8.5364e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:15:04 d2.utils.events]: \u001b[0m eta: 20:28:42  iter: 12919  total_loss: 20.17  loss_ce: 0.2949  loss_cate: 0.1526  loss_mask: 0.7092  loss_dice: 1.035  loss_ce_0: 0.3025  loss_cate_0: 0  loss_mask_0: 0.6983  loss_dice_0: 1.105  loss_ce_1: 0.2801  loss_cate_1: 0  loss_mask_1: 0.7306  loss_dice_1: 1.043  loss_ce_2: 0.2279  loss_cate_2: 0  loss_mask_2: 0.6761  loss_dice_2: 1.014  loss_ce_3: 0.242  loss_cate_3: 0  loss_mask_3: 0.7344  loss_dice_3: 0.9794  loss_ce_4: 0.3513  loss_cate_4: 0  loss_mask_4: 0.6946  loss_dice_4: 1.004  loss_ce_5: 0.3291  loss_cate_5: 0  loss_mask_5: 0.7086  loss_dice_5: 0.9713  loss_ce_6: 0.2808  loss_cate_6: 0  loss_mask_6: 0.712  loss_dice_6: 1.02  loss_ce_7: 0.2857  loss_cate_7: 0  loss_mask_7: 0.7137  loss_dice_7: 1.024  loss_ce_8: 0.263  loss_cate_8: 0  loss_mask_8: 0.723  loss_dice_8: 0.9823  time: 1.1020  data_time: 0.0153  lr: 8.5341e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:15:26 d2.utils.events]: \u001b[0m eta: 20:28:07  iter: 12939  total_loss: 20.03  loss_ce: 0.3055  loss_cate: 0.1462  loss_mask: 0.8275  loss_dice: 0.8966  loss_ce_0: 0.3346  loss_cate_0: 0  loss_mask_0: 0.9328  loss_dice_0: 0.9996  loss_ce_1: 0.2944  loss_cate_1: 0  loss_mask_1: 0.8728  loss_dice_1: 0.9366  loss_ce_2: 0.2753  loss_cate_2: 0  loss_mask_2: 0.8485  loss_dice_2: 0.9028  loss_ce_3: 0.2756  loss_cate_3: 0  loss_mask_3: 0.829  loss_dice_3: 0.9406  loss_ce_4: 0.2782  loss_cate_4: 0  loss_mask_4: 0.8163  loss_dice_4: 0.8724  loss_ce_5: 0.2622  loss_cate_5: 0  loss_mask_5: 0.8396  loss_dice_5: 0.9397  loss_ce_6: 0.2589  loss_cate_6: 0  loss_mask_6: 0.8412  loss_dice_6: 0.8816  loss_ce_7: 0.2795  loss_cate_7: 0  loss_mask_7: 0.8191  loss_dice_7: 0.9264  loss_ce_8: 0.2961  loss_cate_8: 0  loss_mask_8: 0.821  loss_dice_8: 0.9293  time: 1.1020  data_time: 0.0138  lr: 8.5318e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:15:48 d2.utils.events]: \u001b[0m eta: 20:27:58  iter: 12959  total_loss: 18.28  loss_ce: 0.2432  loss_cate: 0.1524  loss_mask: 0.7785  loss_dice: 0.8826  loss_ce_0: 0.3276  loss_cate_0: 0  loss_mask_0: 0.7669  loss_dice_0: 0.9361  loss_ce_1: 0.2126  loss_cate_1: 0  loss_mask_1: 0.8226  loss_dice_1: 0.947  loss_ce_2: 0.2148  loss_cate_2: 0  loss_mask_2: 0.7891  loss_dice_2: 0.9245  loss_ce_3: 0.2343  loss_cate_3: 0  loss_mask_3: 0.7648  loss_dice_3: 0.8983  loss_ce_4: 0.2218  loss_cate_4: 0  loss_mask_4: 0.7792  loss_dice_4: 0.9021  loss_ce_5: 0.2287  loss_cate_5: 0  loss_mask_5: 0.7869  loss_dice_5: 0.9346  loss_ce_6: 0.2296  loss_cate_6: 0  loss_mask_6: 0.7804  loss_dice_6: 0.8909  loss_ce_7: 0.2421  loss_cate_7: 0  loss_mask_7: 0.7527  loss_dice_7: 0.892  loss_ce_8: 0.2314  loss_cate_8: 0  loss_mask_8: 0.7749  loss_dice_8: 0.8914  time: 1.1020  data_time: 0.0151  lr: 8.5295e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:16:11 d2.utils.events]: \u001b[0m eta: 20:27:37  iter: 12979  total_loss: 18.19  loss_ce: 0.1961  loss_cate: 0.1291  loss_mask: 0.687  loss_dice: 0.9103  loss_ce_0: 0.3389  loss_cate_0: 0  loss_mask_0: 0.6894  loss_dice_0: 0.9572  loss_ce_1: 0.265  loss_cate_1: 0  loss_mask_1: 0.6843  loss_dice_1: 0.8807  loss_ce_2: 0.285  loss_cate_2: 0  loss_mask_2: 0.6633  loss_dice_2: 0.9716  loss_ce_3: 0.2423  loss_cate_3: 0  loss_mask_3: 0.67  loss_dice_3: 0.9072  loss_ce_4: 0.273  loss_cate_4: 0  loss_mask_4: 0.6607  loss_dice_4: 0.8766  loss_ce_5: 0.2432  loss_cate_5: 0  loss_mask_5: 0.659  loss_dice_5: 0.929  loss_ce_6: 0.1777  loss_cate_6: 0  loss_mask_6: 0.6744  loss_dice_6: 0.912  loss_ce_7: 0.2057  loss_cate_7: 0  loss_mask_7: 0.661  loss_dice_7: 0.8948  loss_ce_8: 0.1696  loss_cate_8: 0  loss_mask_8: 0.6848  loss_dice_8: 0.915  time: 1.1020  data_time: 0.0138  lr: 8.5272e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:17:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]\n",
      "\u001b[32m[10/12 17:17:29 d2.data.common]: \u001b[0mSerializing 2016 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/12 17:17:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.46 MiB\n",
      "\u001b[32m[10/12 17:18:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 2016 batches\n",
      "\u001b[32m[10/12 17:18:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/2016. Dataloading: 0.0014 s/iter. Inference: 0.1613 s/iter. Eval: 0.0124 s/iter. Total: 0.1752 s/iter. ETA=0:05:51\n",
      "\u001b[32m[10/12 17:18:39 d2.evaluation.evaluator]: \u001b[0mInference done 37/2016. Dataloading: 0.0023 s/iter. Inference: 0.1615 s/iter. Eval: 0.0295 s/iter. Total: 0.1934 s/iter. ETA=0:06:22\n",
      "\u001b[32m[10/12 17:18:44 d2.evaluation.evaluator]: \u001b[0mInference done 63/2016. Dataloading: 0.0023 s/iter. Inference: 0.1622 s/iter. Eval: 0.0285 s/iter. Total: 0.1932 s/iter. ETA=0:06:17\n",
      "\u001b[32m[10/12 17:18:49 d2.evaluation.evaluator]: \u001b[0mInference done 89/2016. Dataloading: 0.0023 s/iter. Inference: 0.1624 s/iter. Eval: 0.0294 s/iter. Total: 0.1943 s/iter. ETA=0:06:14\n",
      "\u001b[32m[10/12 17:18:54 d2.evaluation.evaluator]: \u001b[0mInference done 116/2016. Dataloading: 0.0023 s/iter. Inference: 0.1624 s/iter. Eval: 0.0304 s/iter. Total: 0.1953 s/iter. ETA=0:06:11\n",
      "\u001b[32m[10/12 17:18:59 d2.evaluation.evaluator]: \u001b[0mInference done 143/2016. Dataloading: 0.0023 s/iter. Inference: 0.1623 s/iter. Eval: 0.0309 s/iter. Total: 0.1957 s/iter. ETA=0:06:06\n",
      "\u001b[32m[10/12 17:19:05 d2.evaluation.evaluator]: \u001b[0mInference done 170/2016. Dataloading: 0.0023 s/iter. Inference: 0.1624 s/iter. Eval: 0.0303 s/iter. Total: 0.1951 s/iter. ETA=0:06:00\n",
      "\u001b[32m[10/12 17:19:10 d2.evaluation.evaluator]: \u001b[0mInference done 195/2016. Dataloading: 0.0023 s/iter. Inference: 0.1625 s/iter. Eval: 0.0317 s/iter. Total: 0.1966 s/iter. ETA=0:05:58\n",
      "\u001b[32m[10/12 17:19:15 d2.evaluation.evaluator]: \u001b[0mInference done 222/2016. Dataloading: 0.0023 s/iter. Inference: 0.1624 s/iter. Eval: 0.0309 s/iter. Total: 0.1958 s/iter. ETA=0:05:51\n",
      "\u001b[32m[10/12 17:19:20 d2.evaluation.evaluator]: \u001b[0mInference done 248/2016. Dataloading: 0.0023 s/iter. Inference: 0.1625 s/iter. Eval: 0.0310 s/iter. Total: 0.1960 s/iter. ETA=0:05:46\n",
      "\u001b[32m[10/12 17:19:25 d2.evaluation.evaluator]: \u001b[0mInference done 276/2016. Dataloading: 0.0023 s/iter. Inference: 0.1625 s/iter. Eval: 0.0298 s/iter. Total: 0.1947 s/iter. ETA=0:05:38\n",
      "\u001b[32m[10/12 17:19:30 d2.evaluation.evaluator]: \u001b[0mInference done 302/2016. Dataloading: 0.0023 s/iter. Inference: 0.1625 s/iter. Eval: 0.0299 s/iter. Total: 0.1949 s/iter. ETA=0:05:34\n",
      "\u001b[32m[10/12 17:19:35 d2.evaluation.evaluator]: \u001b[0mInference done 329/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0294 s/iter. Total: 0.1944 s/iter. ETA=0:05:28\n",
      "\u001b[32m[10/12 17:19:41 d2.evaluation.evaluator]: \u001b[0mInference done 357/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0285 s/iter. Total: 0.1936 s/iter. ETA=0:05:21\n",
      "\u001b[32m[10/12 17:19:46 d2.evaluation.evaluator]: \u001b[0mInference done 383/2016. Dataloading: 0.0023 s/iter. Inference: 0.1627 s/iter. Eval: 0.0285 s/iter. Total: 0.1935 s/iter. ETA=0:05:16\n",
      "\u001b[32m[10/12 17:19:51 d2.evaluation.evaluator]: \u001b[0mInference done 410/2016. Dataloading: 0.0023 s/iter. Inference: 0.1625 s/iter. Eval: 0.0281 s/iter. Total: 0.1930 s/iter. ETA=0:05:09\n",
      "\u001b[32m[10/12 17:19:56 d2.evaluation.evaluator]: \u001b[0mInference done 437/2016. Dataloading: 0.0022 s/iter. Inference: 0.1625 s/iter. Eval: 0.0280 s/iter. Total: 0.1929 s/iter. ETA=0:05:04\n",
      "\u001b[32m[10/12 17:20:01 d2.evaluation.evaluator]: \u001b[0mInference done 464/2016. Dataloading: 0.0022 s/iter. Inference: 0.1625 s/iter. Eval: 0.0276 s/iter. Total: 0.1925 s/iter. ETA=0:04:58\n",
      "\u001b[32m[10/12 17:20:06 d2.evaluation.evaluator]: \u001b[0mInference done 491/2016. Dataloading: 0.0022 s/iter. Inference: 0.1624 s/iter. Eval: 0.0276 s/iter. Total: 0.1925 s/iter. ETA=0:04:53\n",
      "\u001b[32m[10/12 17:20:11 d2.evaluation.evaluator]: \u001b[0mInference done 517/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0276 s/iter. Total: 0.1925 s/iter. ETA=0:04:48\n",
      "\u001b[32m[10/12 17:20:16 d2.evaluation.evaluator]: \u001b[0mInference done 543/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0279 s/iter. Total: 0.1928 s/iter. ETA=0:04:44\n",
      "\u001b[32m[10/12 17:20:21 d2.evaluation.evaluator]: \u001b[0mInference done 570/2016. Dataloading: 0.0022 s/iter. Inference: 0.1626 s/iter. Eval: 0.0276 s/iter. Total: 0.1926 s/iter. ETA=0:04:38\n",
      "\u001b[32m[10/12 17:20:26 d2.evaluation.evaluator]: \u001b[0mInference done 597/2016. Dataloading: 0.0023 s/iter. Inference: 0.1626 s/iter. Eval: 0.0276 s/iter. Total: 0.1926 s/iter. ETA=0:04:33\n",
      "\u001b[32m[10/12 17:20:31 d2.evaluation.evaluator]: \u001b[0mInference done 623/2016. Dataloading: 0.0023 s/iter. Inference: 0.1627 s/iter. Eval: 0.0275 s/iter. Total: 0.1926 s/iter. ETA=0:04:28\n",
      "\u001b[32m[10/12 17:20:37 d2.evaluation.evaluator]: \u001b[0mInference done 650/2016. Dataloading: 0.0023 s/iter. Inference: 0.1627 s/iter. Eval: 0.0273 s/iter. Total: 0.1924 s/iter. ETA=0:04:22\n",
      "\u001b[32m[10/12 17:20:42 d2.evaluation.evaluator]: \u001b[0mInference done 676/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0275 s/iter. Total: 0.1927 s/iter. ETA=0:04:18\n",
      "\u001b[32m[10/12 17:20:47 d2.evaluation.evaluator]: \u001b[0mInference done 701/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0279 s/iter. Total: 0.1930 s/iter. ETA=0:04:13\n",
      "\u001b[32m[10/12 17:20:52 d2.evaluation.evaluator]: \u001b[0mInference done 728/2016. Dataloading: 0.0022 s/iter. Inference: 0.1627 s/iter. Eval: 0.0279 s/iter. Total: 0.1930 s/iter. ETA=0:04:08\n",
      "\u001b[32m[10/12 17:20:57 d2.evaluation.evaluator]: \u001b[0mInference done 754/2016. Dataloading: 0.0022 s/iter. Inference: 0.1627 s/iter. Eval: 0.0280 s/iter. Total: 0.1931 s/iter. ETA=0:04:03\n",
      "\u001b[32m[10/12 17:21:02 d2.evaluation.evaluator]: \u001b[0mInference done 780/2016. Dataloading: 0.0022 s/iter. Inference: 0.1627 s/iter. Eval: 0.0281 s/iter. Total: 0.1932 s/iter. ETA=0:03:58\n",
      "\u001b[32m[10/12 17:21:07 d2.evaluation.evaluator]: \u001b[0mInference done 805/2016. Dataloading: 0.0022 s/iter. Inference: 0.1627 s/iter. Eval: 0.0283 s/iter. Total: 0.1934 s/iter. ETA=0:03:54\n",
      "\u001b[32m[10/12 17:21:12 d2.evaluation.evaluator]: \u001b[0mInference done 832/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0281 s/iter. Total: 0.1933 s/iter. ETA=0:03:48\n",
      "\u001b[32m[10/12 17:21:17 d2.evaluation.evaluator]: \u001b[0mInference done 858/2016. Dataloading: 0.0022 s/iter. Inference: 0.1627 s/iter. Eval: 0.0283 s/iter. Total: 0.1934 s/iter. ETA=0:03:43\n",
      "\u001b[32m[10/12 17:21:22 d2.evaluation.evaluator]: \u001b[0mInference done 885/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0280 s/iter. Total: 0.1931 s/iter. ETA=0:03:38\n",
      "\u001b[32m[10/12 17:21:27 d2.evaluation.evaluator]: \u001b[0mInference done 911/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0280 s/iter. Total: 0.1932 s/iter. ETA=0:03:33\n",
      "\u001b[32m[10/12 17:21:33 d2.evaluation.evaluator]: \u001b[0mInference done 938/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0279 s/iter. Total: 0.1931 s/iter. ETA=0:03:28\n",
      "\u001b[32m[10/12 17:21:38 d2.evaluation.evaluator]: \u001b[0mInference done 964/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0281 s/iter. Total: 0.1933 s/iter. ETA=0:03:23\n",
      "\u001b[32m[10/12 17:21:43 d2.evaluation.evaluator]: \u001b[0mInference done 991/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0279 s/iter. Total: 0.1931 s/iter. ETA=0:03:17\n",
      "\u001b[32m[10/12 17:21:48 d2.evaluation.evaluator]: \u001b[0mInference done 1017/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0280 s/iter. Total: 0.1932 s/iter. ETA=0:03:13\n",
      "\u001b[32m[10/12 17:21:53 d2.evaluation.evaluator]: \u001b[0mInference done 1045/2016. Dataloading: 0.0022 s/iter. Inference: 0.1628 s/iter. Eval: 0.0277 s/iter. Total: 0.1929 s/iter. ETA=0:03:07\n",
      "\u001b[32m[10/12 17:21:58 d2.evaluation.evaluator]: \u001b[0mInference done 1071/2016. Dataloading: 0.0022 s/iter. Inference: 0.1629 s/iter. Eval: 0.0278 s/iter. Total: 0.1930 s/iter. ETA=0:03:02\n",
      "\u001b[32m[10/12 17:22:03 d2.evaluation.evaluator]: \u001b[0mInference done 1098/2016. Dataloading: 0.0022 s/iter. Inference: 0.1629 s/iter. Eval: 0.0277 s/iter. Total: 0.1930 s/iter. ETA=0:02:57\n",
      "\u001b[32m[10/12 17:22:08 d2.evaluation.evaluator]: \u001b[0mInference done 1124/2016. Dataloading: 0.0022 s/iter. Inference: 0.1629 s/iter. Eval: 0.0277 s/iter. Total: 0.1930 s/iter. ETA=0:02:52\n",
      "\u001b[32m[10/12 17:22:14 d2.evaluation.evaluator]: \u001b[0mInference done 1151/2016. Dataloading: 0.0022 s/iter. Inference: 0.1629 s/iter. Eval: 0.0277 s/iter. Total: 0.1930 s/iter. ETA=0:02:46\n",
      "\u001b[32m[10/12 17:22:19 d2.evaluation.evaluator]: \u001b[0mInference done 1176/2016. Dataloading: 0.0022 s/iter. Inference: 0.1630 s/iter. Eval: 0.0278 s/iter. Total: 0.1932 s/iter. ETA=0:02:42\n",
      "\u001b[32m[10/12 17:22:24 d2.evaluation.evaluator]: \u001b[0mInference done 1202/2016. Dataloading: 0.0022 s/iter. Inference: 0.1630 s/iter. Eval: 0.0279 s/iter. Total: 0.1933 s/iter. ETA=0:02:37\n",
      "\u001b[32m[10/12 17:22:29 d2.evaluation.evaluator]: \u001b[0mInference done 1227/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0280 s/iter. Total: 0.1934 s/iter. ETA=0:02:32\n",
      "\u001b[32m[10/12 17:22:34 d2.evaluation.evaluator]: \u001b[0mInference done 1254/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0284 s/iter. Total: 0.1938 s/iter. ETA=0:02:27\n",
      "\u001b[32m[10/12 17:22:40 d2.evaluation.evaluator]: \u001b[0mInference done 1281/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0282 s/iter. Total: 0.1937 s/iter. ETA=0:02:22\n",
      "\u001b[32m[10/12 17:22:45 d2.evaluation.evaluator]: \u001b[0mInference done 1307/2016. Dataloading: 0.0022 s/iter. Inference: 0.1631 s/iter. Eval: 0.0282 s/iter. Total: 0.1937 s/iter. ETA=0:02:17\n",
      "\u001b[32m[10/12 17:22:50 d2.evaluation.evaluator]: \u001b[0mInference done 1334/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1936 s/iter. ETA=0:02:12\n",
      "\u001b[32m[10/12 17:22:55 d2.evaluation.evaluator]: \u001b[0mInference done 1360/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1937 s/iter. ETA=0:02:07\n",
      "\u001b[32m[10/12 17:23:00 d2.evaluation.evaluator]: \u001b[0mInference done 1387/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1935 s/iter. ETA=0:02:01\n",
      "\u001b[32m[10/12 17:23:05 d2.evaluation.evaluator]: \u001b[0mInference done 1413/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:01:56\n",
      "\u001b[32m[10/12 17:23:10 d2.evaluation.evaluator]: \u001b[0mInference done 1439/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:01:51\n",
      "\u001b[32m[10/12 17:23:15 d2.evaluation.evaluator]: \u001b[0mInference done 1466/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:01:46\n",
      "\u001b[32m[10/12 17:23:20 d2.evaluation.evaluator]: \u001b[0mInference done 1492/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:01:41\n",
      "\u001b[32m[10/12 17:23:25 d2.evaluation.evaluator]: \u001b[0mInference done 1519/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:01:36\n",
      "\u001b[32m[10/12 17:23:30 d2.evaluation.evaluator]: \u001b[0mInference done 1544/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1937 s/iter. ETA=0:01:31\n",
      "\u001b[32m[10/12 17:23:36 d2.evaluation.evaluator]: \u001b[0mInference done 1571/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:01:26\n",
      "\u001b[32m[10/12 17:23:41 d2.evaluation.evaluator]: \u001b[0mInference done 1597/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1937 s/iter. ETA=0:01:21\n",
      "\u001b[32m[10/12 17:23:46 d2.evaluation.evaluator]: \u001b[0mInference done 1624/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:01:15\n",
      "\u001b[32m[10/12 17:23:51 d2.evaluation.evaluator]: \u001b[0mInference done 1650/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0281 s/iter. Total: 0.1936 s/iter. ETA=0:01:10\n",
      "\u001b[32m[10/12 17:23:56 d2.evaluation.evaluator]: \u001b[0mInference done 1677/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:01:05\n",
      "\u001b[32m[10/12 17:24:01 d2.evaluation.evaluator]: \u001b[0mInference done 1703/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:01:00\n",
      "\u001b[32m[10/12 17:24:06 d2.evaluation.evaluator]: \u001b[0mInference done 1730/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:00:55\n",
      "\u001b[32m[10/12 17:24:11 d2.evaluation.evaluator]: \u001b[0mInference done 1756/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:00:50\n",
      "\u001b[32m[10/12 17:24:16 d2.evaluation.evaluator]: \u001b[0mInference done 1783/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:00:45\n",
      "\u001b[32m[10/12 17:24:22 d2.evaluation.evaluator]: \u001b[0mInference done 1810/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1934 s/iter. ETA=0:00:39\n",
      "\u001b[32m[10/12 17:24:27 d2.evaluation.evaluator]: \u001b[0mInference done 1836/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0278 s/iter. Total: 0.1935 s/iter. ETA=0:00:34\n",
      "\u001b[32m[10/12 17:24:32 d2.evaluation.evaluator]: \u001b[0mInference done 1862/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:00:29\n",
      "\u001b[32m[10/12 17:24:37 d2.evaluation.evaluator]: \u001b[0mInference done 1888/2016. Dataloading: 0.0022 s/iter. Inference: 0.1633 s/iter. Eval: 0.0279 s/iter. Total: 0.1935 s/iter. ETA=0:00:24\n",
      "\u001b[32m[10/12 17:24:42 d2.evaluation.evaluator]: \u001b[0mInference done 1914/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1936 s/iter. ETA=0:00:19\n",
      "\u001b[32m[10/12 17:24:47 d2.evaluation.evaluator]: \u001b[0mInference done 1940/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0279 s/iter. Total: 0.1936 s/iter. ETA=0:00:14\n",
      "\u001b[32m[10/12 17:24:52 d2.evaluation.evaluator]: \u001b[0mInference done 1965/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:00:09\n",
      "\u001b[32m[10/12 17:24:57 d2.evaluation.evaluator]: \u001b[0mInference done 1992/2016. Dataloading: 0.0022 s/iter. Inference: 0.1632 s/iter. Eval: 0.0280 s/iter. Total: 0.1936 s/iter. ETA=0:00:04\n",
      "\u001b[32m[10/12 17:25:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:29.479626 (0.193675 s / iter per device, on 1 devices)\n",
      "\u001b[32m[10/12 17:25:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:05:28 (0.163217 s / iter per device, on 1 devices)\n",
      "miou = 75.83737380502447\n",
      "OA = 89.17197386423746\n",
      "Kappa = 85.89123785903817\n",
      "F1_score = 72.52089464568738\n",
      "\u001b[32m[10/12 17:25:03 d2.evaluation.sem_seg_evaluation]: \u001b[0mOrderedDict([('sem_seg', {'mIoU': 75.83737380502447, 'fwIoU': 81.06680722098206, 'IoU-Background': 42.14378092418612, 'IoU-Surfaces': 84.4018652373978, 'IoU-Building': 92.32545339472044, 'IoU-Low vegetation': 75.1388282910216, 'IoU-tree': 76.67272388036055, 'IoU-Car': 84.34159110246023, 'mACC': 84.75108369176404, 'pACC': 89.17197386423746, 'ACC-Background': 56.07878721154873, 'ACC-Surfaces': 91.88408021404504, 'ACC-Building': 96.05447998096498, 'ACC-Low vegetation': 86.13569475245994, 'ACC-tree': 87.0252772183294, 'ACC-Car': 91.3281827732361})])\n",
      "\u001b[32m[10/12 17:25:03 d2.engine.defaults]: \u001b[0mEvaluation results for Potsdam_test in csv format:\n",
      "\u001b[32m[10/12 17:25:03 d2.evaluation.testing]: \u001b[0mcopypaste: Task: sem_seg\n",
      "\u001b[32m[10/12 17:25:03 d2.evaluation.testing]: \u001b[0mcopypaste: mIoU,fwIoU,mACC,pACC\n",
      "\u001b[32m[10/12 17:25:03 d2.evaluation.testing]: \u001b[0mcopypaste: 75.8374,81.0668,84.7511,89.1720\n",
      "\u001b[32m[10/12 17:25:03 d2.utils.events]: \u001b[0m eta: 20:27:15  iter: 12999  total_loss: 19.16  loss_ce: 0.08989  loss_cate: 0.1348  loss_mask: 0.8498  loss_dice: 0.9425  loss_ce_0: 0.2373  loss_cate_0: 0  loss_mask_0: 0.8496  loss_dice_0: 0.9906  loss_ce_1: 0.1316  loss_cate_1: 0  loss_mask_1: 0.844  loss_dice_1: 0.959  loss_ce_2: 0.1458  loss_cate_2: 0  loss_mask_2: 0.8347  loss_dice_2: 0.883  loss_ce_3: 0.1264  loss_cate_3: 0  loss_mask_3: 0.8272  loss_dice_3: 0.905  loss_ce_4: 0.1085  loss_cate_4: 0  loss_mask_4: 0.823  loss_dice_4: 0.9301  loss_ce_5: 0.0888  loss_cate_5: 0  loss_mask_5: 0.8252  loss_dice_5: 0.8846  loss_ce_6: 0.08868  loss_cate_6: 0  loss_mask_6: 0.8371  loss_dice_6: 0.9146  loss_ce_7: 0.07946  loss_cate_7: 0  loss_mask_7: 0.849  loss_dice_7: 0.9042  loss_ce_8: 0.08266  loss_cate_8: 0  loss_mask_8: 0.8456  loss_dice_8: 0.9524  time: 1.1020  data_time: 0.0129  lr: 8.525e-06  max_mem: 28917M\n",
      "\u001b[32m[10/12 17:25:25 d2.utils.events]: \u001b[0m eta: 20:26:50  iter: 13019  total_loss: 20.35  loss_ce: 0.2544  loss_cate: 0.149  loss_mask: 0.7994  loss_dice: 1.025  loss_ce_0: 0.2667  loss_cate_0: 0  loss_mask_0: 0.8192  loss_dice_0: 1.015  loss_ce_1: 0.2046  loss_cate_1: 0  loss_mask_1: 0.8296  loss_dice_1: 0.9717  loss_ce_2: 0.2325  loss_cate_2: 0  loss_mask_2: 0.8261  loss_dice_2: 0.9345  loss_ce_3: 0.2347  loss_cate_3: 0  loss_mask_3: 0.801  loss_dice_3: 0.9723  loss_ce_4: 0.1965  loss_cate_4: 0  loss_mask_4: 0.8019  loss_dice_4: 0.961  loss_ce_5: 0.2032  loss_cate_5: 0  loss_mask_5: 0.8097  loss_dice_5: 1.017  loss_ce_6: 0.2344  loss_cate_6: 0  loss_mask_6: 0.8018  loss_dice_6: 1.014  loss_ce_7: 0.2429  loss_cate_7: 0  loss_mask_7: 0.8085  loss_dice_7: 1.016  loss_ce_8: 0.2333  loss_cate_8: 0  loss_mask_8: 0.7949  loss_dice_8: 1.016  time: 1.1020  data_time: 0.0131  lr: 8.5227e-06  max_mem: 28917M\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "MaskFormer Training Script.\n",
    "\n",
    "This script is a simplified version of the training script in detectron2/tools.\n",
    "\"\"\"\n",
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Set\n",
    "\n",
    "import torch\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, build_detection_train_loader\n",
    "from detectron2.engine import (\n",
    "    DefaultTrainer,\n",
    "    default_argument_parser,\n",
    "    default_setup,\n",
    "    launch,\n",
    ")\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    LVISEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    verify_results,\n",
    ")\n",
    "from detectron2.projects.deeplab import add_deeplab_config, build_lr_scheduler\n",
    "from detectron2.solver.build import maybe_add_gradient_clipping\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# MaskFormer\n",
    "from mask2former import (\n",
    "    COCOInstanceNewBaselineDatasetMapper,\n",
    "    COCOPanopticNewBaselineDatasetMapper,\n",
    "    InstanceSegEvaluator,\n",
    "    MaskFormerInstanceDatasetMapper,\n",
    "    MaskFormerPanopticDatasetMapper,\n",
    "    MaskFormerSemanticDatasetMapper,\n",
    "    SemanticSegmentorWithTTA,\n",
    "    add_maskformer2_config,\n",
    ")\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Extension of the Trainer class adapted to MaskFormer.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Create evaluator(s) for a given dataset.\n",
    "        This uses the special metadata \"evaluator_type\" associated with each\n",
    "        builtin dataset. For your own dataset, you can simply create an\n",
    "        evaluator manually in your script and do not have to worry about the\n",
    "        hacky if-else logic here.\n",
    "        \"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluator_list = []\n",
    "        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "        # semantic segmentation\n",
    "        if evaluator_type in [\"sem_seg\", \"ade20k_panoptic_seg\"]:\n",
    "            evaluator_list.append(\n",
    "                SemSegEvaluator(\n",
    "                    dataset_name,\n",
    "                    distributed=True,\n",
    "                    output_dir=output_folder,\n",
    "                )\n",
    "            )\n",
    "        # instance segmentation\n",
    "        if evaluator_type == \"coco\":\n",
    "            evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
    "        # panoptic segmentation\n",
    "        if evaluator_type in [\n",
    "            \"coco_panoptic_seg\",\n",
    "            \"ade20k_panoptic_seg\",\n",
    "            \"cityscapes_panoptic_seg\",\n",
    "            \"mapillary_vistas_panoptic_seg\",\n",
    "        ]:\n",
    "            if cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON:\n",
    "                evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
    "        # COCO\n",
    "        if evaluator_type == \"coco_panoptic_seg\" and cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON:\n",
    "            evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
    "        if evaluator_type == \"coco_panoptic_seg\" and cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON:\n",
    "            evaluator_list.append(SemSegEvaluator(dataset_name, distributed=True, output_dir=output_folder))\n",
    "        # Mapillary Vistas\n",
    "        if evaluator_type == \"mapillary_vistas_panoptic_seg\" and cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON:\n",
    "            evaluator_list.append(InstanceSegEvaluator(dataset_name, output_dir=output_folder))\n",
    "        if evaluator_type == \"mapillary_vistas_panoptic_seg\" and cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON:\n",
    "            evaluator_list.append(SemSegEvaluator(dataset_name, distributed=True, output_dir=output_folder))\n",
    "        # Cityscapes\n",
    "        if evaluator_type == \"cityscapes_instance\":\n",
    "            assert (\n",
    "                torch.cuda.device_count() > comm.get_rank()\n",
    "            ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
    "            return CityscapesInstanceEvaluator(dataset_name)\n",
    "        if evaluator_type == \"cityscapes_sem_seg\":\n",
    "            assert (\n",
    "                torch.cuda.device_count() > comm.get_rank()\n",
    "            ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
    "            return CityscapesSemSegEvaluator(dataset_name)\n",
    "        if evaluator_type == \"cityscapes_panoptic_seg\":\n",
    "            if cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON:\n",
    "                assert (\n",
    "                    torch.cuda.device_count() > comm.get_rank()\n",
    "                ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
    "                evaluator_list.append(CityscapesSemSegEvaluator(dataset_name))\n",
    "            if cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON:\n",
    "                assert (\n",
    "                    torch.cuda.device_count() > comm.get_rank()\n",
    "                ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
    "                evaluator_list.append(CityscapesInstanceEvaluator(dataset_name))\n",
    "        # ADE20K\n",
    "        if evaluator_type == \"ade20k_panoptic_seg\" and cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON:\n",
    "            evaluator_list.append(InstanceSegEvaluator(dataset_name, output_dir=output_folder))\n",
    "        # LVIS\n",
    "        if evaluator_type == \"lvis\":\n",
    "            return LVISEvaluator(dataset_name, output_dir=output_folder)\n",
    "        if len(evaluator_list) == 0:\n",
    "            raise NotImplementedError(\n",
    "                \"no Evaluator for the dataset {} with the type {}\".format(\n",
    "                    dataset_name, evaluator_type\n",
    "                )\n",
    "            )\n",
    "        elif len(evaluator_list) == 1:\n",
    "            return evaluator_list[0]\n",
    "        return DatasetEvaluators(evaluator_list)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        # Semantic segmentation dataset mapper\n",
    "        if cfg.INPUT.DATASET_MAPPER_NAME == \"mask_former_semantic\":\n",
    "            mapper = MaskFormerSemanticDatasetMapper(cfg, True)\n",
    "            return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        # Panoptic segmentation dataset mapper\n",
    "        elif cfg.INPUT.DATASET_MAPPER_NAME == \"mask_former_panoptic\":\n",
    "            mapper = MaskFormerPanopticDatasetMapper(cfg, True)\n",
    "            return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        # Instance segmentation dataset mapper\n",
    "        elif cfg.INPUT.DATASET_MAPPER_NAME == \"mask_former_instance\":\n",
    "            mapper = MaskFormerInstanceDatasetMapper(cfg, True)\n",
    "            return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        # coco instance segmentation lsj new baseline\n",
    "        elif cfg.INPUT.DATASET_MAPPER_NAME == \"coco_instance_lsj\":\n",
    "            mapper = COCOInstanceNewBaselineDatasetMapper(cfg, True)\n",
    "            return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        # coco panoptic segmentation lsj new baseline\n",
    "        elif cfg.INPUT.DATASET_MAPPER_NAME == \"coco_panoptic_lsj\":\n",
    "            mapper = COCOPanopticNewBaselineDatasetMapper(cfg, True)\n",
    "            return build_detection_train_loader(cfg, mapper=mapper)\n",
    "        else:\n",
    "            mapper = None\n",
    "            return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer):\n",
    "        \"\"\"\n",
    "        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n",
    "        Overwrite it if you'd like a different scheduler.\n",
    "        \"\"\"\n",
    "        return build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "    @classmethod\n",
    "    def build_optimizer(cls, cfg, model):\n",
    "        weight_decay_norm = cfg.SOLVER.WEIGHT_DECAY_NORM\n",
    "        weight_decay_embed = cfg.SOLVER.WEIGHT_DECAY_EMBED\n",
    "\n",
    "        defaults = {}\n",
    "        defaults[\"lr\"] = cfg.SOLVER.BASE_LR\n",
    "        defaults[\"weight_decay\"] = cfg.SOLVER.WEIGHT_DECAY\n",
    "\n",
    "        norm_module_types = (\n",
    "            torch.nn.BatchNorm1d,\n",
    "            torch.nn.BatchNorm2d,\n",
    "            torch.nn.BatchNorm3d,\n",
    "            torch.nn.SyncBatchNorm,\n",
    "            # NaiveSyncBatchNorm inherits from BatchNorm2d\n",
    "            torch.nn.GroupNorm,\n",
    "            torch.nn.InstanceNorm1d,\n",
    "            torch.nn.InstanceNorm2d,\n",
    "            torch.nn.InstanceNorm3d,\n",
    "            torch.nn.LayerNorm,\n",
    "            torch.nn.LocalResponseNorm,\n",
    "        )\n",
    "\n",
    "        params: List[Dict[str, Any]] = []\n",
    "        memo: Set[torch.nn.parameter.Parameter] = set()\n",
    "        for module_name, module in model.named_modules():\n",
    "            for module_param_name, value in module.named_parameters(recurse=False):\n",
    "                if not value.requires_grad:\n",
    "                    continue\n",
    "                # Avoid duplicating parameters\n",
    "                if value in memo:\n",
    "                    continue\n",
    "                memo.add(value)\n",
    "\n",
    "                hyperparams = copy.copy(defaults)\n",
    "                if \"backbone\" in module_name:\n",
    "                    hyperparams[\"lr\"] = hyperparams[\"lr\"] * cfg.SOLVER.BACKBONE_MULTIPLIER\n",
    "                if (\n",
    "                    \"relative_position_bias_table\" in module_param_name\n",
    "                    or \"absolute_pos_embed\" in module_param_name\n",
    "                ):\n",
    "                    print(module_param_name)\n",
    "                    hyperparams[\"weight_decay\"] = 0.0\n",
    "                if isinstance(module, norm_module_types):\n",
    "                    hyperparams[\"weight_decay\"] = weight_decay_norm\n",
    "                if isinstance(module, torch.nn.Embedding):\n",
    "                    hyperparams[\"weight_decay\"] = weight_decay_embed\n",
    "                params.append({\"params\": [value], **hyperparams})\n",
    "\n",
    "        def maybe_add_full_model_gradient_clipping(optim):\n",
    "            # detectron2 doesn't have full model gradient clipping now\n",
    "            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n",
    "            enable = (\n",
    "                cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n",
    "                and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n",
    "                and clip_norm_val > 0.0\n",
    "            )\n",
    "\n",
    "            class FullModelGradientClippingOptimizer(optim):\n",
    "                def step(self, closure=None):\n",
    "                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n",
    "                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n",
    "                    super().step(closure=closure)\n",
    "\n",
    "            return FullModelGradientClippingOptimizer if enable else optim\n",
    "\n",
    "        optimizer_type = cfg.SOLVER.OPTIMIZER\n",
    "        if optimizer_type == \"SGD\":\n",
    "            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(\n",
    "                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM\n",
    "            )\n",
    "        elif optimizer_type == \"ADAMW\":\n",
    "            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(\n",
    "                params, cfg.SOLVER.BASE_LR\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n",
    "        if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\":\n",
    "            optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n",
    "        return optimizer\n",
    "\n",
    "    @classmethod\n",
    "    def test_with_TTA(cls, cfg, model):\n",
    "        logger = logging.getLogger(\"detectron2.trainer\")\n",
    "        # In the end of training, run an evaluation with TTA.\n",
    "        logger.info(\"Running inference with test-time augmentation ...\")\n",
    "        model = SemanticSegmentorWithTTA(cfg, model)\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
    "        return res\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    # for poly lr schedule\n",
    "    add_deeplab_config(cfg)\n",
    "    add_maskformer2_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    \n",
    "    default_setup(cfg, args)\n",
    "    # Setup logger for \"mask_former\" module\n",
    "    setup_logger(output=cfg.OUTPUT_DIR, distributed_rank=comm.get_rank(), name=\"mask_former\")\n",
    "    return cfg\n",
    "\n",
    "def Potsdam_train_dummy_reg():\n",
    "    dataset_dir='data/potsdam/'\n",
    "    dataset_dicts = []\n",
    "    img_dir = dataset_dir+'images/train_dummy1/'\n",
    "    label_dir = dataset_dir+'annotations/train_dummy1/'\n",
    "    Files=os.listdir(img_dir)\n",
    "    i=0\n",
    "    for file in Files:\n",
    "        record={}\n",
    "        height, width = cv2.imread(img_dir+file).shape[:2]\n",
    "        record[\"file_name\"] = img_dir+file\n",
    "        record[\"image_id\"] = img_dir+file\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        record[\"sem_seg_file_name\"] = label_dir+file[0:-4]+'.png'\n",
    "        dataset_dicts.append(record)\n",
    "       \n",
    "        \n",
    "    return dataset_dicts\n",
    "\n",
    "def main(args):\n",
    "    # from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "    # DatasetCatalog.register(\"Potsdam_train_dummy\", Potsdam_train_dummy_reg)\n",
    "    # MetadataCatalog.get(\"Potsdam_train_dummy\").stuff_classes = ['Background','Surfaces','Building','Low vegetation','tree','Car']\n",
    "    # MetadataCatalog.get(\"Potsdam_train_dummy\").stuff_colors = [(255,255,255),(0,0,255),(0,255,255),(0,255,0),(255,255,0),(255,0,0)]\n",
    "    # #MetadataCatalog.get(\"Potsdam_train\").stuff_colors = [0，1，2，3，4，255]\n",
    "    # MetadataCatalog.get(\"Potsdam_train_dummy\").ignore_label=255\n",
    "    \n",
    "    cfg = setup(args)\n",
    "    if args.eval_only:\n",
    "        model = Trainer.build_model(cfg)\n",
    "        net_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(\"Total Params: {} M\".format(net_params/1e6))\n",
    "        cfg.defrost()\n",
    "        cfg.MODEL.WEIGHTS = \"./output/potsdam_boundary_loss/model_final.pth\"\n",
    "        cfg.freeze()\n",
    "        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "            cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "        )\n",
    "        # './output/model_0084999.pth', resume=args.resume\n",
    "        # cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "        res = Trainer.test(cfg, model)\n",
    "        if cfg.TEST.AUG.ENABLED:\n",
    "            res.update(Trainer.test_with_TTA(cfg, model))\n",
    "        if comm.is_main_process():\n",
    "            verify_results(cfg, res)\n",
    "        return res\n",
    "\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=args.resume)\n",
    "    return trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    parser = default_argument_parser()\n",
    "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "    args = parser.parse_args()\n",
    "    # args.config_file = './output/vaihingen_attempt_1_80000iter_baselr/config.yaml'\n",
    "    # args.config_file = './output/attempt 1/config.yaml'\n",
    "    args.config_file = './configs/ade20k/semantic-segmentation/semask_swin/msfapn_maskformer2_semask_swin_large_IN21k_384_bs16_160k_res640_boundary_loss.yaml'\n",
    "    args.num_gpus = 1\n",
    "    #args.eval_only = True\n",
    "    args.eval_only = False\n",
    "    args.resume = False\n",
    "    print(\"Command Line Args:\", args)    \n",
    "    launch(\n",
    "        main,\n",
    "        args.num_gpus,\n",
    "        num_machines=args.num_machines,\n",
    "        machine_rank=args.machine_rank,\n",
    "        dist_url=args.dist_url,\n",
    "        args=(args,),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SeMask1",
   "language": "python",
   "name": "semask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
